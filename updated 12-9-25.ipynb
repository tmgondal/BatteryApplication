{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ef1a7ba-492f-4285-918c-4a392972c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"EIS_TEST_FILES\": [\n",
      "    \"Mazda-Battery-Cell9.xlsx\"\n",
      "  ],\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"SOH_STD_MAX_OOD\": 2.0,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"GROUP_KFOLDS\": 0,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"USE_PCA_SOC\": false,\n",
      "  \"USE_PCA_SOH\": false,\n",
      "  \"PCA_SOC_COMPONENTS\": 25,\n",
      "  \"PCA_SOH_COMPONENTS\": 30,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"INCLUDE_NORMALIZED_SHAPE_MODEL\": true,\n",
      "  \"ENSEMBLE_SOH\": true,\n",
      "  \"NORMALIZE_SHAPE_BY_HF_RE\": true,\n",
      "  \"SOC_INCLUDE_SHAPE_MODEL\": true,\n",
      "  \"SOC_MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"DECISION_SOH_PERCENT\": 50.0,\n",
      "  \"ILLUSTRATIVE_MIN_SOH\": 40.0,\n",
      "  \"CPP_ROLLING_WINDOW\": 5,\n",
      "  \"CPP_MIN_POINTS\": 6,\n",
      "  \"CPP_FALLBACK\": 20.0,\n",
      "  \"TEST_TEMPERATURE_OVERRIDE\": 25.0,\n",
      "  \"FORCE_RETRAIN\": false,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FEATURE_VERSION\": 88,\n",
      "  \"MAHAL_THRESHOLD\": 10.0,\n",
      "  \"GP_ARD_NORM_THRESHOLD\": 6.0,\n",
      "  \"PLOT_EXPONENT\": 1.25,\n",
      "  \"TARGET_SOH_THRESHOLDS\": [\n",
      "    80.0,\n",
      "    50.0,\n",
      "    40.0\n",
      "  ],\n",
      "  \"OOD_SOC_ENABLE\": true,\n",
      "  \"OOD_SOC_Q\": 0.999,\n",
      "  \"OOD_SOC_SHRINK_SCALE\": 10.0,\n",
      "  \"OOD_SOC_W_MIN\": 0.45,\n",
      "  \"SOC_OOD_K\": 30,\n",
      "  \"SOC_CALIBRATE_ON_OOD\": false,\n",
      "  \"SOC_STD_CAP_MULT_IN\": 1.0,\n",
      "  \"SOC_STD_CAP_MULT_OOD\": 1.2,\n",
      "  \"SOC_LABEL_JITTER\": 0.0\n",
      "}\n",
      "[CPP] dynamic cells=19 global_cpp_median=16.81\n",
      "[LOAD] Found bundle. Signature match: True\n",
      "\n",
      "===== TEST: Mazda-Battery-Cell9.xlsx =====\n",
      "[AUTO-SCALE] Mazda-Battery-Cell9.xlsx: hf_test=0.0127, f7_test=0.007921 → scaled by 2.922 to align.\n",
      "[SoC-OOD] space=shape d=528.13 thr=14.20 w_raw=0.450 w_shape=0.330 w_prior=0.220 prior_mean=65.02 prior_std=22.90 k=30\n",
      "[SoC] Mazda-Battery-Cell9.xlsx: mean=78.49 std=5.88  SOC_mahal_raw=559.8253865879898 thr_raw=14.180680112361749  SOC_mahal_shape=528.130497571179 thr_shape=14.202060296680994  used=shape OOD=True\n",
      "[SoH] Mazda-Battery-Cell9.xlsx: mean=91.62 std=2.00  OOD_flag(SoH)=True  Mahalanobis=612.5266485500102\n",
      "{\n",
      "  \"file\": \"Mazda-Battery-Cell9.xlsx\",\n",
      "  \"feature_checksum\": \"fbdc2246677c3830d920a49b1c0b3e9ba94c19da\",\n",
      "  \"parsed_metadata\": null,\n",
      "  \"predicted_SoC_percent\": 78.49296742979519,\n",
      "  \"SoC_std_estimate\": 5.877396348293306,\n",
      "  \"soc_model_chosen\": \"soc_hgb_shape\",\n",
      "  \"SoC_probabilities\": null,\n",
      "  \"SOC_mahal\": 528.130497571179,\n",
      "  \"SOC_ood\": true,\n",
      "  \"raw_soc_precal\": 82.38681867395798,\n",
      "  \"raw_soc_precal_std\": 5.4244023522799365,\n",
      "  \"shape_soc_mean\": 82.16182211262684,\n",
      "  \"shape_soc_std\": 5.4244023522799365,\n",
      "  \"predicted_SoH_percent\": 91.61500792387868,\n",
      "  \"SoH_std_estimate\": 2.0,\n",
      "  \"raw_model_mean\": 91.61500792387868,\n",
      "  \"raw_model_std\": 7.48060088863432,\n",
      "  \"shape_model_mean\": 91.61500792387868,\n",
      "  \"shape_model_std\": 7.461126805846326,\n",
      "  \"soh_model_chosen\": \"gpr_raw\",\n",
      "  \"cycles_per_percent_used\": 16.81169120554586,\n",
      "  \"cycles_to_thresholds\": {\n",
      "    \"80\": 195.26792656621666,\n",
      "    \"50\": 699.6186627325924,\n",
      "    \"40\": 867.735574788051\n",
      "  },\n",
      "  \"decision_threshold_percent\": 50.0,\n",
      "  \"lower_threshold_percent\": 40.0,\n",
      "  \"OOD_mahal\": 612.5266485500102,\n",
      "  \"OOD_gp_ard_norm\": null,\n",
      "  \"OOD_flag\": true,\n",
      "  \"used_freq_from_file\": true,\n",
      "  \"hf_train\": 0.03015,\n",
      "  \"hf_test_before_scale\": 0.0126976,\n",
      "  \"f7_train\": 0.028479454935939237,\n",
      "  \"f7_test_before_scale\": 0.007920877851120375,\n",
      "  \"autoscale_factor\": 2.921877579079462\n",
      "}\n",
      "[PLOT] Saved: models_eis_phase2_phys\\Mazda-Battery-Cell9_projection.png\n",
      "[JSON] Saved: models_eis_phase2_phys\\Mazda-Battery-Cell9_prediction.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Unified EIS Training + Inference + Dynamic RUL (v8.8 – Robust SoC HGB + dual-space OOD + hf/10Hz autoscale)\n",
    "+ Gradio UI (Jupyter-friendly)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, random, warnings, joblib, hashlib, uuid, io, sys, os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Quiet down some noisy warnings from GPR optimization (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.gaussian_process\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "\n",
    "# =========================\n",
    "# Helpers: environment detection\n",
    "# =========================\n",
    "def _running_in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # noqa\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell in (\"ZMQInteractiveShell\", \"Shell\")  # Jupyter/Lab/VSCode\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIGURATION\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Training data directories (update if needed)\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "\n",
    "    # Test files\n",
    "    EIS_TEST_FILES: List[Path] = None  # assigned after instantiation\n",
    "\n",
    "    # Frequency interpolation grid\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int = 60\n",
    "\n",
    "    # Uncertainty control\n",
    "    SOH_STD_MAX_OOD: float = 2.0\n",
    "\n",
    "    # Train / split settings\n",
    "    TEST_FRAC: float = 0.2\n",
    "    GROUP_KFOLDS: int = 0\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    # PCA toggles\n",
    "    USE_PCA_SOC: bool = False  # <-- tree model (HGB) doesn't need PCA; default off\n",
    "    USE_PCA_SOH: bool = False\n",
    "    PCA_SOC_COMPONENTS: int = 25\n",
    "    PCA_SOH_COMPONENTS: int = 30\n",
    "\n",
    "    # Feature group toggles\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    # DRT params\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    # Capacity-based refinement\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "\n",
    "    # SoH modeling\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    INCLUDE_NORMALIZED_SHAPE_MODEL: bool = True\n",
    "    ENSEMBLE_SOH: bool = True\n",
    "    NORMALIZE_SHAPE_BY_HF_RE: bool = True\n",
    "\n",
    "    # SoC modeling (no GPR)\n",
    "    SOC_INCLUDE_SHAPE_MODEL: bool = True\n",
    "    SOC_MAX_GPR_TRAIN_SAMPLES: int = 3500  # kept for symmetry; unused now\n",
    "\n",
    "    # RUL parameters\n",
    "    DECISION_SOH_PERCENT: float = 50.0\n",
    "    ILLUSTRATIVE_MIN_SOH: float = 40.0\n",
    "    CPP_ROLLING_WINDOW: int = 5\n",
    "    CPP_MIN_POINTS: int = 6\n",
    "    CPP_FALLBACK: float = 20.0\n",
    "\n",
    "    # Inference extras\n",
    "    TEST_TEMPERATURE_OVERRIDE: Optional[float] = 25.0\n",
    "    FORCE_RETRAIN: bool = False  # retrain anyway (overrides signature check) if True\n",
    "\n",
    "    # Saving / logging\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    VERBOSE: bool = True\n",
    "    FEATURE_VERSION: int = 88  # bump: robust SoC HGB, dual-space OOD, hf/10Hz autoscale\n",
    "\n",
    "    # OOD thresholds (SoH)\n",
    "    MAHAL_THRESHOLD: float = 10.0\n",
    "    GP_ARD_NORM_THRESHOLD: float = 6.0\n",
    "\n",
    "    # Projection curve shape exponent\n",
    "    PLOT_EXPONENT: float = 1.25\n",
    "\n",
    "    # Thresholds to report/plot\n",
    "    TARGET_SOH_THRESHOLDS: Tuple[float, ...] = (80.0, 50.0, 40.0)\n",
    "\n",
    "    # --- SoC OOD / prior settings (no fixed 50 prior) ---\n",
    "    OOD_SOC_ENABLE: bool = True\n",
    "    OOD_SOC_Q: float = 0.999          # quantile for distance threshold\n",
    "    OOD_SOC_SHRINK_SCALE: float = 10.0\n",
    "    OOD_SOC_W_MIN: float = 0.45       # keep decent weight on raw model\n",
    "    SOC_OOD_K: int = 30               # neighbors for KNN prior\n",
    "    SOC_CALIBRATE_ON_OOD: bool = False  # avoid isotonic pull on OOD\n",
    "\n",
    "    # Std caps to keep SoC std realistic\n",
    "    SOC_STD_CAP_MULT_IN: float = 1.00   # in-domain cap ~ val RMSE\n",
    "    SOC_STD_CAP_MULT_OOD: float = 1.20  # OOD cap slightly larger\n",
    "\n",
    "    # Optional: de-discretize labels\n",
    "    SOC_LABEL_JITTER: float = 0.0\n",
    "\n",
    "cfg = Config()\n",
    "if cfg.EIS_TEST_FILES is None:\n",
    "    cfg.EIS_TEST_FILES = [Path(\"Mazda-Battery-Cell9.xlsx\")]\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper to tweak paths easily in notebooks\n",
    "def set_paths(eis_dir: str | Path, cap_dir: str | Path, model_dir: str | Path | None = None):\n",
    "    cfg.EIS_DIR = Path(eis_dir)\n",
    "    cfg.CAP_DIR = Path(cap_dir)\n",
    "    if model_dir is not None:\n",
    "        cfg.MODEL_DIR = Path(model_dir)\n",
    "        cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 2. UTILITIES\n",
    "# =========================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path): return str(x)\n",
    "    if isinstance(x, dict): return {k: to_jsonable(v) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "def config_signature(cfg: Config) -> str:\n",
    "    d = asdict(cfg).copy()\n",
    "    d[\"EIS_DIR\"] = str(d[\"EIS_DIR\"]); d[\"CAP_DIR\"]=str(d[\"CAP_DIR\"]); d[\"MODEL_DIR\"]=str(d[\"MODEL_DIR\"])\n",
    "    d.pop(\"EIS_TEST_FILES\", None)\n",
    "    blob = json.dumps(d, sort_keys=True)\n",
    "    return hashlib.sha256(blob.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "# =========================\n",
    "# 3. REGEX\n",
    "# =========================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. PARSERS\n",
    "# =========================\n",
    "def parse_eis_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"SOC\": float(d[\"SOC\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"RealSOH_file\": int(d[\"RealSOH\"])/100.0\n",
    "    }\n",
    "\n",
    "def parse_cap_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"CycleIndex\": int(d[\"Cycle\"])\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 5. LOW-LEVEL LOADERS / INTERPOLATION\n",
    "# =========================\n",
    "def _find_matrix(mat_dict: dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[1] >= 3 and v.shape[0] >= 10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw, y_raw, freq_target):\n",
    "    freq_raw = np.asarray(freq_raw).astype(float)\n",
    "    y_raw = np.asarray(y_raw).astype(float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw = freq_raw[::-1]; y_raw = y_raw[::-1]\n",
    "    uniq, idx = np.unique(freq_raw, return_index=True)\n",
    "    if len(uniq) != len(freq_raw):\n",
    "        order = np.argsort(idx)\n",
    "        freq_raw = uniq[order]; y_raw = y_raw[idx][order]\n",
    "    f = interp1d(freq_raw, y_raw, bounds_error=False,\n",
    "                 fill_value=(y_raw[0], y_raw[-1]), kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "FREQ_CANDS = [\"frequency\",\"freq\",\"f\",\"hz\",\"frequency(hz)\",\"Frequency(Hz)\"]\n",
    "RE_CANDS   = [\"zreal\",\"re(z)\",\"re\",\"real\",\"z_re\",\"zreal(ohm)\",\"re (ohm)\",\"re(z) (ohm)\",\"Zreal\",\"Zreal (ohm)\",\"Zreal(ohm)\"]\n",
    "IM_CANDS   = [\"-zimag\",\"zimag\",\"im(z)\",\"im\",\"imag\",\"imaginary\",\"z_im\",\"zimg\",\"z_imag\",\" -Zimag (ohm)\",\" -Zimag(ohm)\",\"-Zimag\",\"Zimag\",\"Zimag (ohm)\"]\n",
    "\n",
    "def _select_column(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c.lower() in low: return low[c.lower()]\n",
    "    for c in cands:\n",
    "        for col in df.columns:\n",
    "            if c.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def load_mat_eis(path: Path):\n",
    "    mat = loadmat(path)\n",
    "    arr = _find_matrix(mat)\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"No valid EIS matrix in {path.name}\")\n",
    "    return arr[:,0].astype(float), arr[:,1].astype(float), arr[:,2].astype(float)\n",
    "\n",
    "def load_table_eis(path: Path):\n",
    "    # returns (freq, re, im, used_freq_from_file: bool)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty table.\")\n",
    "    fcol = _select_column(df, FREQ_CANDS)\n",
    "    recol = _select_column(df, RE_CANDS)\n",
    "    imcol = _select_column(df, IM_CANDS)\n",
    "    if recol is None or imcol is None:\n",
    "        raise ValueError(f\"Missing Re/Im columns in {path.name}\")\n",
    "    re_vals = pd.to_numeric(df[recol], errors=\"coerce\").to_numpy()\n",
    "    im_vals = pd.to_numeric(df[imcol], errors=\"coerce\").to_numpy()\n",
    "    used_freq = True\n",
    "    if fcol is not None:\n",
    "        freq_vals = pd.to_numeric(df[fcol], errors=\"coerce\").to_numpy()\n",
    "    else:\n",
    "        used_freq = False\n",
    "        n = min(len(re_vals), len(im_vals))\n",
    "        freq_vals = np.geomspace(cfg.F_MAX, cfg.F_MIN, n)\n",
    "    n = min(len(freq_vals), len(re_vals), len(im_vals))\n",
    "    freq_vals = freq_vals[:n]; re_vals = re_vals[:n]; im_vals = im_vals[:n]\n",
    "    if np.nanmean(im_vals) > 0:\n",
    "        im_vals = -im_vals\n",
    "    return freq_vals.astype(float), re_vals.astype(float), im_vals.astype(float), used_freq\n",
    "\n",
    "def load_any_inference(path: Path):\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".mat\":\n",
    "        f,r,i = load_mat_eis(path); used=True\n",
    "    elif suf in (\".csv\",\".xls\",\".xlsx\"):\n",
    "        f,r,i,used = load_table_eis(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported test file extension: {suf}\")\n",
    "    return f,r,i,used\n",
    "\n",
    "# =========================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# =========================\n",
    "def compute_F_features(freq, re_i, im_i):\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    F1 = re_i[0]; F2 = re_i[idx_peak]; F3 = re_i[-1]\n",
    "    sc = np.where(np.sign(im_i[:-1]) != np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k = sc[0]; y0,y1 = im_i[k], im_i[k+1]\n",
    "        w = -y0/(y1 - y0 + 1e-12)\n",
    "        F4 = re_i[k] + w*(re_i[k+1]-re_i[k])\n",
    "    else:\n",
    "        F4 = np.nan\n",
    "    F5 = (re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6 = np.min(im_i)\n",
    "    mid_target = 10.0\n",
    "    idx_mid = int(np.argmin(np.abs(freq-mid_target)))\n",
    "    F7 = re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES = [\n",
    "    \"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "    \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"\n",
    "]\n",
    "\n",
    "def physical_features(freq, re_i, im_i):\n",
    "    freq = np.asarray(freq); re_i = np.asarray(re_i); im_i = np.asarray(im_i)\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    Rs = float(re_i[0]); Rpeak = float(re_i[idx_peak]); Rlow = float(re_i[-1])\n",
    "    Rct = max(Rpeak - Rs, 0.0)\n",
    "    arc_diam = Rlow - Rs\n",
    "    norm_arc = arc_diam / (Rs + 1e-9)\n",
    "    f_peak = float(freq[idx_peak])\n",
    "    tau_peak = 1.0/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K = min(10, len(freq)//3)\n",
    "    if K >= 4:\n",
    "        w_section = (2*np.pi*freq[-K:])**(-0.5)\n",
    "        re_section = re_i[-K:]\n",
    "        if len(np.unique(w_section)) > 2:\n",
    "            warburg_sigma = float(np.polyfit(w_section, re_section, 1)[0])\n",
    "        else:\n",
    "            warburg_sigma = np.nan\n",
    "    else:\n",
    "        warburg_sigma = np.nan\n",
    "    phase = np.arctan2(-im_i, re_i)\n",
    "    mid_mask = (freq>=1) & (freq<=100)\n",
    "    if mid_mask.sum()>2:\n",
    "        phase_mean_mid = float(phase[mid_mask].mean())\n",
    "        phase_std_mid  = float(phase[mid_mask].std())\n",
    "    else:\n",
    "        phase_mean_mid = np.nan; phase_std_mid = np.nan\n",
    "    phase_min = float(phase.min())\n",
    "    lf_mask = (freq<=1.0)\n",
    "    if lf_mask.sum() >= 4:\n",
    "        x = np.log10(freq[lf_mask]+1e-12); y = neg_im[lf_mask]\n",
    "        lf_slope = np.polyfit(x, y, 1)[0]\n",
    "    else:\n",
    "        lf_slope = np.nan\n",
    "    arc_quality = (neg_im.max() - neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS = [(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq, re_i, im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m], im_i[m])\n",
    "            feats += [z.mean(), z.std()]\n",
    "        else:\n",
    "            feats += [np.nan, np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq, re_i, im_i, segments=5):\n",
    "    logf = np.log10(freq)\n",
    "    edges = np.linspace(logf.min(), logf.max(), segments+1)\n",
    "    out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]\n",
    "            out += [np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else:\n",
    "            out += [np.nan, np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES = [\n",
    "    \"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "    \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"\n",
    "]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w = 2*np.pi*freq\n",
    "    tau = np.geomspace(tau_max, tau_min, n_tau)\n",
    "    WT = w[:,None]*tau[None,:]\n",
    "    denom = 1+WT**2\n",
    "    K_re = 1.0/denom\n",
    "    K_im = -WT/denom\n",
    "    R_inf = re_i[0]\n",
    "    y_re = re_i - R_inf\n",
    "    y_im = im_i\n",
    "    Y = np.concatenate([y_re, y_im])\n",
    "    K = np.vstack([K_re, K_im])\n",
    "    A = K.T @ K + lam*np.eye(n_tau)\n",
    "    b = K.T @ Y\n",
    "    gamma = linalg.solve(A,b,assume_a='pos')\n",
    "    gamma = np.clip(gamma,0,None)\n",
    "    return tau, gamma\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    try:\n",
    "        tau,gamma = compute_drt(freq,re_i,im_i,\n",
    "                                 cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,\n",
    "                                 cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau = np.log10(tau)\n",
    "        g_sum = gamma.sum()+1e-12\n",
    "        w_norm = gamma/g_sum\n",
    "        mean_logtau = float((w_norm*log_tau).sum())\n",
    "        var_logtau  = float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p = int(np.argmax(gamma))\n",
    "        peak_tau = float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid = np.median(log_tau)\n",
    "        frac_low = float(w_norm[log_tau<=mid].sum())\n",
    "        frac_high = 1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception:\n",
    "        return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i, im_i, temp, freq, include_names=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts += [re_i, im_i]\n",
    "        names += [f\"Re_{i}\" for i in range(len(re_i))] + [f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z = np.hypot(re_i, im_i)\n",
    "        basics=[re_i[0], re_i[-1], re_i[-1]-re_i[0], z.max(), z.mean(), z.std()]\n",
    "        parts.append(np.array(basics)); names += [\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        Ff=compute_F_features(freq,re_i,im_i); parts.append(np.array(Ff)); names += [f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        Pf=physical_features(freq,re_i,im_i); parts.append(np.array(Pf)); names += PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        Bf=band_stats(freq,re_i,im_i); parts.append(np.array(Bf))\n",
    "        for bi in range(len(BANDS)): names += [f\"band{bi}_mean\", f\"band{bi}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i); parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2): names += [f\"slope_re_seg{i}\", f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        Df=drt_features(freq,re_i,im_i); parts.append(np.array(Df)); names += DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names += [\"Feat_Temp\"]\n",
    "    vec = np.concatenate(parts).astype(float)\n",
    "    vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if include_names: return vec, names\n",
    "    return vec\n",
    "\n",
    "def build_shape_normalized(re_i, im_i, k: int = 5):\n",
    "    hf = float(np.nanmedian(re_i[:max(1, min(k, len(re_i)))]))\n",
    "    if not np.isfinite(hf) or abs(hf) < 1e-9:\n",
    "        hf = 1.0\n",
    "    return re_i / hf, im_i / hf\n",
    "\n",
    "# =========================\n",
    "# 7. CAPACITY & CPP\n",
    "# =========================\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "    - If a plain 2-D numeric matrix exists, use it (original behavior).\n",
    "    - Else, try MATLAB struct with cumulative 'AhAccu' or 'WhAccu' arrays.\n",
    "    \"\"\"\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY):\n",
    "        return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta = parse_cap_metadata(fp.stem)\n",
    "        if not meta:\n",
    "            continue\n",
    "        try:\n",
    "            mat = loadmat(fp, squeeze_me=True, struct_as_record=False)\n",
    "\n",
    "            # Old path: plain 2-D numeric matrix\n",
    "            arr = _find_matrix(mat)\n",
    "            cap = None\n",
    "            if arr is not None:\n",
    "                col = np.argmax(np.abs(arr[-50:, :]).mean(axis=0))\n",
    "                cap = float(np.nanmax(arr[:, col]))\n",
    "            else:\n",
    "                # New path: MATLAB struct with cumulative capacity\n",
    "                d = mat.get(\"data\", None)\n",
    "                if d is not None:\n",
    "                    def _cell_to_1d(x):\n",
    "                        a = np.array(x, dtype=object).squeeze()\n",
    "                        out=[]\n",
    "                        for e in a.flat:\n",
    "                            if isinstance(e, np.ndarray):\n",
    "                                out.append(float(np.nanmax(e.astype(float))) if e.size else np.nan)\n",
    "                            else:\n",
    "                                try: out.append(float(e))\n",
    "                                except Exception: out.append(np.nan)\n",
    "                        z = np.array(out, dtype=float)\n",
    "                        if z.ndim == 0: z = z[None]\n",
    "                        return z\n",
    "\n",
    "                    if hasattr(d, \"AhAccu\"):\n",
    "                        v = _cell_to_1d(getattr(d, \"AhAccu\"))\n",
    "                        if v.size: cap = float(np.nanmax(v))\n",
    "                    if cap is None and hasattr(d, \"WhAccu\"):\n",
    "                        v = _cell_to_1d(getattr(d, \"WhAccu\"))\n",
    "                        if v.size: cap = float(np.nanmax(v) / 3.7)  # approx Ah from Wh\n",
    "\n",
    "            if cap is None or not np.isfinite(cap):\n",
    "                continue\n",
    "\n",
    "            meta[\"MeasuredCapacity_Ah\"] = cap\n",
    "            recs.append(meta)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(recs)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    ref = df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"] = df[\"MeasuredCapacity_Ah\"] / ref\n",
    "    df[\"SoH_percent\"] = df[\"NormCapacity\"] * 100.0\n",
    "    return df\n",
    "\n",
    "def estimate_cpp_per_cell(capacity_df: pd.DataFrame,\n",
    "                          window:int, min_points:int)->Dict[str,float]:\n",
    "    cpp={}\n",
    "    for cid,grp in capacity_df.groupby(\"CellID\"):\n",
    "        g=grp.sort_values(\"CycleIndex\")\n",
    "        if g.shape[0]<min_points: continue\n",
    "        tail=g.tail(window)\n",
    "        x=tail[\"CycleIndex\"].values.astype(float)\n",
    "        y=tail[\"SoH_percent\"].values.astype(float)\n",
    "        if len(np.unique(x))<2: continue\n",
    "        slope=np.polyfit(x,y,1)[0]\n",
    "        if slope >= -1e-6:\n",
    "            continue\n",
    "        cpp[cid]=1.0/abs(slope)\n",
    "    return cpp\n",
    "\n",
    "def build_cpp_map(cap_df: pd.DataFrame):\n",
    "    if cap_df.empty: return {}, cfg.CPP_FALLBACK\n",
    "    cpp_map=estimate_cpp_per_cell(\n",
    "        cap_df[[\"CellID\",\"CycleIndex\",\"SoH_percent\"]],\n",
    "        cfg.CPP_ROLLING_WINDOW, cfg.CPP_MIN_POINTS\n",
    "    )\n",
    "    if not cpp_map:\n",
    "        return {}, cfg.CPP_FALLBACK\n",
    "    return cpp_map, float(np.median(list(cpp_map.values())))\n",
    "\n",
    "def get_cpp(meta: dict, cpp_map: Dict[str,float], global_cpp: float):\n",
    "    if not meta: return global_cpp\n",
    "    return cpp_map.get(meta.get(\"CellID\"), global_cpp)\n",
    "\n",
    "# =========================\n",
    "# 8. DATASET BUILD\n",
    "# =========================\n",
    "def load_single_eis_mat(fp: Path):\n",
    "    meta = parse_eis_metadata(fp.stem)\n",
    "    if meta is None:\n",
    "        raise ValueError(f\"Bad filename: {fp.name}\")\n",
    "    freq,re_z,im_z = load_mat_eis(fp)\n",
    "    re_i=_interp_channel(freq, re_z, CANON_FREQ)\n",
    "    im_i=_interp_channel(freq, im_z, CANON_FREQ)\n",
    "    vec=build_feature_vector(re_i, im_i, meta[\"Temp\"], CANON_FREQ)\n",
    "    return vec, meta, re_i, im_i\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_df: Optional[pd.DataFrame]):\n",
    "    files = sorted(eis_dir.rglob(\"*.mat\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .mat spectra in {eis_dir}\")\n",
    "\n",
    "    f0,r0,i0 = load_mat_eis(files[0])\n",
    "    re0=_interp_channel(f0,r0,CANON_FREQ); im0=_interp_channel(f0,i0,CANON_FREQ)\n",
    "    _, feature_names = build_feature_vector(re0, im0, 25.0, CANON_FREQ, include_names=True)\n",
    "\n",
    "    feats=[]; rows=[]; shape_feats=[]\n",
    "    for fp in tqdm(files, desc=\"Loading training spectra\"):\n",
    "        try:\n",
    "            v, m, rei, imi = load_single_eis_mat(fp)\n",
    "            feats.append(v); rows.append(m)\n",
    "            if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL or cfg.SOC_INCLUDE_SHAPE_MODEL) and cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "                rsh, ish = build_shape_normalized(rei, imi)\n",
    "                shape_vec = build_feature_vector(rsh, ish, m[\"Temp\"], CANON_FREQ)\n",
    "                shape_feats.append(shape_vec)\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip] {fp.name}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid training spectra after filtering.\")\n",
    "\n",
    "    X = np.vstack(feats)\n",
    "    X_shape = np.vstack(shape_feats) if shape_feats else None\n",
    "    meta_df = pd.DataFrame(rows)\n",
    "\n",
    "    # SoH refinement\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup = cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        refined=[]\n",
    "        for cid, stage, fallback in zip(meta_df.CellID, meta_df.SOH_stage, meta_df.RealSOH_file):\n",
    "            nc = lookup.get((cid, stage))\n",
    "            refined.append(100.0*nc if nc is not None else fallback)\n",
    "        meta_df[\"SoH_cont\"]=refined\n",
    "    else:\n",
    "        meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "\n",
    "    # Targets\n",
    "    y_soc = meta_df[\"SOC\"].astype(float).values\n",
    "    y_soh = meta_df[\"SoH_cont\"].values\n",
    "\n",
    "    soh_var = float(np.var(y_soh))\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[DATA] SoH range: {y_soh.min():.2f} – {y_soh.max():.2f} (var={soh_var:.3f})\")\n",
    "        if soh_var < 1.0:\n",
    "            print(\"[WARN] Low SoH variance → model may output near-constant SoH.\")\n",
    "\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        pd.concat(\n",
    "            [meta_df.reset_index(drop=True),\n",
    "             pd.DataFrame(X, columns=feature_names)], axis=1\n",
    "        ).to_parquet(cfg.MODEL_DIR/\"training_features.parquet\", index=False)\n",
    "\n",
    "    # Save anchors for autoscale\n",
    "    try:\n",
    "        idx_hf  = feature_names.index(\"hf_re\")\n",
    "    except ValueError:\n",
    "        idx_hf = None\n",
    "    try:\n",
    "        idx_f7 = feature_names.index(\"F7\")\n",
    "    except ValueError:\n",
    "        idx_f7 = None\n",
    "\n",
    "    train_hf_median = float(np.median(X[:, idx_hf])) if idx_hf is not None else None\n",
    "    train_f7_median = float(np.median(X[:, idx_f7])) if idx_f7 is not None else None\n",
    "\n",
    "    return meta_df, X, (X_shape, feature_names, train_hf_median, train_f7_median), y_soc, y_soh\n",
    "\n",
    "# =========================\n",
    "# 9. SPLITTING\n",
    "# =========================\n",
    "def cell_split_mask(meta_df: pd.DataFrame):\n",
    "    cells = meta_df.CellID.unique()\n",
    "    rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test = max(1, int(len(cells)*cfg.TEST_FRAC))\n",
    "    test_cells = rng.choice(cells, size=n_test, replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "# =========================\n",
    "# 10. TRAINING\n",
    "# =========================\n",
    "def _fit_hgb(X, y, seed):\n",
    "    # Solid defaults; robust on small/medium tabular sets\n",
    "    return HistGradientBoostingRegressor(\n",
    "        learning_rate=0.06,\n",
    "        max_iter=650,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=12,\n",
    "        l2_regularization=1e-3,\n",
    "        random_state=seed\n",
    "    ).fit(X, y)\n",
    "\n",
    "def train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh):\n",
    "    X_shape, feature_names, train_hf_median, train_f7_median = shape_bundle\n",
    "    mask_test = cell_split_mask(meta_df)\n",
    "\n",
    "    # ----- SoC (NO GPR) -----\n",
    "    soc_scaler = StandardScaler()\n",
    "    X_soc_s = soc_scaler.fit_transform(X_raw)\n",
    "    soc_pca = None\n",
    "    X_soc_model = X_soc_s\n",
    "    if cfg.USE_PCA_SOC:\n",
    "        soc_pca = PCA(n_components=min(cfg.PCA_SOC_COMPONENTS, X_soc_s.shape[1]-1),\n",
    "                      random_state=cfg.RANDOM_STATE)\n",
    "        X_soc_model = soc_pca.fit_transform(X_soc_s)\n",
    "\n",
    "    # Optional: slight jitter to de-discretize SoC labels\n",
    "    y_soc_train_all = y_soc.copy()\n",
    "    if cfg.SOC_LABEL_JITTER and cfg.SOC_LABEL_JITTER > 0:\n",
    "        rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "        y_soc_train_all = np.clip(y_soc_train_all + rng.normal(0.0, cfg.SOC_LABEL_JITTER, size=y_soc_train_all.shape), 0.0, 100.0)\n",
    "\n",
    "    soc_candidates = {}\n",
    "\n",
    "    # HGB on raw embedding\n",
    "    soc_hgb_raw = _fit_hgb(X_soc_model[~mask_test], y_soc_train_all[~mask_test], cfg.RANDOM_STATE)\n",
    "    pred_soc_hgb = soc_hgb_raw.predict(X_soc_model[mask_test])\n",
    "    r2_soc_hgb = r2_score(y_soc[mask_test], pred_soc_hgb)\n",
    "    rmse_soc_hgb = math.sqrt(mean_squared_error(y_soc[mask_test], pred_soc_hgb))\n",
    "    soc_candidates[\"soc_hgb_raw\"] = (soc_hgb_raw, r2_soc_hgb, rmse_soc_hgb)\n",
    "\n",
    "    # HGB on shape-normalized embedding (if available)\n",
    "    soc_shape_model = soc_shape_scaler = soc_shape_pca = None\n",
    "    soc_shape_metrics = None\n",
    "    if cfg.SOC_INCLUDE_SHAPE_MODEL and (X_shape is not None):\n",
    "        soc_shape_scaler = StandardScaler()\n",
    "        Xs = soc_shape_scaler.fit_transform(X_shape)\n",
    "        if cfg.USE_PCA_SOC:\n",
    "            soc_shape_pca = PCA(n_components=min(cfg.PCA_SOC_COMPONENTS, Xs.shape[1]-1),\n",
    "                                random_state=cfg.RANDOM_STATE)\n",
    "            Xs_model = soc_shape_pca.fit_transform(Xs)\n",
    "        else:\n",
    "            Xs_model = Xs\n",
    "        soc_hgb_shape = _fit_hgb(Xs_model[~mask_test], y_soc_train_all[~mask_test], cfg.RANDOM_STATE)\n",
    "        sp = soc_hgb_shape.predict(Xs_model[mask_test])\n",
    "        r2_soc_shape = r2_score(y_soc[mask_test], sp)\n",
    "        rmse_soc_shape = math.sqrt(mean_squared_error(y_soc[mask_test], sp))\n",
    "        soc_candidates[\"soc_hgb_shape\"] = (soc_hgb_shape, r2_soc_shape, rmse_soc_shape)\n",
    "        soc_shape_model = soc_hgb_shape\n",
    "        soc_shape_metrics = {\"r2\": r2_soc_shape, \"rmse\": rmse_soc_shape}\n",
    "    else:\n",
    "        Xs_model = None\n",
    "\n",
    "    # Choose best SoC model by R^2\n",
    "    soc_best_name = max(soc_candidates.keys(), key=lambda k: soc_candidates[k][1])\n",
    "    soc_best_model, soc_best_r2, soc_best_rmse = soc_candidates[soc_best_name]\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoC] HGB_raw:   R2={r2_soc_hgb:.3f} RMSE={rmse_soc_hgb:.2f}\")\n",
    "        if soc_shape_metrics:\n",
    "            print(f\"[SoC] HGB_shape: R2={soc_shape_metrics['r2']:.3f} RMSE={soc_shape_metrics['rmse']:.2f}\")\n",
    "        print(f\"[SoC] Selected = {soc_best_name}\")\n",
    "\n",
    "    # ----- SoH (unchanged core: GPR tends to be best here) -----\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "    soh_scaler = StandardScaler()\n",
    "    X_soh_s = soh_scaler.fit_transform(X_raw)\n",
    "    soh_pca=None\n",
    "    X_soh_model = X_soh_s\n",
    "    if cfg.USE_PCA_SOH:\n",
    "        soh_pca = PCA(n_components=min(cfg.PCA_SOH_COMPONENTS, X_soh_s.shape[1]-1),\n",
    "                      random_state=cfg.RANDOM_STATE)\n",
    "        X_soh_model = soh_pca.fit_transform(X_soh_s)\n",
    "\n",
    "    soh_candidates = {}\n",
    "    dim = X_soh_model.shape[1]\n",
    "    kernel = RBF(length_scale=np.ones(dim)*3.0,\n",
    "                 length_scale_bounds=(1e-1,1e6)) + \\\n",
    "             WhiteKernel(noise_level=1e-2,\n",
    "                         noise_level_bounds=(1e-6,1e-1))\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel, alpha=0.0, normalize_y=True,\n",
    "        random_state=cfg.RANDOM_STATE, n_restarts_optimizer=3\n",
    "    )\n",
    "    if X_soh_model.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "        idx = np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "            X_soh_model.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "        gpr.fit(X_soh_model[idx], y_soh[idx])\n",
    "    else:\n",
    "        gpr.fit(X_soh_model, y_soh)\n",
    "    pred_gpr = gpr.predict(X_soh_model[mask_test])\n",
    "    r2_gpr = r2_score(y_soh[mask_test], pred_gpr)\n",
    "    rmse_gpr = math.sqrt(mean_squared_error(y_soh[mask_test], pred_gpr))\n",
    "    soh_candidates[\"gpr_raw\"] = (gpr, r2_gpr, rmse_gpr)\n",
    "\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_iter=500,\n",
    "        l2_regularization=1e-3, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    hgb.fit(X_soh_model[~mask_test], y_soh[~mask_test])\n",
    "    pred_hgb = hgb.predict(X_soh_model[mask_test])\n",
    "    r2_hgb = r2_score(y_soh[mask_test], pred_hgb)\n",
    "    rmse_hgb = math.sqrt(mean_squared_error(y_soh[mask_test], pred_hgb))\n",
    "    soh_candidates[\"hgb_raw\"] = (hgb, r2_hgb, rmse_hgb)\n",
    "\n",
    "    shape_model=shape_scaler=shape_pca=None\n",
    "    shape_metrics=None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and (X_shape is not None):\n",
    "        shape_scaler = StandardScaler()\n",
    "        X_shape_s = shape_scaler.fit_transform(X_shape)\n",
    "        X_shape_model = X_shape_s\n",
    "        if cfg.USE_PCA_SOH:\n",
    "            shape_pca = PCA(n_components=min(cfg.PCA_SOH_COMPONENTS, X_shape_s.shape[1]-1),\n",
    "                            random_state=cfg.RANDOM_STATE)\n",
    "            X_shape_model = shape_pca.fit_transform(X_shape_s)\n",
    "\n",
    "        # Build a brand-new GPR matched to the shape-space dimension (fixes kernel__k1 param leak)\n",
    "        dim_shape = X_shape_model.shape[1]\n",
    "        kernel_shape = (\n",
    "            RBF(length_scale=np.ones(dim_shape) * 3.0,\n",
    "                length_scale_bounds=(1e-1, 1e6))\n",
    "            + WhiteKernel(noise_level=1e-2,\n",
    "                          noise_level_bounds=(1e-6, 1e-1))\n",
    "        )\n",
    "        shape_model = GaussianProcessRegressor(\n",
    "            kernel=kernel_shape,\n",
    "            alpha=0.0,\n",
    "            normalize_y=True,\n",
    "            random_state=cfg.RANDOM_STATE,\n",
    "            n_restarts_optimizer=3\n",
    "        )\n",
    "\n",
    "        # Fit new GPR for shape space\n",
    "        if X_shape_model.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "            idx = np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "                X_shape_model.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "            shape_model.fit(X_shape_model[idx], y_soh[idx])\n",
    "        else:\n",
    "            shape_model.fit(X_shape_model, y_soh)\n",
    "\n",
    "        spred = shape_model.predict(X_shape_model[mask_test])\n",
    "        r2_shape = r2_score(y_soh[mask_test], spred)\n",
    "        rmse_shape = math.sqrt(mean_squared_error(y_soh[mask_test], spred))\n",
    "        soh_candidates[\"gpr_shape\"] = (shape_model, r2_shape, rmse_shape)\n",
    "        shape_metrics = {\"r2\": r2_shape, \"rmse\": rmse_shape}\n",
    "\n",
    "    best_name = max([\"gpr_raw\",\"hgb_raw\"], key=lambda k: soh_candidates[k][1])\n",
    "    best_model, best_r2, best_rmse = soh_candidates[best_name]\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoH] GPR_raw:  R2={r2_gpr:.3f} RMSE={rmse_gpr:.2f}\")\n",
    "        print(f\"[SoH] HGB_raw: R2={r2_hgb:.3f} RMSE={rmse_hgb:.2f}\")\n",
    "        if shape_metrics:\n",
    "            print(f\"[SoH] ShapeGP: R2={shape_metrics['r2']:.3f} RMSE={shape_metrics['rmse']:.2f}\")\n",
    "        print(f\"[SoH] Selected raw model = {best_name}\")\n",
    "\n",
    "    # --- Robust covariance (Ledoit–Wolf) for OOD ---\n",
    "    def _lw_cov_inv(X):\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(X)\n",
    "            cov = lw.covariance_\n",
    "            return np.linalg.pinv(cov), X.mean(axis=0)\n",
    "        except Exception:\n",
    "            return np.eye(X.shape[1]), X.mean(axis=0)\n",
    "\n",
    "    # SoH-space OOD (kept for SoH uncertainty capping)\n",
    "    cov_inv_soh, center_soh = _lw_cov_inv(X_soh_s)\n",
    "\n",
    "    # SoC-space OOD stats (raw embed)\n",
    "    soc_cov_inv_raw, soc_center_raw = _lw_cov_inv(X_soc_model[~mask_test])\n",
    "    d_raw = []\n",
    "    X_tr_raw = X_soc_model[~mask_test]\n",
    "    for i in range(X_tr_raw.shape[0]):\n",
    "        diff = X_tr_raw[i] - soc_center_raw\n",
    "        d_raw.append(float(np.sqrt(diff @ soc_cov_inv_raw @ diff.T)))\n",
    "    thr_raw = float(np.quantile(d_raw, cfg.OOD_SOC_Q)) if len(d_raw) else None\n",
    "\n",
    "    # SoC-shape-space OOD stats (use the model's own embedding)\n",
    "    soc_cov_inv_shape = None; soc_center_shape = None; thr_shape = None\n",
    "    X_tr_shape = None\n",
    "    if cfg.SOC_INCLUDE_SHAPE_MODEL and (soc_shape_model is not None):\n",
    "        X_tr_shape = Xs_model[~mask_test]  # same space as the trained shape model\n",
    "        if X_tr_shape.size:\n",
    "            soc_cov_inv_shape, soc_center_shape = _lw_cov_inv(X_tr_shape)\n",
    "            d_shape=[]\n",
    "            for i in range(X_tr_shape.shape[0]):\n",
    "                diff = X_tr_shape[i] - soc_center_shape\n",
    "                d_shape.append(float(np.sqrt(diff @ soc_cov_inv_shape @ diff.T)))\n",
    "            thr_shape = float(np.quantile(d_shape, cfg.OOD_SOC_Q)) if len(d_shape) else None\n",
    "\n",
    "    # SoC calibrator (fit on holdout predictions of the selected model)\n",
    "    if soc_best_name == \"soc_hgb_raw\":\n",
    "        val_pred = soc_hgb_raw.predict(X_soc_model[mask_test])\n",
    "    else:\n",
    "        val_pred = soc_candidates[\"soc_hgb_shape\"][0].predict(Xs_model[mask_test])\n",
    "\n",
    "    soc_calibrator = IsotonicRegression(y_min=0.0, y_max=100.0, out_of_bounds=\"clip\")\n",
    "    soc_calibrator.fit(val_pred, y_soc[mask_test])\n",
    "\n",
    "    bundle = {\n",
    "        # SoC\n",
    "        \"soc_scaler\": soc_scaler,\n",
    "        \"soc_pca\": soc_pca,\n",
    "        \"soc_model\": soc_best_model,\n",
    "        \"soc_model_name\": soc_best_name,\n",
    "\n",
    "        # shape-based SoC path\n",
    "        \"soc_shape_scaler\": None if soc_shape_model is None else soc_shape_scaler,\n",
    "        \"soc_shape_pca\": None if soc_shape_model is None else soc_shape_pca,\n",
    "        \"soc_shape_model\": soc_shape_model,\n",
    "\n",
    "        \"soc_calibrator\": soc_calibrator,\n",
    "\n",
    "        # Embeddings for KNN prior (both)\n",
    "        \"soc_train_embed_raw\": X_tr_raw,\n",
    "        \"soc_train_y_raw\": y_soc[~mask_test],\n",
    "        \"soc_train_embed_shape\": X_tr_shape,\n",
    "        \"soc_train_y_shape\": y_soc[~mask_test] if X_tr_shape is not None else None,\n",
    "\n",
    "        # SoH\n",
    "        \"soh_scaler\": soh_scaler,\n",
    "        \"soh_pca\": soh_pca,\n",
    "        \"soh_model\": best_model,\n",
    "        \"soh_model_name\": best_name,\n",
    "\n",
    "        # Optional SoH shape model\n",
    "        \"shape_scaler\": shape_scaler,\n",
    "        \"shape_pca\": shape_pca,\n",
    "        \"shape_model\": shape_model,\n",
    "\n",
    "        # Meta\n",
    "        \"freq_grid\": CANON_FREQ,\n",
    "        \"feature_version\": cfg.FEATURE_VERSION,\n",
    "        \"feature_manifest\": feature_names,\n",
    "        \"config_signature\": config_signature(cfg),\n",
    "        \"config\": to_jsonable(asdict(cfg)),\n",
    "        \"metrics\": {\n",
    "            \"soc_r2_selected\": soc_best_r2,\n",
    "            \"soc_rmse_selected\": soc_best_rmse,\n",
    "            \"soh_r2_selected\": best_r2,\n",
    "            \"soh_rmse_selected\": best_rmse\n",
    "        },\n",
    "        \"soc_candidates_metrics\": {\n",
    "            \"soc_hgb_raw\": {\"r2\": r2_soc_hgb, \"rmse\": rmse_soc_hgb},\n",
    "            \"soc_hgb_shape\": soc_shape_metrics\n",
    "        },\n",
    "        \"soh_candidates_metrics\": {\n",
    "            \"gpr_raw\": {\"r2\": r2_gpr, \"rmse\": rmse_gpr},\n",
    "            \"hgb_raw\": {\"r2\": r2_hgb, \"rmse\": rmse_hgb},\n",
    "            \"gpr_shape\": shape_metrics\n",
    "        },\n",
    "        # SoH-space OOD stats\n",
    "        \"train_mahal\": {\"center\": center_soh.tolist(), \"cov_inv\": cov_inv_soh.tolist()},\n",
    "        # SoC OOD stats (dual)\n",
    "        \"soc_train_mahal_raw\": {\n",
    "            \"center\": soc_center_raw.tolist(),\n",
    "            \"cov_inv\": soc_cov_inv_raw.tolist(),\n",
    "            \"threshold\": thr_raw\n",
    "        },\n",
    "        \"soc_train_mahal_shape\": None if soc_center_shape is None else {\n",
    "            \"center\": soc_center_shape.tolist(),\n",
    "            \"cov_inv\": soc_cov_inv_shape.tolist(),\n",
    "            \"threshold\": thr_shape\n",
    "        },\n",
    "        # autoscale anchors\n",
    "        \"train_hf_re_median\": train_hf_median,\n",
    "        \"train_f7_median\": train_f7_median,\n",
    "    }\n",
    "    out_path = cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    joblib.dump(bundle, out_path)\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[MODEL] Saved bundle → {out_path}\")\n",
    "        print(json.dumps(bundle[\"metrics\"], indent=2))\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 11. LOAD (WITH LEGACY + SIGNATURE CHECK)\n",
    "# =========================\n",
    "def load_bundle():\n",
    "    path = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "    bundle = joblib.load(path)\n",
    "\n",
    "    legacy = (\"scaler\" in bundle) and (\"soc_scaler\" not in bundle)\n",
    "    if legacy:\n",
    "        scaler = bundle[\"scaler\"]\n",
    "        pca = bundle.get(\"pca\")\n",
    "        soc_model = bundle.get(\"soc_model\")\n",
    "        soh_model = bundle.get(\"soh_model\")\n",
    "        bundle[\"soc_scaler\"] = scaler\n",
    "        bundle[\"soh_scaler\"] = scaler\n",
    "        bundle[\"soc_pca\"]    = pca\n",
    "        bundle[\"soh_pca\"]    = pca\n",
    "        bundle[\"soh_model\"]  = soh_model\n",
    "        bundle[\"soh_model_name\"] = bundle.get(\"soh_model_name\",\"legacy_model\")\n",
    "        bundle[\"soc_model_name\"] = bundle.get(\"soc_model_name\",\"legacy_soc_model\")\n",
    "        bundle.setdefault(\"metrics\", {\"soh_rmse_selected\":5.0, \"soc_rmse_selected\":8.0})\n",
    "        if \"train_mahal\" not in bundle:\n",
    "            try:\n",
    "                center = scaler.mean_\n",
    "                cov_inv = np.eye(len(center))\n",
    "                bundle[\"train_mahal\"] = {\"center\": center.tolist(), \"cov_inv\": cov_inv.tolist()}\n",
    "            except Exception:\n",
    "                bundle[\"train_mahal\"] = None\n",
    "        bundle.setdefault(\"feature_version\", -1)\n",
    "\n",
    "    for key in [\"soc_scaler\",\"soc_model\",\"soh_scaler\",\"soh_model\",\"freq_grid\"]:\n",
    "        if key not in bundle:\n",
    "            raise KeyError(f\"Bundle missing required key: {key}\")\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 12. INFERENCE FEATURIZATION\n",
    "# =========================\n",
    "def featurize_any(file_path: Path, bundle):\n",
    "    freq_grid = bundle[\"freq_grid\"]\n",
    "    meta = parse_eis_metadata(file_path.stem)\n",
    "    freq,re_raw,im_raw, used_freq = load_any_inference(file_path)\n",
    "    if not used_freq:\n",
    "        warnings.warn(f\"[{file_path.name}] No frequency column found. Using geometric grid fallback.\")\n",
    "    re_i=_interp_channel(freq, re_raw, freq_grid)\n",
    "    im_i=_interp_channel(freq, im_raw, freq_grid)\n",
    "\n",
    "    # ---- AUTO UNIT ALIGNMENT via hf_re and F7 (10 Hz) ----\n",
    "    autoscale_factor = 1.0\n",
    "    hf_test = float(np.nanmedian(re_i[:max(1, min(5, len(re_i)))]))\n",
    "    # F7 ~ 10 Hz (match how we built it)\n",
    "    idx_mid = int(np.argmin(np.abs(freq_grid-10.0)))\n",
    "    f7_test = float(re_i[idx_mid])\n",
    "\n",
    "    hf_train = bundle.get(\"train_hf_re_median\", None)\n",
    "    f7_train = bundle.get(\"train_f7_median\", None)\n",
    "\n",
    "    try:\n",
    "        hf_ok = np.isfinite(hf_test) and abs(hf_test) > 1e-12 and hf_train is not None and np.isfinite(hf_train)\n",
    "        f7_ok = np.isfinite(f7_test) and abs(f7_test) > 1e-12 and f7_train is not None and np.isfinite(f7_train)\n",
    "        scale_candidates = []\n",
    "        if hf_ok:\n",
    "            scale_candidates.append(float(hf_train) / hf_test)\n",
    "        if f7_ok:\n",
    "            scale_candidates.append(float(f7_train) / f7_test)\n",
    "\n",
    "        scale = None\n",
    "        if len(scale_candidates) == 2:\n",
    "            # If both anchors agree within 2×, use geometric mean; else prefer the one further from 1\n",
    "            s1, s2 = scale_candidates\n",
    "            if (min(s1, s2) > 0) and (max(s1, s2) / max(1e-12, min(s1, s2)) <= 2.0):\n",
    "                scale = float(np.sqrt(s1 * s2))\n",
    "            else:\n",
    "                # pick the one implying stronger correction\n",
    "                scale = s1 if abs(s1-1) > abs(s2-1) else s2\n",
    "        elif len(scale_candidates) == 1:\n",
    "            scale = scale_candidates[0]\n",
    "\n",
    "        # Apply if clearly off (outside [0.5, 2.0])\n",
    "        if scale is not None and (scale < 0.5 or scale > 2.0):\n",
    "            re_i *= scale\n",
    "            im_i *= scale\n",
    "            autoscale_factor = float(scale)\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[AUTO-SCALE] {file_path.name}: hf_test={hf_test:.4g}, f7_test={f7_test:.4g} \"\n",
    "                      f\"→ scaled by {autoscale_factor:.4g} to align.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if meta is None and cfg.TEST_TEMPERATURE_OVERRIDE is not None:\n",
    "        temp = cfg.TEST_TEMPERATURE_OVERRIDE\n",
    "    else:\n",
    "        temp = meta[\"Temp\"] if meta else -1\n",
    "    vec = build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "    norm_vec=None\n",
    "    if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL or cfg.SOC_INCLUDE_SHAPE_MODEL) and \\\n",
    "       (bundle.get(\"shape_model\") is not None or bundle.get(\"soc_shape_model\") is not None):\n",
    "        if cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "            rsh, ish = build_shape_normalized(re_i, im_i)\n",
    "            norm_vec = build_feature_vector(rsh, ish, temp, freq_grid)\n",
    "    checksum = hashlib.sha1(np.ascontiguousarray(vec).tobytes()).hexdigest()\n",
    "\n",
    "    extras = {\n",
    "        \"used_freq_from_file\": bool(used_freq),\n",
    "        \"hf_train\": None if hf_train is None else float(hf_train),\n",
    "        \"hf_test_before_scale\": float(hf_test) if np.isfinite(hf_test) else None,\n",
    "        \"f7_train\": None if f7_train is None else float(f7_train),\n",
    "        \"f7_test_before_scale\": float(f7_test) if np.isfinite(f7_test) else None,\n",
    "        \"autoscale_factor\": float(autoscale_factor),\n",
    "    }\n",
    "    return vec, norm_vec, meta, checksum, extras\n",
    "\n",
    "# =========================\n",
    "# 13. OOD & KNN PRIOR UTILITIES\n",
    "# =========================\n",
    "def mahalanobis_distance(x, center, cov_inv):\n",
    "    diff = x - center\n",
    "    return float(np.sqrt(diff @ cov_inv @ diff.T))\n",
    "\n",
    "def _knn_prior_soc(x_embed: np.ndarray, train_embed: np.ndarray, y_train: np.ndarray, k: int):\n",
    "    \"\"\"\n",
    "    Weighted KNN prior in an embedding space.\n",
    "    Returns (prior_mean, prior_std, neighbor_count).\n",
    "    \"\"\"\n",
    "    if train_embed is None or y_train is None or len(y_train)==0:\n",
    "        return None, None, 0\n",
    "    k = max(3, min(int(k), len(y_train)))\n",
    "    d = np.linalg.norm(train_embed - x_embed[None, :], axis=1)\n",
    "    idx = np.argpartition(d, k-1)[:k]\n",
    "    di = d[idx]\n",
    "    yi = y_train[idx].astype(float)\n",
    "    w = 1.0 / (di**2 + 1e-6)\n",
    "    w = w / (np.sum(w) + 1e-12)\n",
    "    prior_mean = float(np.sum(w * yi))\n",
    "    prior_var = float(np.sum(w * (yi - prior_mean)**2))\n",
    "    prior_std = math.sqrt(max(prior_var, 1e-9))\n",
    "    return prior_mean, prior_std, k\n",
    "\n",
    "# =========================\n",
    "# 14. PROJECTION PLOT\n",
    "# =========================\n",
    "def build_projection(soh_current, cpp, lower, exponent=None, n=160):\n",
    "    if soh_current <= lower or cpp <= 0:\n",
    "        return np.array([0.0]), np.array([soh_current])\n",
    "    total = (soh_current - lower) * cpp\n",
    "    cycles = np.linspace(0, total, n)\n",
    "    S0 = soh_current; Smin=lower\n",
    "    if exponent is None: exponent = cfg.PLOT_EXPONENT\n",
    "    soh_curve = Smin + (S0 - Smin)*(1 - cycles/total)**exponent\n",
    "    return cycles, soh_curve\n",
    "\n",
    "def plot_projection(file_base, soh_current, soh_std, cycles_to_map, cpp, ood_flag, out_path, thresholds):\n",
    "    if not thresholds: thresholds = (50.0, 40.0)\n",
    "    min_thr = min(thresholds)\n",
    "    if soh_current <= min_thr:\n",
    "        return\n",
    "    cycles, curve = build_projection(soh_current, cpp, min_thr)\n",
    "    plt.figure(figsize=(6.4,4))\n",
    "    plt.plot(cycles, curve, lw=2, label=\"Projected SoH\")\n",
    "\n",
    "    for thr in thresholds:\n",
    "        style = \"--\" if thr >= 50 else \":\"\n",
    "        color = \"orange\" if thr >= 50 else \"red\"\n",
    "        plt.axhline(thr, color=color, ls=style, label=f\"{int(thr)}%\")\n",
    "        x = cycles_to_map.get(thr, 0.0)\n",
    "        if x > 0:\n",
    "            plt.axvline(x, color=color, ls=\"-.\" if thr>=50 else \":\")\n",
    "            plt.scatter([x],[thr], s=45)\n",
    "            txty = thr + (1.0 if thr>=50 else -2.0)\n",
    "            plt.text(x, txty, f\"{x:.0f} cyc\", ha=\"center\", fontsize=8, color=color)\n",
    "\n",
    "    plt.scatter([0],[soh_current], c=\"green\", s=55, label=f\"Current {soh_current:.2f}%\")\n",
    "    plt.text(0, soh_current+0.7, f\"±{soh_std:.2f}\", color=\"green\", fontsize=8)\n",
    "\n",
    "    if ood_flag:\n",
    "        plt.text(0.98,0.05,\"OOD\", transform=plt.gca().transAxes,\n",
    "                 ha=\"right\", va=\"bottom\", color=\"crimson\", fontsize=11,\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"crimson\"))\n",
    "\n",
    "    plt.xlabel(\"Remaining Cycles\")\n",
    "    plt.ylabel(\"SoH (%)\")\n",
    "    plt.title(f\"RUL Projection – {file_base}\")\n",
    "    plt.grid(alpha=0.35)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 15. INFERENCE (SINGLE FILE)\n",
    "# =========================\n",
    "def predict_file(file_path: Path, bundle, cpp_map, global_cpp):\n",
    "    vec, norm_vec, meta, checksum, extras = featurize_any(file_path, bundle)\n",
    "\n",
    "    # ----- SoC -----\n",
    "    soc_scaler=bundle[\"soc_scaler\"]; soc_pca=bundle.get(\"soc_pca\")\n",
    "    soc_model=bundle[\"soc_model\"]; soc_model_name=bundle.get(\"soc_model_name\",\"unknown\")\n",
    "    X_soc = soc_scaler.transform(vec.reshape(1,-1))\n",
    "    X_soc_in_raw = soc_pca.transform(X_soc) if soc_pca else X_soc\n",
    "\n",
    "    # shape SoC path (if present)\n",
    "    soc_shape_model = bundle.get(\"soc_shape_model\")\n",
    "    X_soc_in_shape = None\n",
    "    if soc_shape_model is not None and norm_vec is not None:\n",
    "        sscaler = bundle.get(\"soc_shape_scaler\"); spca = bundle.get(\"soc_shape_pca\")\n",
    "        Xs = sscaler.transform(norm_vec.reshape(1,-1))\n",
    "        X_soc_in_shape = spca.transform(Xs) if spca else Xs\n",
    "\n",
    "    # --- SoC-space OOD check (dual spaces; pick the smaller distance) ---\n",
    "    def _dist_block(X1, stats_key):\n",
    "        info = bundle.get(stats_key)\n",
    "        if not info: return None, None, None\n",
    "        c = np.array(info.get(\"center\"))\n",
    "        inv = np.array(info.get(\"cov_inv\"))\n",
    "        thr = info.get(\"threshold\", None)\n",
    "        if X1 is None or c is None or inv is None:\n",
    "            return None, None, None\n",
    "        try:\n",
    "            d = float(np.sqrt((X1[0]-c) @ inv @ (X1[0]-c).T))\n",
    "        except Exception:\n",
    "            d = None\n",
    "        return d, float(thr) if thr is not None else None, info\n",
    "\n",
    "    d_raw, thr_raw, _ = _dist_block(X_soc_in_raw, \"soc_train_mahal_raw\")\n",
    "    d_shape, thr_shape, _ = _dist_block(X_soc_in_shape, \"soc_train_mahal_shape\")\n",
    "\n",
    "    # choose embedding with smaller normalized exceedance\n",
    "    def _exceed(d, thr):\n",
    "        if d is None or thr is None or not np.isfinite(thr): return np.inf\n",
    "        return d - thr\n",
    "    choose_shape = (_exceed(d_shape, thr_shape) < _exceed(d_raw, thr_raw))\n",
    "    d_used = d_shape if choose_shape else d_raw\n",
    "    thr_used = thr_shape if choose_shape else thr_raw\n",
    "    X_used = X_soc_in_shape if choose_shape else X_soc_in_raw\n",
    "    train_embed = bundle[\"soc_train_embed_shape\"] if choose_shape else bundle[\"soc_train_embed_raw\"]\n",
    "    train_y = bundle[\"soc_train_y_shape\"] if choose_shape else bundle[\"soc_train_y_raw\"]\n",
    "\n",
    "    soc_mahal = d_used\n",
    "    soc_oob = bool(d_used is not None and thr_used is not None and d_used > thr_used)\n",
    "\n",
    "    # raw model prediction (HGB)\n",
    "    raw_soc_mean = float(soc_model.predict(X_soc_in_raw)[0])\n",
    "    raw_soc_std  = float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0))  # model has no native std\n",
    "\n",
    "    # optional shape SoC prediction (HGB)\n",
    "    soc_shape_mean = None; soc_shape_std = None\n",
    "    if soc_shape_model is not None and X_soc_in_shape is not None:\n",
    "        soc_shape_mean = float(soc_shape_model.predict(X_soc_in_shape)[0])\n",
    "        soc_shape_std  = float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0))\n",
    "        # stabilize pre-OOD by averaging raw + shape\n",
    "        raw_soc_mean = 0.5*(raw_soc_mean + soc_shape_mean)\n",
    "        raw_soc_std  = float(np.sqrt(0.5*(raw_soc_std**2 + soc_shape_std**2)))\n",
    "\n",
    "    # ----- SoH -----\n",
    "    soh_scaler=bundle[\"soh_scaler\"]; soh_pca=bundle.get(\"soh_pca\")\n",
    "    soh_model=bundle[\"soh_model\"]; model_name=bundle.get(\"soh_model_name\",\"unknown\")\n",
    "    X_soh_s = soh_scaler.transform(vec.reshape(1,-1))\n",
    "    X_soh_in = soh_pca.transform(X_soh_s) if soh_pca else X_soh_s\n",
    "\n",
    "    if \"gpr\" in model_name:\n",
    "        from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "        if isinstance(soh_model, GaussianProcessRegressor):\n",
    "            sm, ss = soh_model.predict(X_soh_in, return_std=True)\n",
    "            soh_mean_raw = float(sm[0]); soh_std_raw=float(ss[0])\n",
    "        else:\n",
    "            soh_mean_raw = float(soh_model.predict(X_soh_in)[0])\n",
    "            soh_std_raw  = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "    else:\n",
    "        soh_mean_raw = float(soh_model.predict(X_soh_in)[0])\n",
    "        soh_std_raw  = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    shape_model = bundle.get(\"shape_model\")\n",
    "    shape_soh_mean=None; shape_soh_std=None\n",
    "    if shape_model is not None and norm_vec is not None:\n",
    "        sscaler = bundle.get(\"shape_scaler\")\n",
    "        spca = bundle.get(\"shape_pca\")\n",
    "        X_shape_s = sscaler.transform(norm_vec.reshape(1,-1))\n",
    "        X_shape_in = spca.transform(X_shape_s) if spca else X_shape_s\n",
    "        try:\n",
    "            sm2, ss2 = shape_model.predict(X_shape_in, return_std=True)\n",
    "            shape_soh_mean=float(sm2[0]); shape_soh_std=float(ss2[0])\n",
    "        except Exception:\n",
    "            shape_soh_mean=float(shape_model.predict(X_shape_in)[0])\n",
    "            shape_soh_std=float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    if cfg.ENSEMBLE_SOH and shape_soh_mean is not None:\n",
    "        soh_mean = 0.5*(soh_mean_raw + shape_soh_mean)\n",
    "        stds = [soh_std_raw]\n",
    "        if shape_soh_std is not None: stds.append(shape_soh_std)\n",
    "        soh_std = float(np.sqrt(np.mean(np.array(stds)**2)))\n",
    "    else:\n",
    "        soh_mean, soh_std = soh_mean_raw, soh_std_raw\n",
    "\n",
    "    # ----- OOD diagnostics (SoH space ONLY for SoH uncertainty) -----\n",
    "    train_mahal = bundle.get(\"train_mahal\")\n",
    "    mahal_dist=None\n",
    "    if train_mahal:\n",
    "        cov_inv = np.array(train_mahal[\"cov_inv\"])\n",
    "        center = np.array(train_mahal[\"center\"])\n",
    "        mahal_dist = mahalanobis_distance(X_soh_s[0], center, cov_inv)\n",
    "    ard_norm=None  # kept for backward-compatibility; not essential here\n",
    "    ood_flag=False\n",
    "    if (mahal_dist is not None and mahal_dist > cfg.MAHAL_THRESHOLD):\n",
    "        ood_flag=True\n",
    "\n",
    "    # ----- SoC post-hoc calibration -----\n",
    "    soc_mean = raw_soc_mean\n",
    "    soc_std  = raw_soc_std\n",
    "    soc_cal = bundle.get(\"soc_calibrator\")\n",
    "    if soc_cal is not None and (not soc_oob or cfg.SOC_CALIBRATE_ON_OOD):\n",
    "        try:\n",
    "            soc_mean = float(soc_cal.predict([soc_mean])[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    soc_mean = float(np.clip(soc_mean, 0.0, 100.0))\n",
    "\n",
    "    # ----- If SoC OOD: blend raw + (optional) shape + KNN prior; compute capped std -----\n",
    "    val_rmse_soc = float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0))\n",
    "    if cfg.OOD_SOC_ENABLE and soc_oob:\n",
    "        thr = thr_used if (thr_used is not None and np.isfinite(thr_used)) else np.inf\n",
    "        delta = max(0.0, (soc_mahal or 0.0) - thr)\n",
    "        s = max(1e-6, cfg.OOD_SOC_SHRINK_SCALE)\n",
    "\n",
    "        # Gentle decay of raw weight; never below W_MIN\n",
    "        w_raw = 1.0 / (1.0 + (delta / s))\n",
    "        w_raw = max(cfg.OOD_SOC_W_MIN, float(w_raw))\n",
    "\n",
    "        # Shape weight gets a portion of remaining if available\n",
    "        has_shape_soc = (soc_shape_model is not None and soc_shape_mean is not None)\n",
    "        w_shape = (1.0 - w_raw) * (0.60 if has_shape_soc else 0.0)\n",
    "\n",
    "        # KNN prior from chosen embedding space\n",
    "        prior_mean, prior_std, k_used = _knn_prior_soc(\n",
    "            X_used[0], train_embed, train_y, k=cfg.SOC_OOD_K\n",
    "        )\n",
    "        if prior_mean is None:\n",
    "            prior_mean = soc_mean\n",
    "            prior_std = val_rmse_soc\n",
    "\n",
    "        w_prior = max(0.0, 1.0 - w_raw - w_shape)\n",
    "\n",
    "        # Combine means\n",
    "        soc_mean = (w_raw * soc_mean) + \\\n",
    "                   (w_shape * (soc_shape_mean if has_shape_soc else soc_mean)) + \\\n",
    "                   (w_prior * prior_mean)\n",
    "        soc_mean = float(np.clip(soc_mean, 0.0, 100.0))\n",
    "\n",
    "        # Combine stds (RMS of weighted components) then cap\n",
    "        comp_raw_std   = soc_std\n",
    "        comp_shape_std = soc_shape_std if has_shape_soc and soc_shape_std is not None else soc_std\n",
    "        comp_prior_std = max(prior_std, val_rmse_soc)\n",
    "\n",
    "        var = (w_raw*comp_raw_std)**2 + (w_shape*comp_shape_std)**2 + (w_prior*comp_prior_std)**2\n",
    "        soc_std = float(math.sqrt(max(var, 1e-12)))\n",
    "\n",
    "        # Cap std for OOD (aggressively reduce huge values)\n",
    "        cap_ood = val_rmse_soc * cfg.SOC_STD_CAP_MULT_OOD\n",
    "        soc_std = float(min(soc_std, cap_ood))\n",
    "\n",
    "        if cfg.VERBOSE:\n",
    "            which = \"shape\" if choose_shape else \"raw\"\n",
    "            print(f\"[SoC-OOD] space={which} d={soc_mahal:.2f} thr={thr:.2f} \"\n",
    "                  f\"w_raw={w_raw:.3f} w_shape={w_shape:.3f} w_prior={w_prior:.3f} \"\n",
    "                  f\"prior_mean={prior_mean:.2f} prior_std={prior_std:.2f} k={k_used}\")\n",
    "\n",
    "    else:\n",
    "        # In-domain: cap std tightly to validation RMSE\n",
    "        cap_in = val_rmse_soc * cfg.SOC_STD_CAP_MULT_IN\n",
    "        soc_std = float(min(soc_std, cap_in))\n",
    "\n",
    "    # ----- SoH uncertainty capping (SoH OOD only) -----\n",
    "    soh_val_rmse = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "    if ood_flag:\n",
    "        soh_std = min(soh_std, cfg.SOH_STD_MAX_OOD)\n",
    "    else:\n",
    "        soh_std = min(soh_std, soh_val_rmse)\n",
    "\n",
    "    # ----- RUL (multi-threshold) -----\n",
    "    cpp = get_cpp(meta, cpp_map, global_cpp)\n",
    "    cycles_to = {}\n",
    "    for thr_val in cfg.TARGET_SOH_THRESHOLDS:\n",
    "        cycles_to[thr_val] = float((soh_mean - thr_val) * cpp) if soh_mean > thr_val else 0.0\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoC] {Path(file_path).name}: mean={soc_mean:.2f} std={soc_std:.2f}  \"\n",
    "              f\"SOC_mahal_raw={d_raw} thr_raw={thr_raw}  SOC_mahal_shape={d_shape} thr_shape={thr_shape}  \"\n",
    "              f\"used={'shape' if choose_shape else 'raw'} OOD={soc_oob}\")\n",
    "        print(f\"[SoH] {Path(file_path).name}: mean={soh_mean:.2f} std={soh_std:.2f}  \"\n",
    "              f\"OOD_flag(SoH)={ood_flag}  Mahalanobis={mahal_dist}\")\n",
    "\n",
    "    result={\n",
    "        \"file\": str(file_path),\n",
    "        \"feature_checksum\": checksum,\n",
    "        \"parsed_metadata\": meta,\n",
    "        # SoC\n",
    "        \"predicted_SoC_percent\": float(soc_mean),\n",
    "        \"SoC_std_estimate\": float(soc_std),\n",
    "        \"soc_model_chosen\": soc_model_name,\n",
    "        \"SoC_probabilities\": None,\n",
    "        \"SOC_mahal\": soc_mahal,\n",
    "        \"SOC_ood\": bool(soc_oob),\n",
    "        # helpful debug\n",
    "        \"raw_soc_precal\": float(raw_soc_mean),\n",
    "        \"raw_soc_precal_std\": float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0)),\n",
    "        \"shape_soc_mean\": None if soc_shape_mean is None else float(soc_shape_mean),\n",
    "        \"shape_soc_std\": None if soc_shape_std is None else float(soc_shape_std),\n",
    "        # SoH\n",
    "        \"predicted_SoH_percent\": float(soh_mean),\n",
    "        \"SoH_std_estimate\": float(soh_std),\n",
    "        \"raw_model_mean\": float(soh_mean_raw),\n",
    "        \"raw_model_std\": float(soh_std_raw),\n",
    "        \"shape_model_mean\": None if shape_model is None else float(shape_soh_mean),\n",
    "        \"shape_model_std\": None if shape_model is None else float(shape_soh_std),\n",
    "        \"soh_model_chosen\": model_name,\n",
    "        # RUL\n",
    "        \"cycles_per_percent_used\": float(cpp),\n",
    "        \"cycles_to_thresholds\": {str(int(k)): v for k,v in cycles_to.items()},\n",
    "        \"decision_threshold_percent\": cfg.DECISION_SOH_PERCENT,\n",
    "        \"lower_threshold_percent\": cfg.ILLUSTRATIVE_MIN_SOH,\n",
    "        # OOD (SoH diagnostics)\n",
    "        \"OOD_mahal\": None if mahal_dist is None else float(mahal_dist),\n",
    "        \"OOD_gp_ard_norm\": None,\n",
    "        \"OOD_flag\": bool(ood_flag),\n",
    "        # Loader / autoscale diagnostics\n",
    "        \"used_freq_from_file\": extras.get(\"used_freq_from_file\"),\n",
    "        \"hf_train\": extras.get(\"hf_train\"),\n",
    "        \"hf_test_before_scale\": extras.get(\"hf_test_before_scale\"),\n",
    "        \"f7_train\": extras.get(\"f7_train\"),\n",
    "        \"f7_test_before_scale\": extras.get(\"f7_test_before_scale\"),\n",
    "        \"autoscale_factor\": extras.get(\"autoscale_factor\"),\n",
    "    }\n",
    "    return result, ood_flag, cycles_to\n",
    "\n",
    "# =========================\n",
    "# 16. MAIN (batch mode)\n",
    "# =========================\n",
    "def main():\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps(to_jsonable(asdict(cfg)), indent=2))\n",
    "\n",
    "    assert cfg.EIS_DIR.exists(), f\"EIS_DIR missing: {cfg.EIS_DIR}\"\n",
    "    if cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        assert cfg.CAP_DIR.exists(), f\"CAP_DIR missing: {cfg.CAP_DIR}\"\n",
    "\n",
    "    # Capacity + dynamic CPP\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR)\n",
    "    if cap_df.empty:\n",
    "        if cfg.VERBOSE: print(\"[INFO] No / empty capacity data -> fallback Cpp.\")\n",
    "        cpp_map, global_cpp = {}, cfg.CPP_FALLBACK\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[CPP] dynamic cells={len(cpp_map)} global_cpp_median={global_cpp:.2f}\")\n",
    "\n",
    "    # Train or load with signature check\n",
    "    bundle_path = cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    need_retrain = True\n",
    "    if bundle_path.exists():\n",
    "        try:\n",
    "            bundle = load_bundle()\n",
    "            same_sig = (bundle.get(\"config_signature\") == config_signature(cfg)) and \\\n",
    "                       (bundle.get(\"feature_version\") == cfg.FEATURE_VERSION)\n",
    "            need_retrain = not same_sig\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[LOAD] Found bundle. Signature match: {same_sig}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD] Could not load existing bundle cleanly: {e}\")\n",
    "            need_retrain = True\n",
    "\n",
    "    if cfg.FORCE_RETRAIN:\n",
    "        need_retrain = True\n",
    "\n",
    "    if need_retrain:\n",
    "        if cfg.VERBOSE: print(\"[TRAIN] Building dataset & training models...\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[TRAIN] Samples={X_raw.shape[0]} Features={X_raw.shape[1]} Cells={meta_df.CellID.nunique()}\")\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh)\n",
    "    else:\n",
    "        bundle = load_bundle()\n",
    "\n",
    "    # Inference (batch over cfg.EIS_TEST_FILES)\n",
    "    for test_fp in cfg.EIS_TEST_FILES:\n",
    "        print(f\"\\n===== TEST: {test_fp.name} =====\")\n",
    "        if not Path(test_fp).exists():\n",
    "            print(f\"[WARN] Test file not found: {test_fp}\")\n",
    "            continue\n",
    "        try:\n",
    "            result, ood_flag, cycles_to_map = predict_file(Path(test_fp), bundle, cpp_map, global_cpp)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Prediction failed for {Path(test_fp).name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        out_plot = cfg.MODEL_DIR / f\"{Path(test_fp).stem}_projection.png\"\n",
    "        plot_projection(\n",
    "            Path(test_fp).stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            cycles_to_map,\n",
    "            result[\"cycles_per_percent_used\"],\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot,\n",
    "            thresholds=cfg.TARGET_SOH_THRESHOLDS\n",
    "        )\n",
    "\n",
    "        out_json = cfg.MODEL_DIR / f\"{Path(test_fp).stem}_prediction.json\"\n",
    "        with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(f\"[PLOT] Saved: {out_plot}\")\n",
    "        print(f\"[JSON] Saved: {out_json}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "# =========================\n",
    "# 17. GRADIO UI (Jupyter-friendly)\n",
    "# =========================\n",
    "def _prepare_models_for_ui(force_retrain: bool=False):\n",
    "    bundle_path = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "\n",
    "    # Capacity + dynamic CPP (mirrors main)\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR) if cfg.REFINE_SOH_WITH_CAPACITY else pd.DataFrame()\n",
    "    if cap_df.empty:\n",
    "        cpp_map, global_cpp = {}, cfg.CPP_FALLBACK\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "\n",
    "    need_retrain = force_retrain or (not bundle_path.exists())\n",
    "    if not need_retrain and bundle_path.exists():\n",
    "        try:\n",
    "            bundle = load_bundle()\n",
    "            same_sig = (bundle.get(\"config_signature\") == config_signature(cfg)) and \\\n",
    "                       (bundle.get(\"feature_version\") == cfg.FEATURE_VERSION)\n",
    "            need_retrain = not same_sig\n",
    "        except Exception:\n",
    "            need_retrain = True\n",
    "\n",
    "    if need_retrain:\n",
    "        if not cfg.EIS_DIR.exists():\n",
    "            raise FileNotFoundError(f\"EIS_DIR missing: {cfg.EIS_DIR}. Update cfg.EIS_DIR before training.\")\n",
    "        if cfg.REFINE_SOH_WITH_CAPACITY and not cfg.CAP_DIR.exists():\n",
    "            raise FileNotFoundError(f\"CAP_DIR missing: {cfg.CAP_DIR}. Update cfg.CAP_DIR or disable REFINE_SOH_WITH_CAPACITY.\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh)\n",
    "    else:\n",
    "        bundle = load_bundle()\n",
    "\n",
    "    return bundle, cpp_map, global_cpp\n",
    "\n",
    "def _ui_predict(file_obj, override_temp, force_retrain):\n",
    "    try:\n",
    "        # Optionally override test temperature from UI\n",
    "        orig_temp_override = cfg.TEST_TEMPERATURE_OVERRIDE\n",
    "        if override_temp is None or str(override_temp).strip() == \"\":\n",
    "            cfg.TEST_TEMPERATURE_OVERRIDE = orig_temp_override\n",
    "        else:\n",
    "            try:\n",
    "                cfg.TEST_TEMPERATURE_OVERRIDE = float(override_temp)\n",
    "            except Exception:\n",
    "                cfg.TEST_TEMPERATURE_OVERRIDE = orig_temp_override\n",
    "\n",
    "        # Prepare model assets\n",
    "        bundle, cpp_map, global_cpp = _prepare_models_for_ui(force_retrain=bool(force_retrain))\n",
    "\n",
    "        # Normalize Gradio File\n",
    "        test_fp: Optional[Path] = None\n",
    "        if file_obj is None:\n",
    "            raise ValueError(\"Please upload a file.\")\n",
    "        if isinstance(file_obj, (str, Path)):\n",
    "            test_fp = Path(file_obj)\n",
    "        elif isinstance(file_obj, dict) and \"name\" in file_obj:\n",
    "            test_fp = Path(file_obj[\"name\"])\n",
    "        elif hasattr(file_obj, \"name\"):\n",
    "            name = Path(getattr(file_obj, \"name\", \"upload\")).name\n",
    "            suffix = Path(name).suffix or \"\"\n",
    "            tmp_name = cfg.MODEL_DIR / f\"ui_{uuid.uuid4().hex}{suffix}\"\n",
    "            try:\n",
    "                file_obj.seek(0)\n",
    "            except Exception:\n",
    "                pass\n",
    "            data = file_obj.read()\n",
    "            if isinstance(data, str):\n",
    "                data = data.encode(\"utf-8\")\n",
    "            with open(tmp_name, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            test_fp = tmp_name\n",
    "        else:\n",
    "            tmp_name = cfg.MODEL_DIR / f\"ui_{uuid.uuid4().hex}\"\n",
    "            data = file_obj.read()\n",
    "            if isinstance(data, str):\n",
    "                data = data.encode(\"utf-8\")\n",
    "            with open(tmp_name, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            test_fp = tmp_name\n",
    "\n",
    "        # Predict\n",
    "        result, ood_flag, cycles_to_map = predict_file(test_fp, bundle, cpp_map, global_cpp)\n",
    "\n",
    "        # Build plot (in-memory)\n",
    "        out_plot = cfg.MODEL_DIR / f\"{Path(test_fp).stem}_projection_ui.png\"\n",
    "        plot_projection(\n",
    "            Path(test_fp).stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            cycles_to_map,\n",
    "            result[\"cycles_per_percent_used\"],\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot,\n",
    "            thresholds=cfg.TARGET_SOH_THRESHOLDS\n",
    "        )\n",
    "        with open(out_plot, \"rb\") as f:\n",
    "            img_bytes = f.read()\n",
    "        plot_img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        # Compact, user-facing JSON\n",
    "        pretty = {\n",
    "            \"file\": result[\"file\"],\n",
    "            \"feature_checksum\": result.get(\"feature_checksum\"),\n",
    "            \"SoC_percent\": round(result[\"predicted_SoC_percent\"], 2),\n",
    "            \"SoC_std\": round(result[\"SoC_std_estimate\"], 2),\n",
    "            \"SoH_percent\": round(result[\"predicted_SoH_percent\"], 2),\n",
    "            \"SoH_std\": round(result[\"SoH_std_estimate\"], 2),\n",
    "            \"OOD\": bool(result[\"OOD_flag\"]),\n",
    "            \"cycles_per_percent\": result[\"cycles_per_percent_used\"],\n",
    "            \"cycles_to_thresholds\": {k: round(v, 1) for k, v in result[\"cycles_to_thresholds\"].items()},\n",
    "            \"soc_model\": result.get(\"soc_model_chosen\"),\n",
    "            \"soh_model\": result.get(\"soh_model_chosen\"),\n",
    "            \"decision_threshold_percent\": result[\"decision_threshold_percent\"],\n",
    "            \"lower_threshold_percent\": result[\"lower_threshold_percent\"]\n",
    "        }\n",
    "        pretty_json = json.dumps(pretty, indent=2)\n",
    "\n",
    "        soc_value = float(result[\"predicted_SoC_percent\"])\n",
    "\n",
    "        # restore override\n",
    "        cfg.TEST_TEMPERATURE_OVERRIDE = orig_temp_override\n",
    "        return plot_img, pretty_json, soc_value\n",
    "    except Exception as e:\n",
    "        err = {\"error\": str(e)}\n",
    "        return None, json.dumps(err, indent=2), None\n",
    "\n",
    "def launch_gradio(server_name: str = \"127.0.0.1\",\n",
    "                  server_port: int = 7860,\n",
    "                  share: bool = False,\n",
    "                  inbrowser: bool = False):\n",
    "    try:\n",
    "        import gradio as gr\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Gradio is not installed. Please: pip install gradio\") from e\n",
    "\n",
    "    if _running_in_notebook():\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] nest_asyncio not available. If you see 'event loop' errors, pip install nest_asyncio.\", e)\n",
    "\n",
    "    with gr.Blocks(title=\"EIS SoC/SoH + RUL (v8.8)\") as demo:\n",
    "        gr.Markdown(\"## Unified EIS: SoC / SoH inference & RUL projection\\nUpload a single EIS file (.csv / .xlsx / .xls / .mat) to get the projection plot and SoC value.\")\n",
    "        with gr.Row():\n",
    "            file_in = gr.File(label=\"Upload EIS file\", file_count=\"single\")\n",
    "        with gr.Row():\n",
    "            override_temp = gr.Textbox(label=\"Test temperature override (°C, optional)\", placeholder=str(cfg.TEST_TEMPERATURE_OVERRIDE))\n",
    "            force_retrain = gr.Checkbox(label=\"Force retrain before inference\", value=False)\n",
    "        predict_btn = gr.Button(\"Predict\")\n",
    "        with gr.Row():\n",
    "            img_out = gr.Image(label=\"RUL Projection Plot\", type=\"pil\")\n",
    "            json_out = gr.Code(label=\"Results (JSON)\")\n",
    "        soc_out = gr.Number(label=\"Predicted SoC (%)\", precision=2)\n",
    "\n",
    "        with gr.Row():\n",
    "            rt_btn = gr.Button(\"Retrain bundle only\")\n",
    "            rt_status = gr.Markdown()\n",
    "\n",
    "        def _do_predict(file_obj, temp, fr):\n",
    "            return _ui_predict(file_obj, temp, fr)\n",
    "\n",
    "        def _do_retrain():\n",
    "            try:\n",
    "                _prepare_models_for_ui(force_retrain=True)\n",
    "                return \"✅ Retrained successfully.\"\n",
    "            except Exception as e:\n",
    "                return f\"❌ Retrain failed: {e}\"\n",
    "\n",
    "        predict_btn.click(_do_predict, inputs=[file_in, override_temp, force_retrain], outputs=[img_out, json_out, soc_out])\n",
    "        rt_btn.click(_do_retrain, inputs=None, outputs=rt_status)\n",
    "\n",
    "    launch_kwargs = dict(server_name=server_name, server_port=server_port, share=share)\n",
    "    if _running_in_notebook():\n",
    "        launch_kwargs.update(dict(inline=True, inbrowser=False, prevent_thread_lock=True, debug=False))\n",
    "        demo.queue(concurrency_count=2, max_size=10)\n",
    "        for attempt in range(6):\n",
    "            try:\n",
    "                return demo.launch(**launch_kwargs)\n",
    "            except OSError as e:\n",
    "                if \"Address already in use\" in str(e).lower():\n",
    "                    launch_kwargs[\"server_port\"] = int(launch_kwargs.get(\"server_port\", 7860)) + 1\n",
    "                    continue\n",
    "                raise\n",
    "            except TypeError:\n",
    "                for k in (\"inline\", \"prevent_thread_lock\"):\n",
    "                    launch_kwargs.pop(k, None)\n",
    "                return demo.launch(**launch_kwargs)\n",
    "    else:\n",
    "        try:\n",
    "            return demo.launch(server_name=server_name, server_port=server_port, share=share, inbrowser=inbrowser)\n",
    "        except OSError as e:\n",
    "            if \"Address already in use\" in str(e):\n",
    "                return demo.launch(server_name=server_name, server_port=server_port+1, share=share, inbrowser=inbrowser)\n",
    "            raise\n",
    "\n",
    "# =========================\n",
    "# 18. ENTRYPOINT\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ui\", action=\"store_true\", help=\"Launch the Gradio UI\")\n",
    "    parser.add_argument(\"--share\", action=\"store_true\", help=\"Create a public share link\")\n",
    "    parser.add_argument(\"--host\", default=\"127.0.0.1\", help=\"Server host (default: 127.0.0.1)\")\n",
    "    parser.add_argument(\"--port\", type=int, default=7860, help=\"Server port (default: 7860)\")\n",
    "    parser.add_argument(\"--inbrowser\", action=\"store_true\", help=\"Open UI in browser automatically\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.ui:\n",
    "        launch_gradio(server_name=args.host, server_port=args.port, share=args.share, inbrowser=args.inbrowser)\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794eb67-100e-4bb4-aed7-20109eda431d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
