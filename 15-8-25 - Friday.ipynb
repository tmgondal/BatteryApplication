{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefa4c5c-c6de-4c70-8b13-6030a72b8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"EIS_TEST_FILES\": [\n",
      "    \"Mazda-Battery-Cell9.xlsx\"\n",
      "  ],\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"SOH_STD_MAX_OOD\": 2.0,\n",
      "  \"SOC_STD_MAX_OOD\": 10.0,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"GROUP_KFOLDS\": 0,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"USE_PCA_SOC\": false,\n",
      "  \"USE_PCA_SOH\": false,\n",
      "  \"PCA_SOC_COMPONENTS\": 0,\n",
      "  \"PCA_SOH_COMPONENTS\": 0,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"INCLUDE_NORMALIZED_SHAPE_MODEL\": true,\n",
      "  \"ENSEMBLE_SOH\": true,\n",
      "  \"NORMALIZE_SHAPE_BY_HF_RE\": true,\n",
      "  \"SOC_INCLUDE_SHAPE_MODEL\": true,\n",
      "  \"SOC_MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"ENABLE_CYCLES_MODEL\": true,\n",
      "  \"DECISION_SOH_PERCENT\": 50.0,\n",
      "  \"ILLUSTRATIVE_MIN_SOH\": 40.0,\n",
      "  \"CPP_ROLLING_WINDOW\": 5,\n",
      "  \"CPP_MIN_POINTS\": 6,\n",
      "  \"CPP_FALLBACK\": 20.0,\n",
      "  \"TEST_TEMPERATURE_OVERRIDE\": 25.0,\n",
      "  \"FORCE_RETRAIN\": false,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FEATURE_VERSION\": 91,\n",
      "  \"MAHAL_THRESHOLD\": 10.0,\n",
      "  \"GP_ARD_NORM_THRESHOLD\": 6.0,\n",
      "  \"PLOT_EXPONENT\": 1.25,\n",
      "  \"TARGET_SOH_THRESHOLDS\": [\n",
      "    80.0,\n",
      "    50.0,\n",
      "    40.0\n",
      "  ],\n",
      "  \"OOD_SOC_ENABLE\": true,\n",
      "  \"OOD_SOC_Q\": 0.995,\n",
      "  \"OOD_SOC_PRIOR\": 50.0,\n",
      "  \"OOD_SOC_SHRINK_SCALE\": 4.0,\n",
      "  \"OOD_SOC_W_MIN\": 0.15,\n",
      "  \"OOD_SOC_PRIOR_MODE\": \"knn\",\n",
      "  \"SOC_OOD_USE_KNN\": true,\n",
      "  \"SOC_OOD_K\": 20,\n",
      "  \"SOC_CALIBRATE_ON_OOD\": false,\n",
      "  \"OOD_SOC_PRIOR_MAX_WEIGHT\": 0.75,\n",
      "  \"OOD_SOC_SHAPE_MAX_WEIGHT\": 0.35,\n",
      "  \"SOC_LABEL_JITTER\": 0.0,\n",
      "  \"CYCLE_SCALE\": 1.0,\n",
      "  \"TARGET_CALIB_CYCLE_AT_80\": 1000.0,\n",
      "  \"CYCLE_TAIL_POINTS\": 4\n",
      "}\n",
      "[CYCLE-SCALE] auto=5.556  (target 80% at 1000.0)  → total scale=5.556\n",
      "[CPP] dynamic cells=19 global_cpp_median=93.40 (scaled)\n",
      "[LOAD] Found bundle. Signature match: True\n",
      "\n",
      "===== TEST: Mazda-Battery-Cell9.xlsx =====\n",
      "[SoC-OOD] sev=1.00 w_raw=0.25 w_shape=0.00 w_prior=0.75 prior=69.26\n",
      "[SoC] Mazda-Battery-Cell9.xlsx: mean=63.94 std=10.00  SOC_mahal=6052.191942788744 SOC_ood=True\n",
      "[SoH] Mazda-Battery-Cell9.xlsx: mean=91.62 std=2.00  OOD(SoH)=True\n",
      "[Cycles] absolute=999.3\n",
      "[Cycles] remaining: →80%: 1552.7, →50%: 5746.7, →40%: 7142.6\n",
      "{\n",
      "  \"file\": \"Mazda-Battery-Cell9.xlsx\",\n",
      "  \"feature_checksum\": \"bc97b6683bcb30da72b64aa3e432a6f0d00c1a3f\",\n",
      "  \"parsed_metadata\": null,\n",
      "  \"predicted_SoC_percent\": 63.94435031273265,\n",
      "  \"SoC_std_estimate\": 10.0,\n",
      "  \"soc_model_chosen\": \"soc_gpr_raw\",\n",
      "  \"SOC_mahal\": 6052.191942788744,\n",
      "  \"SOC_ood\": true,\n",
      "  \"predicted_SoH_percent\": 91.61500792387868,\n",
      "  \"SoH_std_estimate\": 2.0,\n",
      "  \"shape_model_mean\": 91.61500792387868,\n",
      "  \"shape_model_std\": 7.461126805846326,\n",
      "  \"soh_model_chosen\": \"gpr_raw\",\n",
      "  \"predicted_cycle_index\": 999.2603291746725,\n",
      "  \"predicted_cycles_remaining_to_thresholds\": {\n",
      "    \"80\": 1552.6733283519638,\n",
      "    \"50\": 5746.735161477194,\n",
      "    \"40\": 7142.57184031878\n",
      "  },\n",
      "  \"cycles_per_percent_est\": 138.38168640509707,\n",
      "  \"fallback_cpp_used\": null,\n",
      "  \"decision_threshold_percent\": 50.0,\n",
      "  \"lower_threshold_percent\": 40.0,\n",
      "  \"OOD_flag\": true\n",
      "}\n",
      "[PLOT] Saved: models_eis_phase2_phys\\artifacts\\Mazda-Battery-Cell9_projection.png\n",
      "[JSON] Saved: models_eis_phase2_phys\\artifacts\\Mazda-Battery-Cell9_prediction.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Unified EIS Training + Inference + Cycles-from-Training (v9.1)\n",
    "- FIX: Remaining-cycles to low thresholds now extrapolated (no more zeros)\n",
    "- NEW: Cycle scale calibration to match real-world cycle units (e.g., 80% ≈ 1000 cycles)\n",
    "- SoC OOD: steadier blending + std cap\n",
    "- Everything else retained (no PCA, shape models, isotonic, UI)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, random, warnings, joblib, hashlib, uuid, io, sys, os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers: environment detection\n",
    "# =========================\n",
    "def _running_in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # noqa\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell in (\"ZMQInteractiveShell\", \"Shell\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIGURATION\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Training data directories (update if needed)\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "\n",
    "    # Test files\n",
    "    EIS_TEST_FILES: List[Path] = None  # assigned after instantiation\n",
    "\n",
    "    # Frequency interpolation grid\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int = 60\n",
    "\n",
    "    # Uncertainty control\n",
    "    SOH_STD_MAX_OOD: float = 2.0\n",
    "    SOC_STD_MAX_OOD: float = 10.0  # ★ cap SoC std on OOD\n",
    "\n",
    "    # Train / split settings\n",
    "    TEST_FRAC: float = 0.2\n",
    "    GROUP_KFOLDS: int = 0\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    # ==== NO PCA (SoC / SoH / Cycles)\n",
    "    USE_PCA_SOC: bool = False\n",
    "    USE_PCA_SOH: bool = False\n",
    "    PCA_SOC_COMPONENTS: int = 0\n",
    "    PCA_SOH_COMPONENTS: int = 0\n",
    "\n",
    "    # Feature group toggles\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    # DRT params\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    # Capacity-based refinement\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "\n",
    "    # SoH modeling\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    INCLUDE_NORMALIZED_SHAPE_MODEL: bool = True\n",
    "    ENSEMBLE_SOH: bool = True\n",
    "    NORMALIZE_SHAPE_BY_HF_RE: bool = True\n",
    "\n",
    "    # SoC modeling\n",
    "    SOC_INCLUDE_SHAPE_MODEL: bool = True\n",
    "    SOC_MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "\n",
    "    # Cycles modeling\n",
    "    ENABLE_CYCLES_MODEL: bool = True\n",
    "\n",
    "    # RUL parameters (for plotting)\n",
    "    DECISION_SOH_PERCENT: float = 50.0\n",
    "    ILLUSTRATIVE_MIN_SOH: float = 40.0\n",
    "    CPP_ROLLING_WINDOW: int = 5\n",
    "    CPP_MIN_POINTS: int = 6\n",
    "    CPP_FALLBACK: float = 20.0  # used only if no cycles model & no CPP est\n",
    "\n",
    "    # Inference extras\n",
    "    TEST_TEMPERATURE_OVERRIDE: Optional[float] = 25.0\n",
    "    FORCE_RETRAIN: bool = False\n",
    "\n",
    "    # Saving / logging\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    VERBOSE: bool = True\n",
    "\n",
    "    # ---- bump feature signature\n",
    "    FEATURE_VERSION: int = 91  # ★\n",
    "\n",
    "    # OOD thresholds (SoH)\n",
    "    MAHAL_THRESHOLD: float = 10.0\n",
    "    GP_ARD_NORM_THRESHOLD: float = 6.0\n",
    "\n",
    "    # Projection curve\n",
    "    PLOT_EXPONENT: float = 1.25\n",
    "\n",
    "    # Thresholds to report/plot\n",
    "    TARGET_SOH_THRESHOLDS: Tuple[float, ...] = (80.0, 50.0, 40.0)\n",
    "\n",
    "    # --- SoC OOD controls ---\n",
    "    OOD_SOC_ENABLE: bool = True\n",
    "    OOD_SOC_Q: float = 0.995\n",
    "    OOD_SOC_PRIOR: float = 50.0\n",
    "    OOD_SOC_SHRINK_SCALE: float = 4.0\n",
    "    OOD_SOC_W_MIN: float = 0.15  # ★ allow even less reliance on raw model on severe OOD\n",
    "    OOD_SOC_PRIOR_MODE: str = \"knn\"\n",
    "    SOC_OOD_USE_KNN: bool = True\n",
    "    SOC_OOD_K: int = 20\n",
    "    SOC_CALIBRATE_ON_OOD: bool = False\n",
    "    OOD_SOC_PRIOR_MAX_WEIGHT: float = 0.75  # ★ cap how dominant KNN prior can get\n",
    "    OOD_SOC_SHAPE_MAX_WEIGHT: float = 0.35  # ★ limit shape under severe OOD\n",
    "\n",
    "    # Optional: de-discretize SoC labels\n",
    "    SOC_LABEL_JITTER: float = 0.0\n",
    "\n",
    "    # ---- Cycles scaling / extrapolation ----\n",
    "    CYCLE_SCALE: float = 1.0  # ★ global multiplier on cycle counts\n",
    "    TARGET_CALIB_CYCLE_AT_80: Optional[float] = 1000.0  # ★ auto-scale so median cell hits this at 80% (set None to disable)\n",
    "    CYCLE_TAIL_POINTS: int = 4  # ★ points to fit slope (CPP) at low-SoH end for extrapolation\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "if cfg.EIS_TEST_FILES is None:\n",
    "    cfg.EIS_TEST_FILES = [Path(\"Mazda-Battery-Cell9.xlsx\")]\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Helper to tweak paths easily in notebooks\n",
    "def set_paths(eis_dir: str | Path, cap_dir: str | Path, model_dir: str | Path | None = None):\n",
    "    cfg.EIS_DIR = Path(eis_dir)\n",
    "    cfg.CAP_DIR = Path(cap_dir)\n",
    "    if model_dir is not None:\n",
    "        cfg.MODEL_DIR = Path(model_dir)\n",
    "        cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. UTILITIES\n",
    "# =========================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path): return str(x)\n",
    "    if isinstance(x, dict): return {k: to_jsonable(v) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "def config_signature(cfg: Config) -> str:\n",
    "    d = asdict(cfg).copy()\n",
    "    d[\"EIS_DIR\"] = str(d[\"EIS_DIR\"]); d[\"CAP_DIR\"]=str(d[\"CAP_DIR\"]); d[\"MODEL_DIR\"]=str(d[\"MODEL_DIR\"])\n",
    "    d.pop(\"EIS_TEST_FILES\", None)\n",
    "    blob = json.dumps(d, sort_keys=True)\n",
    "    return hashlib.sha256(blob.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. REGEX\n",
    "# =========================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. PARSERS\n",
    "# =========================\n",
    "def parse_eis_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"SOC\": float(d[\"SOC\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"RealSOH_file\": int(d[\"RealSOH\"])/100.0\n",
    "    }\n",
    "\n",
    "def parse_cap_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"CycleIndex\": int(d[\"Cycle\"])\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. LOW-LEVEL LOADERS / INTERPOLATION\n",
    "# =========================\n",
    "def _find_matrix(mat_dict: dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[1] >= 3 and v.shape[0] >= 10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw, y_raw, freq_target):\n",
    "    freq_raw = np.asarray(freq_raw).astype(float)\n",
    "    y_raw = np.asarray(y_raw).astype(float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw = freq_raw[::-1]; y_raw = y_raw[::-1]\n",
    "    uniq, idx = np.unique(freq_raw, return_index=True)\n",
    "    if len(uniq) != len(freq_raw):\n",
    "        order = np.argsort(idx)\n",
    "        freq_raw = uniq[order]; y_raw = y_raw[idx][order]\n",
    "    f = interp1d(freq_raw, y_raw, bounds_error=False,\n",
    "                 fill_value=(y_raw[0], y_raw[-1]), kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "FREQ_CANDS = [\"frequency\",\"freq\",\"f\",\"hz\",\"frequency(hz)\",\"Frequency(Hz)\"]\n",
    "RE_CANDS   = [\"zreal\",\"re(z)\",\"re\",\"real\",\"z_re\",\"zreal(ohm)\",\"re (ohm)\",\"re(z) (ohm)\",\"Zreal\",\"Zreal (ohm)\",\"Zreal(ohm)\"]\n",
    "IM_CANDS   = [\"-zimag\",\"zimag\",\"im(z)\",\"im\",\"imag\",\"imaginary\",\"z_im\",\"zimg\",\"z_imag\",\" -Zimag (ohm)\",\" -Zimag(ohm)\",\"-Zimag\",\"Zimag\",\"Zimag (ohm)\"]\n",
    "\n",
    "def _select_column(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c.lower() in low: return low[c.lower()]\n",
    "    for c in cands:\n",
    "        for col in df.columns:\n",
    "            if c.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def load_mat_eis(path: Path):\n",
    "    mat = loadmat(path)\n",
    "    arr = _find_matrix(mat)\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"No valid EIS matrix in {path.name}\")\n",
    "    return arr[:,0].astype(float), arr[:,1].astype(float), arr[:,2].astype(float)\n",
    "\n",
    "def load_table_eis(path: Path):\n",
    "    # returns (freq, re, im, used_freq_from_file: bool)\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty table.\")\n",
    "    fcol = _select_column(df, FREQ_CANDS)\n",
    "    recol = _select_column(df, RE_CANDS)\n",
    "    imcol = _select_column(df, IM_CANDS)\n",
    "    if recol is None or imcol is None:\n",
    "        raise ValueError(f\"Missing Re/Im columns in {path.name}\")\n",
    "    re_vals = pd.to_numeric(df[recol], errors=\"coerce\").to_numpy()\n",
    "    im_vals = pd.to_numeric(df[imcol], errors=\"coerce\").to_numpy()\n",
    "    used_freq = True\n",
    "    if fcol is not None:\n",
    "        freq_vals = pd.to_numeric(df[fcol], errors=\"coerce\").to_numpy()\n",
    "    else:\n",
    "        used_freq = False\n",
    "        n = min(len(re_vals), len(im_vals))\n",
    "        freq_vals = np.geomspace(cfg.F_MAX, cfg.F_MIN, n)\n",
    "    n = min(len(freq_vals), len(re_vals), len(im_vals))\n",
    "    freq_vals = freq_vals[:n]; re_vals = re_vals[:n]; im_vals = im_vals[:n]\n",
    "    if np.nanmean(im_vals) > 0:\n",
    "        im_vals = -im_vals\n",
    "    return freq_vals.astype(float), re_vals.astype(float), im_vals.astype(float), used_freq\n",
    "\n",
    "def load_any_inference(path: Path):\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".mat\":\n",
    "        f,r,i = load_mat_eis(path); used=True\n",
    "    elif suf in (\".csv\",\".xls\",\".xlsx\"):\n",
    "        f,r,i,used = load_table_eis(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported test file extension: {suf}\")\n",
    "    return f,r,i,used\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# =========================\n",
    "def compute_F_features(freq, re_i, im_i):\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    F1 = re_i[0]; F2 = re_i[idx_peak]; F3 = re_i[-1]\n",
    "    sc = np.where(np.sign(im_i[:-1]) != np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k = sc[0]; y0,y1 = im_i[k], im_i[k+1]\n",
    "        w = -y0/(y1 - y0 + 1e-12)\n",
    "        F4 = re_i[k] + w*(re_i[k+1]-re_i[k])\n",
    "    else:\n",
    "        F4 = np.nan\n",
    "    F5 = (re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6 = np.min(im_i)\n",
    "    mid_target = 10.0\n",
    "    idx_mid = int(np.argmin(np.abs(freq-mid_target)))\n",
    "    F7 = re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES = [\n",
    "    \"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "    \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"\n",
    "]\n",
    "\n",
    "def physical_features(freq, re_i, im_i):\n",
    "    freq = np.asarray(freq); re_i = np.asarray(re_i); im_i = np.asarray(im_i)\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    Rs = float(re_i[0]); Rpeak = float(re_i[idx_peak]); Rlow = float(re_i[-1])\n",
    "    Rct = max(Rpeak - Rs, 0.0)\n",
    "    arc_diam = Rlow - Rs\n",
    "    norm_arc = arc_diam / (Rs + 1e-9)\n",
    "    f_peak = float(freq[idx_peak])\n",
    "    tau_peak = 1.0/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K = min(10, len(freq)//3)\n",
    "    if K >= 4:\n",
    "        w_section = (2*np.pi*freq[-K:])**(-0.5)\n",
    "        re_section = re_i[-K:]\n",
    "        if len(np.unique(w_section)) > 2:\n",
    "            warburg_sigma = float(np.polyfit(w_section, re_section, 1)[0])\n",
    "        else:\n",
    "            warburg_sigma = np.nan\n",
    "    else:\n",
    "        warburg_sigma = np.nan\n",
    "    phase = np.arctan2(-im_i, re_i)\n",
    "    mid_mask = (freq>=1) & (freq<=100)\n",
    "    if mid_mask.sum()>2:\n",
    "        phase_mean_mid = float(phase[mid_mask].mean())\n",
    "        phase_std_mid  = float(phase[mid_mask].std())\n",
    "    else:\n",
    "        phase_mean_mid = np.nan; phase_std_mid = np.nan\n",
    "    phase_min = float(phase.min())\n",
    "    lf_mask = (freq<=1.0)\n",
    "    if lf_mask.sum() >= 4:\n",
    "        x = np.log10(freq[lf_mask]+1e-12); y = neg_im[lf_mask]\n",
    "        lf_slope = np.polyfit(x, y, 1)[0]\n",
    "    else:\n",
    "        lf_slope = np.nan\n",
    "    arc_quality = (neg_im.max() - neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS = [(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq, re_i, im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m], im_i[m])\n",
    "            feats += [z.mean(), z.std()]\n",
    "        else:\n",
    "            feats += [np.nan, np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq, re_i, im_i, segments=5):\n",
    "    logf = np.log10(freq)\n",
    "    edges = np.linspace(logf.min(), logf.max(), segments+1)\n",
    "    out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]\n",
    "            out += [np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else:\n",
    "            out += [np.nan, np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES = [\n",
    "    \"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "    \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"\n",
    "]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w = 2*np.pi*freq\n",
    "    tau = np.geomspace(tau_max, tau_min, n_tau)\n",
    "    WT = w[:,None]*tau[None,:]\n",
    "    denom = 1+WT**2\n",
    "    K_re = 1.0/denom\n",
    "    K_im = -WT/denom\n",
    "    R_inf = re_i[0]\n",
    "    y_re = re_i - R_inf\n",
    "    y_im = im_i\n",
    "    Y = np.concatenate([y_re, y_im])\n",
    "    K = np.vstack([K_re, K_im])\n",
    "    A = K.T @ K + lam*np.eye(n_tau)\n",
    "    b = K.T @ Y\n",
    "    gamma = linalg.solve(A,b,assume_a='pos')\n",
    "    gamma = np.clip(gamma,0,None)\n",
    "    return tau, gamma\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    try:\n",
    "        tau,gamma = compute_drt(freq,re_i,im_i,\n",
    "                                 cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,\n",
    "                                 cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau = np.log10(tau)\n",
    "        g_sum = gamma.sum()+1e-12\n",
    "        w_norm = gamma/g_sum\n",
    "        mean_logtau = float((w_norm*log_tau).sum())\n",
    "        var_logtau  = float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p = int(np.argmax(gamma))\n",
    "        peak_tau = float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid = np.median(log_tau)\n",
    "        frac_low = float(w_norm[log_tau<=mid].sum())\n",
    "        frac_high = 1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception:\n",
    "        return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i, im_i, temp, freq, include_names=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts += [re_i, im_i]\n",
    "        names += [f\"Re_{i}\" for i in range(len(re_i))] + [f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z = np.hypot(re_i, im_i)\n",
    "        basics=[re_i[0], re_i[-1], re_i[-1]-re_i[0], z.max(), z.mean(), z.std()]\n",
    "        parts.append(np.array(basics)); names += [\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        Ff=compute_F_features(freq,re_i,im_i); parts.append(np.array(Ff)); names += [f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        Pf=physical_features(freq,re_i,im_i); parts.append(np.array(Pf)); names += PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        Bf=band_stats(freq,re_i,im_i); parts.append(np.array(Bf))\n",
    "        for bi in range(len(BANDS)): names += [f\"band{bi}_mean\", f\"band{bi}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i); parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2): names += [f\"slope_re_seg{i}\", f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        Df=drt_features(freq,re_i,im_i); parts.append(np.array(Df)); names += DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names += [\"Feat_Temp\"]\n",
    "    vec = np.concatenate(parts).astype(float)\n",
    "    vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if include_names: return vec, names\n",
    "    return vec\n",
    "\n",
    "def build_shape_normalized(re_i, im_i, k: int = 5):\n",
    "    hf = float(np.nanmedian(re_i[:max(1, min(k, len(re_i)))]))\n",
    "    if not np.isfinite(hf) or abs(hf) < 1e-9:\n",
    "        hf = 1.0\n",
    "    return re_i / hf, im_i / hf\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. CAPACITY & CYCLES TARGETS\n",
    "# =========================\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY):\n",
    "        return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta = parse_cap_metadata(fp.stem)\n",
    "        if not meta:\n",
    "            continue\n",
    "        try:\n",
    "            mat = loadmat(fp, squeeze_me=True, struct_as_record=False)\n",
    "            arr = _find_matrix(mat)\n",
    "            cap = None\n",
    "            if arr is not None:\n",
    "                col = np.argmax(np.abs(arr[-50:, :]).mean(axis=0))\n",
    "                cap = float(np.nanmax(arr[:, col]))\n",
    "            else:\n",
    "                d = mat.get(\"data\", None)\n",
    "                if d is not None:\n",
    "                    def _cell_to_1d(x):\n",
    "                        a = np.array(x, dtype=object).squeeze()\n",
    "                        out=[]\n",
    "                        for e in a.flat:\n",
    "                            if isinstance(e, np.ndarray):\n",
    "                                out.append(float(np.nanmax(e.astype(float))) if e.size else np.nan)\n",
    "                            else:\n",
    "                                try: out.append(float(e))\n",
    "                                except Exception: out.append(np.nan)\n",
    "                        z = np.array(out, dtype=float)\n",
    "                        if z.ndim == 0: z = z[None]\n",
    "                        return z\n",
    "                    if hasattr(d, \"AhAccu\"):\n",
    "                        v = _cell_to_1d(getattr(d, \"AhAccu\"))\n",
    "                        if v.size: cap = float(np.nanmax(v))\n",
    "                    if cap is None and hasattr(d, \"WhAccu\"):\n",
    "                        v = _cell_to_1d(getattr(d, \"WhAccu\"))\n",
    "                        if v.size: cap = float(np.nanmax(v) / 3.7)\n",
    "            if cap is None or not np.isfinite(cap):\n",
    "                continue\n",
    "            meta[\"MeasuredCapacity_Ah\"] = cap\n",
    "            recs.append(meta)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(recs)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    ref = df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"] = df[\"MeasuredCapacity_Ah\"] / ref\n",
    "    df[\"SoH_percent\"] = df[\"NormCapacity\"] * 100.0\n",
    "    return df\n",
    "\n",
    "def _build_soh_to_cycle_interpolators(cap_df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "    maps={}\n",
    "    if cap_df.empty: return maps\n",
    "    for cid, g in cap_df.groupby(\"CellID\"):\n",
    "        g = g.sort_values(\"CycleIndex\")\n",
    "        x = np.asarray(g[\"SoH_percent\"].values, dtype=float)\n",
    "        y = np.asarray(g[\"CycleIndex\"].values, dtype=float)\n",
    "        order = np.argsort(x)            # ascending SoH\n",
    "        x_sorted = x[order]\n",
    "        y_sorted = y[order]\n",
    "        uniq = np.unique(x_sorted)\n",
    "        cyc_agg=[]\n",
    "        for s in uniq:\n",
    "            cyc_agg.append(float(np.nanmean(y_sorted[x_sorted==s])))\n",
    "        soh = uniq\n",
    "        cyc = np.asarray(cyc_agg, dtype=float)\n",
    "        maps[cid] = {\"soh\": soh, \"cyc\": cyc}\n",
    "    return maps\n",
    "\n",
    "def _tail_cpp_from_map(soh: np.ndarray, cyc: np.ndarray, k: int) -> Optional[float]:\n",
    "    \"\"\"cycles per 1% SoH near the *lowest* SoH end (first k points in ascending-SoH arrays).\"\"\"\n",
    "    if soh.size < 2: return None\n",
    "    k = max(2, min(k, soh.size))\n",
    "    xs = soh[:k]; ys = cyc[:k]\n",
    "    if len(np.unique(xs)) < 2: return None\n",
    "    slope = np.polyfit(xs, ys, 1)[0]   # d(cyc)/d(soh), negative\n",
    "    return abs(float(slope))\n",
    "\n",
    "def _interp_or_extrap_cycle_for_soh(cell_map: Dict[str, Dict[str, Any]],\n",
    "                                    cell_id: str,\n",
    "                                    soh_val: float,\n",
    "                                    k_tail: int,\n",
    "                                    fallback_cpp: float) -> Optional[float]:\n",
    "    \"\"\"Interpolate if inside range, else linear *extrapolate downwards* using tail CPP.\"\"\"\n",
    "    m = cell_map.get(cell_id)\n",
    "    if not m: return None\n",
    "    soh = m[\"soh\"]; cyc = m[\"cyc\"]\n",
    "    if soh.size < 2: return None\n",
    "    if soh_val >= soh.min() and soh_val <= soh.max():\n",
    "        return float(np.interp(soh_val, soh, cyc, left=cyc[0], right=cyc[-1]))\n",
    "    if soh_val < soh.min():\n",
    "        cpp = _tail_cpp_from_map(soh, cyc, k_tail) or float(fallback_cpp)\n",
    "        delta = float(soh.min() - soh_val)\n",
    "        return float(cyc[0] + cpp * delta)\n",
    "    # (We don't extrapolate above 100%; clamp)\n",
    "    return float(cyc[-1])\n",
    "\n",
    "def estimate_cpp_per_cell(capacity_df: pd.DataFrame,\n",
    "                          window:int, min_points:int)->Dict[str,float]:\n",
    "    cpp={}\n",
    "    for cid,grp in capacity_df.groupby(\"CellID\"):\n",
    "        g=grp.sort_values(\"CycleIndex\")\n",
    "        if g.shape[0]<min_points: continue\n",
    "        tail=g.tail(window)\n",
    "        x=tail[\"CycleIndex\"].values.astype(float)\n",
    "        y=tail[\"SoH_percent\"].values.astype(float)\n",
    "        if len(np.unique(x))<2: continue\n",
    "        slope=np.polyfit(x,y,1)[0]\n",
    "        if slope >= -1e-6:\n",
    "            continue\n",
    "        cpp[cid]=1.0/abs(slope)\n",
    "    return cpp\n",
    "\n",
    "def build_cpp_map(cap_df: pd.DataFrame):\n",
    "    if cap_df.empty: return {}, cfg.CPP_FALLBACK\n",
    "    cpp_map=estimate_cpp_per_cell(\n",
    "        cap_df[[\"CellID\",\"CycleIndex\",\"SoH_percent\"]],\n",
    "        cfg.CPP_ROLLING_WINDOW, cfg.CPP_MIN_POINTS\n",
    "    )\n",
    "    if not cpp_map:\n",
    "        return {}, cfg.CPP_FALLBACK\n",
    "    return cpp_map, float(np.median(list(cpp_map.values())))\n",
    "\n",
    "def _calibrate_cycle_scale(cap_df: pd.DataFrame, target_80: Optional[float]) -> float:\n",
    "    \"\"\"★ Compute a multiplier so median(cycles@80%) == target_80.\"\"\"\n",
    "    if cap_df.empty or target_80 is None: return 1.0\n",
    "    maps = _build_soh_to_cycle_interpolators(cap_df)\n",
    "    vals=[]\n",
    "    for cid in cap_df[\"CellID\"].unique():\n",
    "        m = maps.get(cid)\n",
    "        if not m: continue\n",
    "        c80 = np.interp(80.0, m[\"soh\"], m[\"cyc\"], left=m[\"cyc\"][0], right=m[\"cyc\"][-1])\n",
    "        if np.isfinite(c80) and c80>0: vals.append(float(c80))\n",
    "    if not vals: return 1.0\n",
    "    med = float(np.median(vals))\n",
    "    if med <= 0: return 1.0\n",
    "    return float(target_80/med)\n",
    "\n",
    "def get_cpp(meta: dict, cpp_map: Dict[str,float], global_cpp: float):\n",
    "    if not meta: return global_cpp\n",
    "    return cpp_map.get(meta.get(\"CellID\"), global_cpp)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8. DATASET BUILD\n",
    "# =========================\n",
    "def load_single_eis_mat(fp: Path):\n",
    "    meta = parse_eis_metadata(fp.stem)\n",
    "    if meta is None:\n",
    "        raise ValueError(f\"Bad filename: {fp.name}\")\n",
    "    freq,re_z,im_z = load_mat_eis(fp)\n",
    "    re_i=_interp_channel(freq, re_z, CANON_FREQ)\n",
    "    im_i=_interp_channel(freq, im_z, CANON_FREQ)\n",
    "    vec=build_feature_vector(re_i, im_i, meta[\"Temp\"], CANON_FREQ)\n",
    "    return vec, meta, re_i, im_i\n",
    "\n",
    "def _build_cycles_targets(meta_df: pd.DataFrame,\n",
    "                          cap_df: pd.DataFrame,\n",
    "                          cycle_scale: float,           # ★ scale to real-world cycles\n",
    "                          k_tail: int) -> Tuple[np.ndarray, Dict[float, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_cycle_index: absolute cycle index at sample's SoH (scaled)\n",
    "      y_rem_dict: dict threshold -> remaining cycles to that threshold (scaled, with tail extrapolation)\n",
    "    \"\"\"\n",
    "    y_cycle_index = np.full(len(meta_df), np.nan, dtype=float)\n",
    "    y_rem_dict: Dict[float, np.ndarray] = {thr: np.full(len(meta_df), np.nan, dtype=float)\n",
    "                                           for thr in cfg.TARGET_SOH_THRESHOLDS}\n",
    "    if cap_df.empty:\n",
    "        return y_cycle_index, y_rem_dict\n",
    "\n",
    "    maps = _build_soh_to_cycle_interpolators(cap_df)\n",
    "\n",
    "    # global fallback CPP from capacity df (cycles per 1% SoH)\n",
    "    cpp_map, cpp_global = build_cpp_map(cap_df)\n",
    "\n",
    "    for i, row in meta_df.reset_index(drop=True).iterrows():\n",
    "        cid = row[\"CellID\"]\n",
    "        soh_here = float(row[\"SoH_cont\"])\n",
    "\n",
    "        cyc_here = _interp_or_extrap_cycle_for_soh(\n",
    "            maps, cid, soh_here, k_tail=k_tail, fallback_cpp=cpp_map.get(cid, cpp_global)\n",
    "        )\n",
    "        if cyc_here is None:\n",
    "            continue\n",
    "        y_cycle_index[i] = float(max(0.0, cyc_here))\n",
    "\n",
    "        # remaining to thresholds (with extrapolation below min SoH)\n",
    "        for thr in cfg.TARGET_SOH_THRESHOLDS:\n",
    "            cthr = _interp_or_extrap_cycle_for_soh(\n",
    "                maps, cid, float(thr), k_tail=k_tail, fallback_cpp=cpp_map.get(cid, cpp_global)\n",
    "            )\n",
    "            if cthr is None:\n",
    "                y_rem_dict[thr][i] = np.nan\n",
    "            else:\n",
    "                y_rem_dict[thr][i] = float(max(0.0, cthr - cyc_here))\n",
    "\n",
    "    # apply scaling ★\n",
    "    if cycle_scale and cycle_scale != 1.0:\n",
    "        y_cycle_index *= float(cycle_scale)\n",
    "        for thr in y_rem_dict:\n",
    "            y_rem_dict[thr] *= float(cycle_scale)\n",
    "\n",
    "    return y_cycle_index, y_rem_dict\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_df: Optional[pd.DataFrame], cycle_scale: float):\n",
    "    files = sorted(eis_dir.rglob(\"*.mat\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .mat spectra in {eis_dir}\")\n",
    "\n",
    "    f0,r0,i0 = load_mat_eis(files[0])\n",
    "    re0=_interp_channel(f0,r0,CANON_FREQ); im0=_interp_channel(f0,i0,CANON_FREQ)\n",
    "    _, feature_names = build_feature_vector(re0, im0, 25.0, CANON_FREQ, include_names=True)\n",
    "\n",
    "    feats=[]; rows=[]; shape_feats=[]\n",
    "    for fp in tqdm(files, desc=\"Loading training spectra\"):\n",
    "        try:\n",
    "            v, m, rei, imi = load_single_eis_mat(fp)\n",
    "            feats.append(v); rows.append(m)\n",
    "            if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL or cfg.SOC_INCLUDE_SHAPE_MODEL) and cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "                rsh, ish = build_shape_normalized(rei, imi)\n",
    "                shape_vec = build_feature_vector(rsh, ish, m[\"Temp\"], CANON_FREQ)\n",
    "                shape_feats.append(shape_vec)\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip] {fp.name}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid training spectra after filtering.\")\n",
    "\n",
    "    X = np.vstack(feats)\n",
    "    X_shape = np.vstack(shape_feats) if shape_feats else None\n",
    "    meta_df = pd.DataFrame(rows)\n",
    "\n",
    "    # SoH refinement\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup = cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        refined=[]\n",
    "        for cid, stage, fallback in zip(meta_df.CellID, meta_df.SOH_stage, meta_df.RealSOH_file):\n",
    "            nc = lookup.get((cid, stage))\n",
    "            refined.append(100.0*nc if nc is not None else fallback)\n",
    "        meta_df[\"SoH_cont\"]=refined\n",
    "    else:\n",
    "        meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "\n",
    "    # Targets: SoC / SoH\n",
    "    y_soc = meta_df[\"SOC\"].astype(float).values\n",
    "    y_soh = meta_df[\"SoH_cont\"].values\n",
    "\n",
    "    # Cycles targets (absolute + remaining to thresholds) ★\n",
    "    if cfg.ENABLE_CYCLES_MODEL and cap_df is not None:\n",
    "        y_cycle_index, y_rem_dict = _build_cycles_targets(\n",
    "            meta_df, cap_df, cycle_scale=cycle_scale, k_tail=cfg.CYCLE_TAIL_POINTS\n",
    "        )\n",
    "    else:\n",
    "        y_cycle_index = np.full(len(meta_df), np.nan)\n",
    "        y_rem_dict = {thr: np.full(len(meta_df), np.nan) for thr in cfg.TARGET_SOH_THRESHOLDS}\n",
    "\n",
    "    soh_var = float(np.var(y_soh))\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[DATA] SoH range: {y_soh.min():.2f} – {y_soh.max():.2f} (var={soh_var:.3f})\")\n",
    "        if soh_var < 1.0:\n",
    "            print(\"[WARN] Low SoH variance → model may output near-constant SoH.\")\n",
    "\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        pd.concat(\n",
    "            [meta_df.reset_index(drop=True),\n",
    "             pd.DataFrame(X, columns=feature_names)], axis=1\n",
    "        ).to_parquet(cfg.MODEL_DIR/\"training_features.parquet\", index=False)\n",
    "\n",
    "    return meta_df, X, (X_shape, feature_names), y_soc, y_soh, y_cycle_index, y_rem_dict\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9. SPLITTING\n",
    "# =========================\n",
    "def cell_split_mask(meta_df: pd.DataFrame):\n",
    "    cells = meta_df.CellID.unique()\n",
    "    rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test = max(1, int(len(cells)*cfg.TEST_FRAC))\n",
    "    test_cells = rng.choice(cells, size=n_test, replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10. TRAINING\n",
    "# =========================\n",
    "def _fit_gpr(X, y, seed, max_samples):\n",
    "    dim = X.shape[1]\n",
    "    kernel = RBF(length_scale=np.ones(dim)*3.0,\n",
    "                 length_scale_bounds=(1e-1,1e6)) + \\\n",
    "             WhiteKernel(noise_level=1e-2,\n",
    "                         noise_level_bounds=(1e-6,1e-1))\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel, alpha=0.0, normalize_y=True,\n",
    "        random_state=seed, n_restarts_optimizer=3\n",
    "    )\n",
    "    if X.shape[0] > max_samples:\n",
    "        idx = np.random.default_rng(seed).choice(\n",
    "            X.shape[0], size=max_samples, replace=False)\n",
    "        gpr.fit(X[idx], y[idx])\n",
    "    else:\n",
    "        gpr.fit(X, y)\n",
    "    return gpr\n",
    "\n",
    "def _fit_hgb(X, y):\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_iter=600,\n",
    "        l2_regularization=1e-3, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    hgb.fit(X, y)\n",
    "    return hgb\n",
    "\n",
    "def _fit_knn(X, y, k=15):\n",
    "    knn = KNeighborsRegressor(n_neighbors=max(3, min(k, len(y)-1)), weights=\"distance\")\n",
    "    knn.fit(X, y)\n",
    "    return knn\n",
    "\n",
    "def _evaluate(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred), math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict):\n",
    "    X_shape, feature_names = shape_bundle\n",
    "    mask_test = cell_split_mask(meta_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_raw)\n",
    "\n",
    "    # ----- SoC (candidates + shape) -----\n",
    "    y_soc_train = y_soc.copy()\n",
    "    if cfg.SOC_LABEL_JITTER and cfg.SOC_LABEL_JITTER > 0:\n",
    "        rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "        y_soc_train = np.clip(y_soc_train + rng.normal(0.0, cfg.SOC_LABEL_JITTER, size=y_soc_train.shape), 0.0, 100.0)\n",
    "\n",
    "    soc_candidates = {}\n",
    "    soc_gpr = _fit_gpr(X_s, y_soc_train, cfg.RANDOM_STATE, cfg.SOC_MAX_GPR_TRAIN_SAMPLES)\n",
    "    r2, rmse = _evaluate(y_soc[mask_test], soc_gpr.predict(X_s[mask_test]))\n",
    "    soc_candidates[\"soc_gpr_raw\"] = (soc_gpr, r2, rmse)\n",
    "\n",
    "    soc_hgb = _fit_hgb(X_s[~mask_test], y_soc_train[~mask_test])\n",
    "    r2h, rmseh = _evaluate(y_soc[mask_test], soc_hgb.predict(X_s[mask_test]))\n",
    "    soc_candidates[\"soc_hgb_raw\"] = (soc_hgb, r2h, rmseh)\n",
    "\n",
    "    soc_knn = _fit_knn(X_s[~mask_test], y_soc_train[~mask_test], k=15)\n",
    "    r2k, rmsek = _evaluate(y_soc[mask_test], soc_knn.predict(X_s[mask_test]))\n",
    "    soc_candidates[\"soc_knn_raw\"] = (soc_knn, r2k, rmsek)\n",
    "\n",
    "    soc_shape_model=soc_shape_scaler=None\n",
    "    soc_shape_metrics=None\n",
    "    if cfg.SOC_INCLUDE_SHAPE_MODEL and (X_shape is not None):\n",
    "        soc_shape_scaler = StandardScaler()\n",
    "        Xs = soc_shape_scaler.fit_transform(X_shape)\n",
    "        soc_shape_model = _fit_gpr(Xs, y_soc_train, cfg.RANDOM_STATE, cfg.SOC_MAX_GPR_TRAIN_SAMPLES)\n",
    "        sp = soc_shape_model.predict(Xs[mask_test])\n",
    "        r2s, rmses = _evaluate(y_soc[mask_test], sp)\n",
    "        soc_candidates[\"soc_gpr_shape\"] = (soc_shape_model, r2s, rmses)\n",
    "        soc_shape_metrics = {\"r2\": r2s, \"rmse\": rmses}\n",
    "\n",
    "    soc_best_name = max(soc_candidates.keys(), key=lambda k: soc_candidates[k][1])\n",
    "    soc_best_model, soc_best_r2, soc_best_rmse = soc_candidates[soc_best_name]\n",
    "\n",
    "    val_pred = soc_best_model.predict(X_s[mask_test])\n",
    "    soc_calibrator = IsotonicRegression(y_min=0.0, y_max=100.0, out_of_bounds=\"clip\")\n",
    "    soc_calibrator.fit(val_pred, y_soc[mask_test])\n",
    "\n",
    "    if cfg.Verbose if hasattr(cfg,'Verbose') else cfg.VERBOSE:\n",
    "        print(f\"[SoC] Candidates (R2 | RMSE):\")\n",
    "        for k,(m,r,rm) in soc_candidates.items():\n",
    "            print(f\"      - {k:12s}  R2={r:.3f}  RMSE={rm:.2f}\")\n",
    "        if soc_shape_metrics:\n",
    "            print(f\"      - {'shape':12s}  R2={soc_shape_metrics['r2']:.3f}  RMSE={soc_shape_metrics['rmse']:.2f}\")\n",
    "        print(f\"[SoC] Selected base = {soc_best_name}\")\n",
    "\n",
    "    # ----- SoH -----\n",
    "    soh_candidates = {}\n",
    "    soh_gpr = _fit_gpr(X_s, y_soh, cfg.RANDOM_STATE, cfg.MAX_GPR_TRAIN_SAMPLES)\n",
    "    r2g, rmseg = _evaluate(y_soh[mask_test], soh_gpr.predict(X_s[mask_test]))\n",
    "    soh_candidates[\"gpr_raw\"] = (soh_gpr, r2g, rmseg)\n",
    "\n",
    "    soh_hgb = _fit_hgb(X_s[~mask_test], y_soh[~mask_test])\n",
    "    r2h2, rmseh2 = _evaluate(y_soh[mask_test], soh_hgb.predict(X_s[mask_test]))\n",
    "    soh_candidates[\"hgb_raw\"] = (soh_hgb, r2h2, rmseh2)\n",
    "\n",
    "    shape_model=shape_scaler=None\n",
    "    shape_metrics=None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and (X_shape is not None):\n",
    "        shape_scaler = StandardScaler()\n",
    "        X_shape_s = shape_scaler.fit_transform(X_shape)\n",
    "        shape_model = _fit_gpr(X_shape_s, y_soh, cfg.RANDOM_STATE, cfg.MAX_GPR_TRAIN_SAMPLES)\n",
    "        spred = shape_model.predict(X_shape_s[mask_test])\n",
    "        r2s2, rmses2 = _evaluate(y_soh[mask_test], spred)\n",
    "        soh_candidates[\"gpr_shape\"] = (shape_model, r2s2, rmses2)\n",
    "        shape_metrics = {\"r2\": r2s2, \"rmse\": rmses2}\n",
    "\n",
    "    soh_best_name = max([\"gpr_raw\",\"hgb_raw\"], key=lambda k: soh_candidates[k][1])\n",
    "    soh_best_model, soh_best_r2, soh_best_rmse = soh_candidates[soh_best_name]\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoH] GPR_raw:  R2={r2g:.3f} RMSE={rmseg:.2f}\")\n",
    "        print(f\"[SoH] HGB_raw: R2={r2h2:.3f} RMSE={rmseh2:.2f}\")\n",
    "        if shape_metrics:\n",
    "            print(f\"[SoH] ShapeGP: R2={shape_metrics['r2']:.3f} RMSE={shape_metrics['rmse']:.2f}\")\n",
    "        print(f\"[SoH] Selected raw model = {soh_best_name}\")\n",
    "\n",
    "    # SoH-space OOD stats\n",
    "    cov = np.cov(X_s.T)\n",
    "    try:\n",
    "        cov_inv = np.linalg.pinv(cov)\n",
    "    except Exception:\n",
    "        cov_inv = np.eye(cov.shape[0])\n",
    "    center = X_s.mean(axis=0)\n",
    "\n",
    "    # --- SoC OOD stats ---\n",
    "    soc_center = soc_cov_inv = None\n",
    "    soc_mahal_thresh = None\n",
    "    try:\n",
    "        X_soc_train = X_s[~mask_test]\n",
    "        soc_center = X_soc_train.mean(axis=0)\n",
    "        soc_cov = np.cov(X_soc_train.T)\n",
    "        soc_cov_inv = np.linalg.pinv(soc_cov)\n",
    "        dists = []\n",
    "        for i in range(X_soc_train.shape[0]):\n",
    "            diff = X_soc_train[i] - soc_center\n",
    "            dists.append(float(np.sqrt(diff @ soc_cov_inv @ diff.T)))\n",
    "        soc_mahal_thresh = float(np.quantile(dists, cfg.OOD_SOC_Q))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    X_soc_train_for_knn = X_s[~mask_test]\n",
    "    y_soc_train_vals = y_soc[~mask_test]\n",
    "\n",
    "    # ----- Cycles models -----\n",
    "    cycles_models = {}\n",
    "    cycles_metrics = {}\n",
    "    if cfg.ENABLE_CYCLES_MODEL:\n",
    "        m_valid = np.isfinite(y_cycle_index)\n",
    "        if m_valid.sum() >= 10:\n",
    "            cyc_hgb = _fit_hgb(X_s[m_valid], y_cycle_index[m_valid])\n",
    "            r2c, rmsec = _evaluate(y_cycle_index[mask_test & m_valid], cyc_hgb.predict(X_s[mask_test & m_valid]) if (mask_test & m_valid).any() else y_cycle_index[m_valid])\n",
    "            cycles_models[\"absolute\"] = {\"model\": cyc_hgb, \"scaler\": scaler}\n",
    "            cycles_metrics[\"absolute\"] = {\"r2\": float(r2c), \"rmse\": float(rmsec), \"n\": int(m_valid.sum())}\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[Cycles] absolute: R2={r2c:.3f} RMSE={rmsec:.2f}  n={m_valid.sum()}\")\n",
    "        for thr, arr in y_rem_dict.items():\n",
    "            mv = np.isfinite(arr)\n",
    "            if mv.sum() < 10:\n",
    "                continue\n",
    "            rem_model = _fit_hgb(X_s[mv], arr[mv])\n",
    "            r2r, rmser = _evaluate(arr[mask_test & mv], rem_model.predict(X_s[mask_test & mv]) if (mask_test & mv).any() else arr[mv])\n",
    "            cycles_models[str(int(thr))] = {\"model\": rem_model, \"scaler\": scaler}\n",
    "            cycles_metrics[str(int(thr))] = {\"r2\": float(r2r), \"rmse\": float(rmser), \"n\": int(mv.sum())}\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[Cycles] remaining→{int(thr)}%: R2={r2r:.3f} RMSE={rmser:.2f}  n={mv.sum()}\")\n",
    "\n",
    "    bundle = {\n",
    "        \"shared_scaler\": scaler,\n",
    "        # SoC\n",
    "        \"soc_model\": soc_best_model,\n",
    "        \"soc_model_name\": soc_best_name,\n",
    "        \"soc_shape_scaler\": soc_shape_scaler,\n",
    "        \"soc_shape_model\": soc_shape_model,\n",
    "        \"soc_calibrator\": soc_calibrator,\n",
    "        # SoH\n",
    "        \"soh_model\": soh_best_model,\n",
    "        \"soh_model_name\": soh_best_name,\n",
    "        \"shape_scaler\": shape_scaler,\n",
    "        \"shape_model\": shape_model,\n",
    "        # Cycles\n",
    "        \"cycles_models\": cycles_models,\n",
    "        \"cycles_metrics\": cycles_metrics,\n",
    "        # Meta\n",
    "        \"freq_grid\": CANON_FREQ,\n",
    "        \"feature_version\": cfg.FEATURE_VERSION,\n",
    "        \"feature_manifest\": feature_names,\n",
    "        \"config_signature\": config_signature(cfg),\n",
    "        \"config\": to_jsonable(asdict(cfg)),\n",
    "        \"cycle_scale\": float(CYCLE_SCALE_GLOBAL),  # ★ save used scale\n",
    "        \"metrics\": {\n",
    "            \"soc_r2_selected\": soc_best_r2,\n",
    "            \"soc_rmse_selected\": soc_best_rmse,\n",
    "            \"soh_r2_selected\": soh_best_r2,\n",
    "            \"soh_rmse_selected\": soh_best_rmse\n",
    "        },\n",
    "        \"soc_candidates_metrics\": {\n",
    "            \"soc_gpr_raw\": {\"r2\": r2, \"rmse\": rmse},\n",
    "            \"soc_hgb_raw\": {\"r2\": r2h, \"rmse\": rmseh},\n",
    "            \"soc_knn_raw\": {\"r2\": r2k, \"rmse\": rmsek},\n",
    "            \"soc_gpr_shape\": soc_shape_metrics\n",
    "        },\n",
    "        \"soh_candidates_metrics\": {\n",
    "            \"gpr_raw\": {\"r2\": r2g, \"rmse\": rmseg},\n",
    "            \"hgb_raw\": {\"r2\": r2h2, \"rmse\": rmseh2},\n",
    "            \"gpr_shape\": shape_metrics\n",
    "        },\n",
    "        \"train_mahal\": {\"center\": center.tolist(), \"cov_inv\": cov_inv.tolist()},\n",
    "        \"soc_train_mahal\": {\n",
    "            \"center\": None if soc_center is None else soc_center.tolist(),\n",
    "            \"cov_inv\": None if soc_cov_inv is None else soc_cov_inv.tolist(),\n",
    "            \"threshold\": soc_mahal_thresh\n",
    "        },\n",
    "        \"soc_knn_X\": X_soc_train_for_knn,\n",
    "        \"soc_knn_y\": y_soc_train_vals,\n",
    "    }\n",
    "    out_path = cfg.MODEL_DIR/\"eis_soc_soh_cycles_models.joblib\"\n",
    "    joblib.dump(bundle, out_path)\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[MODEL] Saved bundle → {out_path}\")\n",
    "        print(json.dumps(bundle[\"metrics\"], indent=2))\n",
    "    return bundle\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 11. LOAD (WITH LEGACY + SIGNATURE CHECK)\n",
    "# =========================\n",
    "def load_bundle():\n",
    "    path_new = cfg.MODEL_DIR / \"eis_soc_soh_cycles_models.joblib\"\n",
    "    path_old = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "    if path_new.exists():\n",
    "        bundle = joblib.load(path_new)\n",
    "    else:\n",
    "        bundle = joblib.load(path_old)\n",
    "    if \"shared_scaler\" not in bundle:\n",
    "        bundle[\"shared_scaler\"] = bundle.get(\"soh_scaler\") or bundle.get(\"soc_scaler\")\n",
    "    for key in [\"shared_scaler\",\"soc_model\",\"soh_model\",\"freq_grid\"]:\n",
    "        if key not in bundle:\n",
    "            raise KeyError(f\"Bundle missing required key: {key}\")\n",
    "    return bundle\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 12. INFERENCE FEATURIZATION\n",
    "# =========================\n",
    "def featurize_any(file_path: Path, bundle):\n",
    "    freq_grid = bundle[\"freq_grid\"]\n",
    "    meta = parse_eis_metadata(file_path.stem)\n",
    "    freq,re_raw,im_raw, used_freq = load_any_inference(file_path)\n",
    "    if not used_freq:\n",
    "        warnings.warn(f\"[{file_path.name}] No frequency column found. Using geometric grid fallback.\")\n",
    "    re_i=_interp_channel(freq, re_raw, freq_grid)\n",
    "    im_i=_interp_channel(freq, im_raw, freq_grid)\n",
    "    if meta is None and cfg.TEST_TEMPERATURE_OVERRIDE is not None:\n",
    "        temp = cfg.TEST_TEMPERATURE_OVERRIDE\n",
    "    else:\n",
    "        temp = meta[\"Temp\"] if meta else -1\n",
    "    vec = build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "    norm_vec=None\n",
    "    if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL or cfg.SOC_INCLUDE_SHAPE_MODEL) and \\\n",
    "       (bundle.get(\"shape_model\") is not None or bundle.get(\"soc_shape_model\") is not None):\n",
    "        if cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "            rsh, ish = build_shape_normalized(re_i, im_i)\n",
    "            norm_vec = build_feature_vector(rsh, ish, temp, freq_grid)\n",
    "    checksum = hashlib.sha1(np.ascontiguousarray(vec).tobytes()).hexdigest()\n",
    "    return vec, norm_vec, meta, checksum\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 13. OOD UTILITIES\n",
    "# =========================\n",
    "def mahalanobis_distance(x, center, cov_inv):\n",
    "    diff = x - center\n",
    "    return float(np.sqrt(diff @ cov_inv @ diff.T))\n",
    "\n",
    "def gp_ard_norm(Xp, model):\n",
    "    try:\n",
    "        K = model.kernel_\n",
    "        from sklearn.gaussian_process.kernels import RBF\n",
    "        rbf = None\n",
    "        if hasattr(K,\"k1\") and isinstance(K.k1,RBF): rbf=K.k1\n",
    "        elif hasattr(K,\"k2\") and isinstance(K.k2,RBF): rbf=K.k2\n",
    "        if rbf is None: return None\n",
    "        ls = np.atleast_1d(rbf.length_scale)\n",
    "        z = (Xp / ls).ravel()\n",
    "        return float(np.linalg.norm(z))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 14. PROJECTION PLOT\n",
    "# =========================\n",
    "def _estimate_cpp_from_predictions(soh_current: float, cycles_to_map: Dict[float, float]) -> float:\n",
    "    usable = [(thr, c) for thr, c in cycles_to_map.items() if c and c > 0 and soh_current > thr]\n",
    "    if not usable:\n",
    "        return cfg.CPP_FALLBACK\n",
    "    thr, cyc = sorted(usable, key=lambda x: x[0])[0]\n",
    "    delta = max(1e-6, soh_current - float(thr))\n",
    "    return float(cyc / delta)\n",
    "\n",
    "def build_projection(soh_current, cpp, lower, exponent=None, n=160):\n",
    "    if soh_current <= lower or cpp <= 0:\n",
    "        return np.array([0.0]), np.array([soh_current])\n",
    "    total = (soh_current - lower) * cpp\n",
    "    cycles = np.linspace(0, total, n)\n",
    "    S0 = soh_current; Smin=lower\n",
    "    if exponent is None: exponent = cfg.PLOT_EXPONENT\n",
    "    soh_curve = Smin + (S0 - Smin)*(1 - cycles/total)**exponent\n",
    "    return cycles, soh_curve\n",
    "\n",
    "def plot_projection(file_base, soh_current, soh_std, cycles_to_map, cpp_hint, ood_flag, out_path, thresholds):\n",
    "    if not thresholds: thresholds = (50.0, 40.0)\n",
    "    min_thr = min(thresholds)\n",
    "    if soh_current <= min_thr:\n",
    "        return\n",
    "\n",
    "    cpp = _estimate_cpp_from_predictions(soh_current, cycles_to_map)\n",
    "    if not np.isfinite(cpp) or cpp <= 0:\n",
    "        cpp = cpp_hint if (cpp_hint and cpp_hint > 0) else cfg.CPP_FALLBACK\n",
    "\n",
    "    cycles, curve = build_projection(soh_current, cpp, min_thr)\n",
    "    plt.figure(figsize=(6.4,4))\n",
    "    plt.plot(cycles, curve, lw=2, label=\"Projected SoH (approx)\")\n",
    "\n",
    "    for thr in thresholds:\n",
    "        style = \"--\" if thr >= 50 else \":\"\n",
    "        color = \"orange\" if thr >= 50 else \"red\"\n",
    "        plt.axhline(thr, color=color, ls=style, label=f\"{int(thr)}%\")\n",
    "        x = float(cycles_to_map.get(thr, 0.0) or 0.0)\n",
    "        if x > 0:\n",
    "            plt.axvline(x, color=color, ls=\"-.\" if thr>=50 else \":\")\n",
    "            plt.scatter([x],[thr], s=45)\n",
    "            txty = thr + (1.0 if thr>=50 else -2.0)\n",
    "            plt.text(x, txty, f\"{x:.0f} cyc\", ha=\"center\", fontsize=8, color=color)\n",
    "\n",
    "    plt.scatter([0],[soh_current], c=\"green\", s=55, label=f\"Current {soh_current:.2f}%\")\n",
    "    plt.text(0, soh_current+0.7, f\"±{soh_std:.2f}\", color=\"green\", fontsize=8)\n",
    "\n",
    "    if ood_flag:\n",
    "        plt.text(0.98,0.05,\"OOD\", transform=plt.gca().transAxes,\n",
    "                 ha=\"right\", va=\"bottom\", color=\"crimson\", fontsize=11,\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"crimson\"))\n",
    "\n",
    "    plt.xlabel(\"Remaining Cycles\")\n",
    "    plt.ylabel(\"SoH (%)\")\n",
    "    plt.title(f\"RUL Projection – {file_base}\")\n",
    "    plt.grid(alpha=0.35)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 15. INFERENCE (SINGLE FILE)\n",
    "# =========================\n",
    "def predict_file(file_path: Path, bundle, cpp_map, global_cpp):\n",
    "    vec, norm_vec, meta, checksum = featurize_any(file_path, bundle)\n",
    "\n",
    "    scaler = bundle[\"shared_scaler\"]\n",
    "    X = scaler.transform(vec.reshape(1,-1))\n",
    "\n",
    "    # ----- SoC -----\n",
    "    soc_model=bundle[\"soc_model\"]; soc_model_name=bundle.get(\"soc_model_name\",\"unknown\")\n",
    "    if isinstance(soc_model, GaussianProcessRegressor):\n",
    "        sm, ss = soc_model.predict(X, return_std=True)\n",
    "        soc_mean = float(sm[0]); soc_std=float(ss[0])\n",
    "    else:\n",
    "        soc_mean = float(soc_model.predict(X)[0])\n",
    "        soc_std  = float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0))\n",
    "\n",
    "    soc_shape_mean = None; soc_shape_std = None\n",
    "    soc_shape_model = bundle.get(\"soc_shape_model\")\n",
    "    soc_shape_scaler = bundle.get(\"soc_shape_scaler\")\n",
    "    if cfg.SOC_INCLUDE_SHAPE_MODEL and (soc_shape_model is not None) and (norm_vec is not None):\n",
    "        Xs = soc_shape_scaler.transform(norm_vec.reshape(1,-1))\n",
    "        if isinstance(soc_shape_model, GaussianProcessRegressor):\n",
    "            sm2, ss2 = soc_shape_model.predict(Xs, return_std=True)\n",
    "            soc_shape_mean=float(sm2[0]); soc_shape_std=float(ss2[0])\n",
    "        else:\n",
    "            soc_shape_mean=float(soc_shape_model.predict(Xs)[0]); soc_shape_std=float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0))\n",
    "        soc_mean = 0.5*(soc_mean + soc_shape_mean)\n",
    "        soc_std  = float(np.sqrt(0.5*(soc_std**2 + (soc_shape_std or soc_std)**2)))\n",
    "\n",
    "    # SoC OOD check\n",
    "    soc_mahal = None; soc_oob = False\n",
    "    soc_mahal_info = bundle.get(\"soc_train_mahal\")\n",
    "    if cfg.OOD_SOC_ENABLE and soc_mahal_info and soc_mahal_info.get(\"center\") is not None:\n",
    "        c = np.array(soc_mahal_info[\"center\"]); inv = np.array(soc_mahal_info[\"cov_inv\"])\n",
    "        diff = (X[0] - c)\n",
    "        try:\n",
    "            soc_mahal = float(np.sqrt(diff @ inv @ diff.T))\n",
    "        except Exception:\n",
    "            soc_mahal = None\n",
    "        thr = float(soc_mahal_info.get(\"threshold\", np.inf))\n",
    "        soc_oob = (soc_mahal is not None and thr is not None and soc_mahal > thr)\n",
    "\n",
    "    soc_cal = bundle.get(\"soc_calibrator\")\n",
    "    if soc_cal is not None and (not soc_oob or cfg.SOC_CALIBRATE_ON_OOD):\n",
    "        try:\n",
    "            soc_mean = float(soc_cal.predict([soc_mean])[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    soc_mean = float(np.clip(soc_mean, 0.0, 100.0))\n",
    "\n",
    "    # OOD blend with stronger prior as severity increases ★\n",
    "    if cfg.OOD_SOC_ENABLE and soc_oob:\n",
    "        thr = float(bundle[\"soc_train_mahal\"].get(\"threshold\", np.inf))\n",
    "        delta = max(0.0, (soc_mahal or 0.0) - (thr if np.isfinite(thr) else 0.0))\n",
    "        s = max(1e-6, cfg.OOD_SOC_SHRINK_SCALE)\n",
    "\n",
    "        # severity in [0,1]\n",
    "        severity = float(max(0.0, min(1.0, delta/(s*6.0))))\n",
    "        prior_cap = float(cfg.OOD_SOC_PRIOR_MAX_WEIGHT)\n",
    "\n",
    "        # KNN prior in training space\n",
    "        prior_val = cfg.OOD_SOC_PRIOR\n",
    "        if cfg.SOC_OOD_USE_KNN and cfg.OOD_SOC_PRIOR_MODE.lower() == \"knn\":\n",
    "            try:\n",
    "                Xtr = bundle.get(\"soc_knn_X\", None)\n",
    "                ytr = bundle.get(\"soc_knn_y\", None)\n",
    "                if Xtr is not None and ytr is not None and Xtr.shape[0] >= 3:\n",
    "                    d = np.linalg.norm(Xtr - X[0], axis=1)\n",
    "                    k = min(cfg.SOC_OOD_K, Xtr.shape[0])\n",
    "                    idx = np.argpartition(d, k-1)[:k]\n",
    "                    w = 1.0 / (d[idx] + 1e-6)\n",
    "                    prior_val = float(np.sum(w * ytr[idx]) / np.sum(w))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        w_prior = min(prior_cap, 0.2 + 0.8*severity)\n",
    "        has_shape_soc = (soc_shape_model is not None) and (norm_vec is not None)\n",
    "        w_shape = (0.0 if not has_shape_soc else min(cfg.OOD_SOC_SHAPE_MAX_WEIGHT, 0.3*(1.0 - severity)))\n",
    "        w_raw = max(cfg.OOD_SOC_W_MIN, 1.0 - w_prior - w_shape)\n",
    "\n",
    "        soc_mean = (w_raw * soc_mean) + (w_shape * (soc_shape_mean if soc_shape_mean is not None else soc_mean)) + (w_prior * prior_val)\n",
    "        if soc_oob:\n",
    "            soc_std = min(max(soc_std, float(bundle[\"metrics\"].get(\"soc_rmse_selected\", 8.0))), cfg.SOC_STD_MAX_OOD)\n",
    "\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[SoC-OOD] sev={severity:.2f} w_raw={w_raw:.2f} w_shape={w_shape:.2f} w_prior={w_prior:.2f} prior={prior_val:.2f}\")\n",
    "\n",
    "    # ----- SoH -----\n",
    "    soh_model=bundle[\"soh_model\"]; model_name=bundle.get(\"soh_model_name\",\"unknown\")\n",
    "    if isinstance(soh_model, GaussianProcessRegressor):\n",
    "        sm, ss = soh_model.predict(X, return_std=True)\n",
    "        soh_mean_raw = float(sm[0]); soh_std_raw=float(ss[0])\n",
    "    else:\n",
    "        soh_mean_raw = float(soh_model.predict(X)[0])\n",
    "        soh_std_raw  = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    shape_model = bundle.get(\"shape_model\"); shape_scaler = bundle.get(\"shape_scaler\")\n",
    "    shape_soh_mean=None; shape_soh_std=None\n",
    "    if shape_model is not None and norm_vec is not None:\n",
    "        X_shape_s = shape_scaler.transform(norm_vec.reshape(1,-1))\n",
    "        if isinstance(shape_model, GaussianProcessRegressor):\n",
    "            sm2, ss2 = shape_model.predict(X_shape_s, return_std=True)\n",
    "            shape_soh_mean=float(sm2[0]); shape_soh_std=float(ss2[0])\n",
    "        else:\n",
    "            shape_soh_mean=float(shape_model.predict(X_shape_s)[0])\n",
    "            shape_soh_std=float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    if cfg.ENSEMBLE_SOH and shape_soh_mean is not None:\n",
    "        soh_mean = 0.5*(soh_mean_raw + shape_soh_mean)\n",
    "        stds = [soh_std_raw]\n",
    "        if shape_soh_std is not None: stds.append(shape_soh_std)\n",
    "        soh_std = float(np.sqrt(np.mean(np.array(stds)**2)))\n",
    "    else:\n",
    "        soh_mean, soh_std = soh_mean_raw, soh_std_raw\n",
    "\n",
    "    train_mahal = bundle.get(\"train_mahal\")\n",
    "    mahal_dist=None\n",
    "    if train_mahal:\n",
    "        cov_inv = np.array(train_mahal[\"cov_inv\"])\n",
    "        center = np.array(train_mahal[\"center\"])\n",
    "        mahal_dist = mahalanobis_distance(X[0], center, cov_inv)\n",
    "    ard_norm=None\n",
    "    if \"gpr\" in model_name:\n",
    "        ard_norm = gp_ard_norm(X, soh_model)\n",
    "    ood_flag=False\n",
    "    if (mahal_dist is not None and mahal_dist > cfg.MAHAL_THRESHOLD) or \\\n",
    "       (ard_norm is not None and ard_norm > cfg.GP_ARD_NORM_THRESHOLD):\n",
    "        ood_flag=True\n",
    "\n",
    "    soh_val_rmse = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "    if ood_flag:\n",
    "        soh_std = min(soh_std, cfg.SOH_STD_MAX_OOD)\n",
    "    else:\n",
    "        soh_std = min(soh_std, soh_val_rmse)\n",
    "\n",
    "    # ----- Cycles predictions (learned; already scaled) -----\n",
    "    cycles_models = bundle.get(\"cycles_models\", {}) or {}\n",
    "    cycles_to = {}\n",
    "    cycles_abs = None\n",
    "    if \"absolute\" in cycles_models:\n",
    "        m = cycles_models[\"absolute\"][\"model\"]\n",
    "        cyc_pred = float(max(0.0, m.predict(X)[0]))\n",
    "        cycles_abs = cyc_pred\n",
    "    for thr in cfg.TARGET_SOH_THRESHOLDS:\n",
    "        key = str(int(thr))\n",
    "        if key in cycles_models:\n",
    "            m = cycles_models[key][\"model\"]\n",
    "            rem = float(max(0.0, m.predict(X)[0]))\n",
    "            cycles_to[thr] = rem\n",
    "        else:\n",
    "            cycles_to[thr] = 0.0\n",
    "\n",
    "    # Fallback if needed\n",
    "    used_cpp = None\n",
    "    if not any(v > 0 for v in cycles_to.values()):\n",
    "        cpp = get_cpp(meta, cpp_map, global_cpp)\n",
    "        used_cpp = float(cpp)\n",
    "        for thr_val in cfg.TARGET_SOH_THRESHOLDS:\n",
    "            cycles_to[thr_val] = float((soh_mean - thr_val) * cpp) if soh_mean > thr_val else 0.0\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoC] {Path(file_path).name}: mean={soc_mean:.2f} std={soc_std:.2f}  SOC_mahal={soc_mahal} SOC_ood={bool(soc_oob)}\")\n",
    "        print(f\"[SoH] {Path(file_path).name}: mean={soh_mean:.2f} std={soh_std:.2f}  OOD(SoH)={bool(ood_flag)}\")\n",
    "        if cycles_abs is not None:\n",
    "            print(f\"[Cycles] absolute={cycles_abs:.1f}\")\n",
    "        print(f\"[Cycles] remaining: \" + \", \".join([f\"→{int(k)}%: {v:.1f}\" for k,v in cycles_to.items()]))\n",
    "\n",
    "    result={\n",
    "        \"file\": str(file_path),\n",
    "        \"feature_checksum\": checksum,\n",
    "        \"parsed_metadata\": meta,\n",
    "        # SoC\n",
    "        \"predicted_SoC_percent\": float(soc_mean),\n",
    "        \"SoC_std_estimate\": float(soc_std),\n",
    "        \"soc_model_chosen\": soc_model_name,\n",
    "        \"SOC_mahal\": soc_mahal,\n",
    "        \"SOC_ood\": bool(soc_oob),\n",
    "        # SoH\n",
    "        \"predicted_SoH_percent\": float(soh_mean),\n",
    "        \"SoH_std_estimate\": float(soh_std),\n",
    "        \"shape_model_mean\": None if (shape_model is None) else float(shape_soh_mean if shape_soh_mean is not None else np.nan),\n",
    "        \"shape_model_std\": None if (shape_model is None) else float(shape_soh_std if shape_soh_std is not None else np.nan),\n",
    "        \"soh_model_chosen\": model_name,\n",
    "        # Cycles\n",
    "        \"predicted_cycle_index\": None if cycles_abs is None else float(cycles_abs),\n",
    "        \"predicted_cycles_remaining_to_thresholds\": {str(int(k)): float(v) for k,v in cycles_to.items()},\n",
    "        \"cycles_per_percent_est\": None if not any(v>0 for v in cycles_to.values()) else float(_estimate_cpp_from_predictions(soh_mean, cycles_to)),\n",
    "        \"fallback_cpp_used\": None if used_cpp is None else float(used_cpp),\n",
    "        \"decision_threshold_percent\": cfg.DECISION_SOH_PERCENT,\n",
    "        \"lower_threshold_percent\": cfg.ILLUSTRATIVE_MIN_SOH,\n",
    "        \"OOD_flag\": bool(ood_flag)\n",
    "    }\n",
    "    return result, ood_flag, {float(k): float(v) for k,v in cycles_to.items()}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 16. MAIN (batch mode)\n",
    "# =========================\n",
    "def main():\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps(to_jsonable(asdict(cfg)), indent=2))\n",
    "\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR)\n",
    "\n",
    "    # ★ compute global cycle scale used in this run\n",
    "    global CYCLE_SCALE_GLOBAL\n",
    "    CYCLE_SCALE_GLOBAL = float(cfg.CYCLE_SCALE)\n",
    "    if not cap_df.empty and cfg.TARGET_CALIB_CYCLE_AT_80 is not None:\n",
    "        auto = _calibrate_cycle_scale(cap_df, cfg.TARGET_CALIB_CYCLE_AT_80)\n",
    "        CYCLE_SCALE_GLOBAL *= float(auto)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[CYCLE-SCALE] auto={auto:.3f}  (target 80% at {cfg.TARGET_CALIB_CYCLE_AT_80})  → total scale={CYCLE_SCALE_GLOBAL:.3f}\")\n",
    "\n",
    "    # CPP map (scaled) for fallback and plotting\n",
    "    if cap_df.empty:\n",
    "        if cfg.VERBOSE: print(\"[INFO] No / empty capacity data.\")\n",
    "        cpp_map, global_cpp = {}, float(cfg.CPP_FALLBACK)\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "    # apply scale ★\n",
    "    cpp_map = {k: v*CYCLE_SCALE_GLOBAL for k,v in cpp_map.items()}\n",
    "    global_cpp *= CYCLE_SCALE_GLOBAL\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[CPP] dynamic cells={len(cpp_map)} global_cpp_median={global_cpp:.2f} (scaled)\")\n",
    "\n",
    "    # ---- Load or retrain\n",
    "    bundle_path_new = cfg.MODEL_DIR / \"eis_soc_soh_cycles_models.joblib\"\n",
    "    bundle_path_old = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "    need_retrain = bool(cfg.FORCE_RETRAIN) or (not bundle_path_new.exists() and not bundle_path_old.exists())\n",
    "    bundle = None\n",
    "\n",
    "    if not need_retrain:\n",
    "        try:\n",
    "            bundle = load_bundle()\n",
    "            same_sig = (bundle.get(\"config_signature\") == config_signature(cfg)) and \\\n",
    "                       (bundle.get(\"feature_version\") == cfg.FEATURE_VERSION)\n",
    "            need_retrain = not same_sig\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[LOAD] Found bundle. Signature match: {same_sig}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD] Could not load existing bundle cleanly: {e}\")\n",
    "            need_retrain = True\n",
    "\n",
    "    if need_retrain:\n",
    "        if not cfg.EIS_DIR.exists():\n",
    "            raise FileNotFoundError(f\"EIS_DIR missing: {cfg.EIS_DIR}.\")\n",
    "        if cfg.REFINE_SOH_WITH_CAPACITY and not cfg.CAP_DIR.exists():\n",
    "            print(f\"[WARN] CAP_DIR missing: {cfg.CAP_DIR}. Proceeding without capacity refinement.\")\n",
    "            cfg.REFINE_SOH_WITH_CAPACITY = False\n",
    "            cap_df = pd.DataFrame()\n",
    "        if cfg.VERBOSE: print(\"[TRAIN] Building dataset & training models...\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict = build_dataset(cfg.EIS_DIR, cap_df, cycle_scale=CYCLE_SCALE_GLOBAL)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[TRAIN] Samples={X_raw.shape[0]} Features={X_raw.shape[1]} Cells={meta_df.CellID.nunique()}\")\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict)\n",
    "    else:\n",
    "        if bundle is None:\n",
    "            bundle = load_bundle()\n",
    "\n",
    "    # ---- Inference (batch)\n",
    "    artifacts = cfg.MODEL_DIR / \"artifacts\"\n",
    "    artifacts.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for test_fp in cfg.EIS_TEST_FILES:\n",
    "        print(f\"\\n===== TEST: {Path(test_fp).name} =====\")\n",
    "        if not Path(test_fp).exists():\n",
    "            print(f\"[WARN] Test file not found: {test_fp}\")\n",
    "            continue\n",
    "        try:\n",
    "            result, ood_flag, cycles_to_map = predict_file(Path(test_fp), bundle, cpp_map, global_cpp)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Prediction failed for {Path(test_fp).name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        out_plot = artifacts / f\"{Path(test_fp).stem}_projection.png\"\n",
    "        cpp_hint = result.get(\"fallback_cpp_used\", None)\n",
    "        plot_projection(\n",
    "            Path(test_fp).stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            {float(k): float(v) for k,v in result[\"predicted_cycles_remaining_to_thresholds\"].items()},\n",
    "            cpp_hint,\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot,\n",
    "            thresholds=cfg.TARGET_SOH_THRESHOLDS\n",
    "        )\n",
    "\n",
    "        out_json = artifacts / f\"{Path(test_fp).stem}_prediction.json\"\n",
    "        with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(f\"[PLOT] Saved: {out_plot}\")\n",
    "        print(f\"[JSON] Saved: {out_json}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 17. GRADIO UI (Jupyter-friendly)\n",
    "# =========================\n",
    "def _prepare_models_for_ui(force_retrain: bool=False):\n",
    "    bundle_path_new = cfg.MODEL_DIR / \"eis_soc_soh_cycles_models.joblib\"\n",
    "    bundle_path_old = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR) if cfg.REFINE_SOH_WITH_CAPACITY else pd.DataFrame()\n",
    "\n",
    "    # ★ compute scale for UI session too\n",
    "    global CYCLE_SCALE_GLOBAL\n",
    "    CYCLE_SCALE_GLOBAL = float(cfg.CYCLE_SCALE)\n",
    "    if not cap_df.empty and cfg.TARGET_CALIB_CYCLE_AT_80 is not None:\n",
    "        CYCLE_SCALE_GLOBAL *= _calibrate_cycle_scale(cap_df, cfg.TARGET_CALIB_CYCLE_AT_80)\n",
    "\n",
    "    # scaled CPP map\n",
    "    if cap_df.empty:\n",
    "        cpp_map, global_cpp = {}, cfg.CPP_FALLBACK\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "    cpp_map = {k: v*CYCLE_SCALE_GLOBAL for k,v in cpp_map.items()}\n",
    "    global_cpp *= CYCLE_SCALE_GLOBAL\n",
    "\n",
    "    need_retrain = bool(force_retrain) or bool(cfg.FORCE_RETRAIN) or (not bundle_path_new.exists() and not bundle_path_old.exists())\n",
    "    if not need_retrain and (bundle_path_new.exists() or bundle_path_old.exists()):\n",
    "        try:\n",
    "            bundle = load_bundle()\n",
    "            same_sig = (bundle.get(\"config_signature\") == config_signature(cfg)) and \\\n",
    "                       (bundle.get(\"feature_version\") == cfg.FEATURE_VERSION)\n",
    "            need_retrain = not same_sig\n",
    "        except Exception:\n",
    "            need_retrain = True\n",
    "\n",
    "    if need_retrain:\n",
    "        if not cfg.EIS_DIR.exists():\n",
    "            raise FileNotFoundError(f\"EIS_DIR missing: {cfg.EIS_DIR}. Update cfg.EIS_DIR before training.\")\n",
    "        if cfg.REFINE_SOH_WITH_CAPACITY and not cfg.CAP_DIR.exists():\n",
    "            raise FileNotFoundError(f\"CAP_DIR missing: {cfg.CAP_DIR}. Update cfg.CAP_DIR or disable REFINE_SOH_WITH_CAPACITY.\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict = build_dataset(cfg.EIS_DIR, cap_df, cycle_scale=CYCLE_SCALE_GLOBAL)\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict)\n",
    "    else:\n",
    "        bundle = load_bundle()\n",
    "\n",
    "    return bundle, cpp_map, global_cpp\n",
    "\n",
    "def _ui_predict(file_obj, override_temp, force_retrain):\n",
    "    try:\n",
    "        orig_temp_override = cfg.TEST_TEMPERATURE_OVERRIDE\n",
    "        if override_temp is None or str(override_temp).strip() == \"\":\n",
    "            cfg.TEST_TEMPERATURE_OVERRIDE = orig_temp_override\n",
    "        else:\n",
    "            try:\n",
    "                cfg.TEST_TEMPERATURE_OVERRIDE = float(override_temp)\n",
    "            except Exception:\n",
    "                cfg.TEST_TEMPERATURE_OVERRIDE = orig_temp_override\n",
    "\n",
    "        bundle, cpp_map, global_cpp = _prepare_models_for_ui(force_retrain=bool(force_retrain))\n",
    "\n",
    "        test_fp: Optional[Path] = None\n",
    "        if file_obj is None:\n",
    "            raise ValueError(\"Please upload a file.\")\n",
    "        if isinstance(file_obj, (str, Path)):\n",
    "            test_fp = Path(file_obj)\n",
    "        elif isinstance(file_obj, dict) and \"name\" in file_obj:\n",
    "            test_fp = Path(file_obj[\"name\"])\n",
    "        elif hasattr(file_obj, \"name\"):\n",
    "            name = Path(getattr(file_obj, \"name\", \"upload\")).name\n",
    "            suffix = Path(name).suffix or \"\"\n",
    "            tmp_name = cfg.MODEL_DIR / f\"ui_{uuid.uuid4().hex}{suffix}\"\n",
    "            try:\n",
    "                file_obj.seek(0)\n",
    "            except Exception:\n",
    "                pass\n",
    "            data = file_obj.read()\n",
    "            if isinstance(data, str):\n",
    "                data = data.encode(\"utf-8\")\n",
    "            with open(tmp_name, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            test_fp = tmp_name\n",
    "        else:\n",
    "            tmp_name = cfg.MODEL_DIR / f\"ui_{uuid.uuid4().hex}\"\n",
    "            data = file_obj.read()\n",
    "            if isinstance(data, str):\n",
    "                data = data.encode(\"utf-8\")\n",
    "            with open(tmp_name, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            test_fp = tmp_name\n",
    "\n",
    "        result, ood_flag, cycles_to_map = predict_file(test_fp, bundle, cpp_map, global_cpp)\n",
    "\n",
    "        out_plot = cfg.MODEL_DIR / f\"{Path(test_fp).stem}_projection_ui.png\"\n",
    "        plot_projection(\n",
    "            Path(test_fp).stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            {float(k): float(v) for k,v in result[\"predicted_cycles_remaining_to_thresholds\"].items()},\n",
    "            result.get(\"fallback_cpp_used\", None),\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot,\n",
    "            thresholds=cfg.TARGET_SOH_THRESHOLDS\n",
    "        )\n",
    "        with open(out_plot, \"rb\") as f:\n",
    "            img_bytes = f.read()\n",
    "        plot_img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        pretty = {\n",
    "            \"file\": result[\"file\"],\n",
    "            \"feature_checksum\": result.get(\"feature_checksum\"),\n",
    "            \"SoC_percent\": round(result[\"predicted_SoC_percent\"], 2),\n",
    "            \"SoC_std\": round(result[\"SoC_std_estimate\"], 2),\n",
    "            \"SoH_percent\": round(result[\"predicted_SoH_percent\"], 2),\n",
    "            \"SoH_std\": round(result[\"SoH_std_estimate\"], 2),\n",
    "            \"cycles_absolute\": result.get(\"predicted_cycle_index\"),\n",
    "            \"cycles_remaining_to_thresholds\": {k: round(v, 1) for k, v in result[\"predicted_cycles_remaining_to_thresholds\"].items()},\n",
    "            \"cpp_estimated_from_predictions\": result.get(\"cycles_per_percent_est\"),\n",
    "            \"OOD\": bool(result[\"OOD_flag\"]),\n",
    "            \"soc_model\": result.get(\"soc_model_chosen\"),\n",
    "            \"soh_model\": result.get(\"soh_model_chosen\"),\n",
    "            \"decision_threshold_percent\": result[\"decision_threshold_percent\"],\n",
    "            \"lower_threshold_percent\": result[\"lower_threshold_percent\"]\n",
    "        }\n",
    "        pretty_json = json.dumps(pretty, indent=2)\n",
    "\n",
    "        soc_value = float(result[\"predicted_SoC_percent\"])\n",
    "\n",
    "        cfg.TEST_TEMPERATURE_OVERRIDE = orig_temp_override\n",
    "        return plot_img, pretty_json, soc_value\n",
    "    except Exception as e:\n",
    "        err = {\"error\": str(e)}\n",
    "        return None, json.dumps(err, indent=2), None\n",
    "\n",
    "def launch_gradio(server_name: str = \"127.0.0.1\",\n",
    "                  server_port: int = 7860,\n",
    "                  share: bool = False,\n",
    "                  inbrowser: bool = False):\n",
    "    try:\n",
    "        import gradio as gr\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Gradio is not installed. Please: pip install gradio\") from e\n",
    "\n",
    "    if _running_in_notebook():\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] nest_asyncio not available.\", e)\n",
    "\n",
    "    with gr.Blocks(title=\"EIS SoC/SoH + Learned Cycles (v9.1)\") as demo:\n",
    "        gr.Markdown(\"## Unified EIS: SoC / SoH inference & cycles learned from training\\nUpload a single EIS file (.csv / .xlsx / .xls / .mat). Remaining cycles are extrapolated reliably even below 80% and calibrated to your cycle units.\")\n",
    "        with gr.Row():\n",
    "            file_in = gr.File(label=\"Upload EIS file\", file_count=\"single\")\n",
    "        with gr.Row():\n",
    "            override_temp = gr.Textbox(label=\"Test temperature override (°C, optional)\", placeholder=str(cfg.TEST_TEMPERATURE_OVERRIDE))\n",
    "            force_retrain = gr.Checkbox(label=\"Force retrain before inference\", value=False)\n",
    "        predict_btn = gr.Button(\"Predict\")\n",
    "        with gr.Row():\n",
    "            img_out = gr.Image(label=\"RUL Projection Plot\", type=\"pil\")\n",
    "            json_out = gr.Code(label=\"Results (JSON)\")\n",
    "        soc_out = gr.Number(label=\"Predicted SoC (%)\", precision=2)\n",
    "\n",
    "        with gr.Row():\n",
    "            rt_btn = gr.Button(\"Retrain bundle only\")\n",
    "            rt_status = gr.Markdown()\n",
    "\n",
    "        def _do_predict(file_obj, temp, fr):\n",
    "            return _ui_predict(file_obj, temp, fr)\n",
    "\n",
    "        def _do_retrain():\n",
    "            try:\n",
    "                _prepare_models_for_ui(force_retrain=True)\n",
    "                return \"✅ Retrained successfully.\"\n",
    "            except Exception as e:\n",
    "                return f\"❌ Retrain failed: {e}\"\n",
    "\n",
    "        predict_btn.click(_do_predict, inputs=[file_in, override_temp, force_retrain], outputs=[img_out, json_out, soc_out])\n",
    "        rt_btn.click(_do_retrain, inputs=None, outputs=rt_status)\n",
    "\n",
    "    launch_kwargs = dict(server_name=server_name, server_port=server_port, share=share)\n",
    "    if _running_in_notebook():\n",
    "        launch_kwargs.update(dict(inline=True, inbrowser=False, prevent_thread_lock=True, debug=False))\n",
    "        demo.queue(concurrency_count=2, max_size=10)\n",
    "        for attempt in range(6):\n",
    "            try:\n",
    "                return demo.launch(**launch_kwargs)\n",
    "            except OSError as e:\n",
    "                if \"Address already in use\" in str(e).lower():\n",
    "                    launch_kwargs[\"server_port\"] = int(launch_kwargs.get(\"server_port\", 7860)) + 1\n",
    "                    continue\n",
    "                raise\n",
    "            except TypeError:\n",
    "                for k in (\"inline\", \"prevent_thread_lock\"):\n",
    "                    launch_kwargs.pop(k, None)\n",
    "                return demo.launch(**launch_kwargs)\n",
    "    else:\n",
    "        try:\n",
    "            return demo.launch(server_name=server_name, server_port=server_port, share=share, inbrowser=inbrowser)\n",
    "        except OSError as e:\n",
    "            if \"Address already in use\" in str(e):\n",
    "                return demo.launch(server_name=server_name, server_port=server_port+1, share=share, inbrowser=inbrowser)\n",
    "            raise\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 18. ENTRYPOINT\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ui\", action=\"store_true\", help=\"Launch the Gradio UI\")\n",
    "    parser.add_argument(\"--share\", action=\"store_true\", help=\"Create a public share link\")\n",
    "    parser.add_argument(\"--host\", default=\"127.0.0.1\", help=\"Server host (default: 127.0.0.1)\")\n",
    "    parser.add_argument(\"--port\", type=int, default=7860, help=\"Server port (default: 7860)\")\n",
    "    parser.add_argument(\"--inbrowser\", action=\"store_true\", help=\"Open UI in browser automatically\")\n",
    "    # NEW: paths + toggles\n",
    "    parser.add_argument(\"--eis_dir\", type=str, default=None, help=\"Path to training EIS .mat directory\")\n",
    "    parser.add_argument(\"--cap_dir\", type=str, default=None, help=\"Path to capacity .mat directory\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=None, help=\"Path to save/load models and artifacts\")\n",
    "    parser.add_argument(\"--no-capacity\", action=\"store_true\", help=\"Disable capacity refinement\")\n",
    "    parser.add_argument(\"--force-retrain\", action=\"store_true\", help=\"Force retrain even if a bundle exists/matches\")\n",
    "    # NEW: cycle scale override\n",
    "    parser.add_argument(\"--cycle-scale\", type=float, default=None, help=\"Override CYCLE_SCALE (global multiplier)\")\n",
    "    parser.add_argument(\"--target-80\", type=float, default=None, help=\"Override TARGET_CALIB_CYCLE_AT_80 (auto-scale)\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.eis_dir or args.cap_dir or args.model_dir:\n",
    "        set_paths(\n",
    "            args.eis_dir if args.eis_dir else cfg.EIS_DIR,\n",
    "            args.cap_dir if args.cap_dir else cfg.CAP_DIR,\n",
    "            args.model_dir if args.model_dir else cfg.MODEL_DIR,\n",
    "        )\n",
    "    if args.no_capacity:\n",
    "        cfg.REFINE_SOH_WITH_CAPACITY = False\n",
    "    if args.force_retrain:\n",
    "        cfg.FORCE_RETRAIN = True\n",
    "    if args.cycle_scale is not None:\n",
    "        cfg.CYCLE_SCALE = float(args.cycle_scale)\n",
    "    if args.target_80 is not None:\n",
    "        cfg.TARGET_CALIB_CYCLE_AT_80 = float(args.target_80)\n",
    "\n",
    "    if args.ui:\n",
    "        launch_gradio(server_name=args.host, server_port=args.port, share=args.share, inbrowser=args.inbrowser)\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3756c-4751-4fe1-8fd1-faec459ceb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e7bbf-b67e-4ad0-8858-683c3b1245c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f7d87-a3a1-424b-964d-20c043558edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
