{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d908f64b-08fe-48ab-8d9a-019776173106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tmgon\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tmgon\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"EIS_TEST_FILES\": [\n",
      "    \"Mazda-Battery-Cell1.xlsx\",\n",
      "    \"Mazda-Battery-Cell2.xlsx\"\n",
      "  ],\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"USE_PCA_SOC\": true,\n",
      "  \"USE_PCA_SOH\": false,\n",
      "  \"PCA_SOC_COMPONENTS\": 25,\n",
      "  \"PCA_SOH_COMPONENTS\": 30,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"INCLUDE_SHAPE_NORMALIZED_BRANCH\": true,\n",
      "  \"NORMALIZE_SHAPE_BY_HF_RE\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"ENSEMBLE_SOH\": true,\n",
      "  \"ENSEMBLE_STD_MODE\": \"rms\",\n",
      "  \"DECISION_SOH_PERCENT\": 50.0,\n",
      "  \"ILLUSTRATIVE_MIN_SOH\": 40.0,\n",
      "  \"CPP_ROLLING_WINDOW\": 5,\n",
      "  \"CPP_MIN_POINTS\": 6,\n",
      "  \"CPP_FALLBACK\": 20.0,\n",
      "  \"TEST_TEMPERATURE_OVERRIDE\": 25.0,\n",
      "  \"FORCE_RETRAIN\": true,\n",
      "  \"MAHAL_THRESHOLD\": 10.0,\n",
      "  \"GP_ARD_NORM_THRESHOLD\": 6.0,\n",
      "  \"MIN_UNIQUE_SOH\": 4,\n",
      "  \"MIN_REQUIRED_SOH_STD\": 0.75,\n",
      "  \"PLOT_EXPONENT\": 1.25,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FEATURE_VERSION\": 8\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "EIS_DIR missing: C:\\Users\\tmgon\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 942\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 942\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 874\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mVERBOSE:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps({k: to_jsonable(v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m asdict(cfg)\u001b[38;5;241m.\u001b[39mitems()}, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mEIS_DIR\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEIS_DIR missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mEIS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    875\u001b[0m cap_df \u001b[38;5;241m=\u001b[39m load_capacity_info(cfg\u001b[38;5;241m.\u001b[39mCAP_DIR)\n\u001b[0;32m    877\u001b[0m bundle_path \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mMODEL_DIR\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meis_soc_soh_phys_models.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: EIS_DIR missing: C:\\Users\\tmgon\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Unified EIS → SoC (classification) & SoH (regression) Model (Realistic v8a)\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "Changes vs v8:\n",
    "  * Temperature feature renamed to Temp_feat (avoid duplicate with metadata Temp).\n",
    "  * Defensive duplicate renaming before parquet save.\n",
    "\n",
    "See previous description for full feature list & rationale.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, random, joblib, warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================================\n",
    "# 1. CONFIGURATION\n",
    "# ====================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "\n",
    "    EIS_TEST_FILES: List[Path] = None\n",
    "\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int = 60\n",
    "\n",
    "    TEST_FRAC: float = 0.2\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    USE_PCA_SOC: bool = True\n",
    "    USE_PCA_SOH: bool = False\n",
    "    PCA_SOC_COMPONENTS: int = 25\n",
    "    PCA_SOH_COMPONENTS: int = 30\n",
    "\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    INCLUDE_SHAPE_NORMALIZED_BRANCH: bool = True\n",
    "    NORMALIZE_SHAPE_BY_HF_RE: bool = True\n",
    "\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    ENSEMBLE_SOH: bool = True\n",
    "    ENSEMBLE_STD_MODE: str = \"rms\"\n",
    "\n",
    "    DECISION_SOH_PERCENT: float = 50.0\n",
    "    ILLUSTRATIVE_MIN_SOH: float = 40.0\n",
    "    CPP_ROLLING_WINDOW: int = 5\n",
    "    CPP_MIN_POINTS: int = 6\n",
    "    CPP_FALLBACK: float = 20.0\n",
    "\n",
    "    TEST_TEMPERATURE_OVERRIDE: Optional[float] = 25.0\n",
    "    FORCE_RETRAIN: bool = True\n",
    "\n",
    "    MAHAL_THRESHOLD: float = 10.0\n",
    "    GP_ARD_NORM_THRESHOLD: float = 6.0\n",
    "\n",
    "    MIN_UNIQUE_SOH: int = 4\n",
    "    MIN_REQUIRED_SOH_STD: float = 0.75\n",
    "\n",
    "    PLOT_EXPONENT: float = 1.25\n",
    "\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    VERBOSE: bool = True\n",
    "    FEATURE_VERSION: int = 8\n",
    "\n",
    "cfg = Config()\n",
    "if cfg.EIS_TEST_FILES is None:\n",
    "    cfg.EIS_TEST_FILES = [\n",
    "        Path(\"Mazda-Battery-Cell1.xlsx\"),\n",
    "        Path(\"Mazda-Battery-Cell2.xlsx\")\n",
    "    ]\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====================================\n",
    "# 2. UTILITIES\n",
    "# ====================================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path): return str(x)\n",
    "    if isinstance(x, dict): return {k: to_jsonable(v) for k,v in x.items()}\n",
    "    if isinstance(x, (list,tuple)): return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "# ====================================\n",
    "# 3. REGEX\n",
    "# ====================================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "def parse_eis_metadata(stem: str):\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"SOC\": int(d[\"SOC\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"RealSOH_file\": int(d[\"RealSOH\"]) / 100.0\n",
    "    }\n",
    "\n",
    "def parse_cap_metadata(stem: str):\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"CycleIndex\": int(d[\"Cycle\"])\n",
    "    }\n",
    "\n",
    "# ====================================\n",
    "# 4. LOADING / INTERPOLATION\n",
    "# ====================================\n",
    "def _find_matrix(mat_dict: dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v, np.ndarray) and v.ndim==2 and v.shape[1]>=3 and v.shape[0]>=10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw, y_raw, freq_target):\n",
    "    freq_raw=np.asarray(freq_raw).astype(float)\n",
    "    y_raw=np.asarray(y_raw).astype(float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw=freq_raw[::-1]; y_raw=y_raw[::-1]\n",
    "    uniq, idx = np.unique(freq_raw, return_index=True)\n",
    "    if len(uniq)!=len(freq_raw):\n",
    "        order=np.argsort(idx)\n",
    "        freq_raw=uniq[order]; y_raw=y_raw[idx][order]\n",
    "    f = interp1d(freq_raw, y_raw, bounds_error=False,\n",
    "                 fill_value=(y_raw[0], y_raw[-1]), kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "FREQ_CANDS=[\"frequency\",\"freq\",\"f\",\"hz\",\"frequency(hz)\",\"Frequency(Hz)\"]\n",
    "RE_CANDS=[\"zreal\",\"re(z)\",\"re\",\"real\",\"z_re\",\"zreal(ohm)\",\"re (ohm)\",\"re(z) (ohm)\",\"Zreal\",\"Zreal (ohm)\",\"Zreal(ohm)\"]\n",
    "IM_CANDS=[\"-zimag\",\"zimag\",\"im(z)\",\"im\",\"imag\",\"imaginary\",\"z_im\",\"zimg\",\"z_imag\",\n",
    "          \" -Zimag (ohm)\",\" -Zimag(ohm)\",\"-Zimag\",\"Zimag\",\"Zimag (ohm)\"]\n",
    "\n",
    "def _select_column(df: pd.DataFrame, cands: List[str]):\n",
    "    lower={c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c.lower() in lower: return lower[c.lower()]\n",
    "    for c in cands:\n",
    "        for col in df.columns:\n",
    "            if c.lower() in col.lower(): return col\n",
    "    return None\n",
    "\n",
    "def load_mat_eis(path: Path):\n",
    "    mat=loadmat(path); arr=_find_matrix(mat)\n",
    "    if arr is None: raise ValueError(f\"No valid matrix in {path.name}\")\n",
    "    return arr[:,0].astype(float), arr[:,1].astype(float), arr[:,2].astype(float)\n",
    "\n",
    "def load_table_eis(path: Path):\n",
    "    if path.suffix.lower()==\".csv\":\n",
    "        df=pd.read_csv(path)\n",
    "    else:\n",
    "        df=pd.read_excel(path)\n",
    "    if df.empty: raise ValueError(\"Empty EIS table.\")\n",
    "    fcol=_select_column(df,FREQ_CANDS)\n",
    "    recol=_select_column(df,RE_CANDS)\n",
    "    imcol=_select_column(df,IM_CANDS)\n",
    "    if recol is None or imcol is None:\n",
    "        raise ValueError(f\"Missing Re/Im columns in {path.name}\")\n",
    "    re_vals = pd.to_numeric(df[recol], errors=\"coerce\").to_numpy()\n",
    "    im_vals = pd.to_numeric(df[imcol], errors=\"coerce\").to_numpy()\n",
    "    if fcol:\n",
    "        freq_vals = pd.to_numeric(df[fcol], errors=\"coerce\").to_numpy()\n",
    "    else:\n",
    "        n = min(len(re_vals), len(im_vals))\n",
    "        freq_vals = np.geomspace(cfg.F_MAX, cfg.F_MIN, n)\n",
    "    n=min(len(freq_vals), len(re_vals), len(im_vals))\n",
    "    freq_vals=freq_vals[:n]; re_vals=re_vals[:n]; im_vals=im_vals[:n]\n",
    "    if np.nanmean(im_vals) > 0: im_vals = -im_vals\n",
    "    return freq_vals, re_vals, im_vals\n",
    "\n",
    "def load_any_inference(path: Path):\n",
    "    suf=path.suffix.lower()\n",
    "    if suf==\".mat\": return load_mat_eis(path)\n",
    "    if suf in (\".csv\",\".xls\",\".xlsx\"): return load_table_eis(path)\n",
    "    raise ValueError(f\"Unsupported extension {suf}\")\n",
    "\n",
    "# ====================================\n",
    "# 5. FEATURES\n",
    "# ====================================\n",
    "def compute_F_features(freq, re_i, im_i):\n",
    "    neg_im=-im_i\n",
    "    idx_peak=int(np.argmax(neg_im))\n",
    "    F1=re_i[0]; F2=re_i[idx_peak]; F3=re_i[-1]\n",
    "    sc=np.where(np.sign(im_i[:-1])!=np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k=sc[0]; y0,y1=im_i[k],im_i[k+1]\n",
    "        w=-y0/(y1-y0+1e-12)\n",
    "        F4=re_i[k]+w*(re_i[k+1]-re_i[k])\n",
    "    else:\n",
    "        F4=np.nan\n",
    "    F5=(re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6=np.min(im_i)\n",
    "    mid_target=10.0\n",
    "    idx_mid=int(np.argmin(np.abs(freq-mid_target)))\n",
    "    F7=re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES=[\n",
    "    \"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "    \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"\n",
    "]\n",
    "\n",
    "def physical_features(freq, re_i, im_i):\n",
    "    freq=np.asarray(freq); re_i=np.asarray(re_i); im_i=np.asarray(im_i)\n",
    "    neg_im=-im_i\n",
    "    idx_peak=int(np.argmax(neg_im))\n",
    "    Rs=float(re_i[0]); Rpeak=float(re_i[idx_peak]); Rlow=float(re_i[-1])\n",
    "    Rct=max(Rpeak-Rs,0.0)\n",
    "    arc_diam=Rlow-Rs\n",
    "    norm_arc=arc_diam/(Rs+1e-9)\n",
    "    f_peak=float(freq[idx_peak])\n",
    "    tau_peak=1/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K=min(10,len(freq)//3)\n",
    "    if K>=4:\n",
    "        w_section=(2*np.pi*freq[-K:])**(-0.5)\n",
    "        re_section=re_i[-K:]\n",
    "        warburg_sigma=float(np.polyfit(w_section,re_section,1)[0]) if len(np.unique(w_section))>2 else np.nan\n",
    "    else:\n",
    "        warburg_sigma=np.nan\n",
    "    phase=np.arctan2(-im_i,re_i)\n",
    "    mid_mask=(freq>=1)&(freq<=100)\n",
    "    if mid_mask.sum()>2:\n",
    "        phase_mean_mid=float(phase[mid_mask].mean())\n",
    "        phase_std_mid=float(phase[mid_mask].std())\n",
    "    else:\n",
    "        phase_mean_mid=np.nan; phase_std_mid=np.nan\n",
    "    phase_min=float(phase.min())\n",
    "    lf_mask=(freq<=1.0)\n",
    "    if lf_mask.sum()>=4:\n",
    "        x=np.log10(freq[lf_mask]+1e-12); y=neg_im[lf_mask]\n",
    "        lf_slope=np.polyfit(x,y,1)[0]\n",
    "    else:\n",
    "        lf_slope=np.nan\n",
    "    arc_quality=(neg_im.max()-neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS=[(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq,re_i,im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m], im_i[m])\n",
    "            feats += [z.mean(), z.std()]\n",
    "        else:\n",
    "            feats += [np.nan, np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq,re_i,im_i,segments=5):\n",
    "    logf=np.log10(freq)\n",
    "    edges=np.linspace(logf.min(),logf.max(),segments+1)\n",
    "    out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]\n",
    "            out += [np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else:\n",
    "            out += [np.nan, np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES=[\n",
    "    \"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "    \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"\n",
    "]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w=2*np.pi*freq\n",
    "    tau=np.geomspace(tau_max,tau_min,n_tau)\n",
    "    WT=w[:,None]*tau[None,:]\n",
    "    denom=1+WT**2\n",
    "    K_re=1.0/denom\n",
    "    K_im=-WT/denom\n",
    "    R_inf=re_i[0]\n",
    "    y_re=re_i-R_inf\n",
    "    y_im=im_i\n",
    "    Y=np.concatenate([y_re,y_im])\n",
    "    K=np.vstack([K_re,K_im])\n",
    "    A=K.T@K + lam*np.eye(n_tau)\n",
    "    b=K.T@Y\n",
    "    gamma=linalg.solve(A,b,assume_a='pos')\n",
    "    gamma=np.clip(gamma,0,None)\n",
    "    return tau,gamma\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    try:\n",
    "        tau,gamma=compute_drt(freq,re_i,im_i,\n",
    "                              cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,\n",
    "                              cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau=np.log10(tau)\n",
    "        g_sum=gamma.sum()+1e-12\n",
    "        w_norm=gamma/g_sum\n",
    "        mean_logtau=float((w_norm*log_tau).sum())\n",
    "        var_logtau=float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p=int(np.argmax(gamma))\n",
    "        peak_tau=float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid=np.median(log_tau)\n",
    "        frac_low=float(w_norm[log_tau<=mid].sum())\n",
    "        frac_high=1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception:\n",
    "        return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i, im_i, temp, freq, include_names=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts += [re_i, im_i]\n",
    "        names += [f\"Re_{i}\" for i in range(len(re_i))] + [f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z=np.hypot(re_i,im_i)\n",
    "        basics=[re_i[0], re_i[-1], re_i[-1]-re_i[0], z.max(), z.mean(), z.std()]\n",
    "        parts.append(np.array(basics)); names += [\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        Ff=compute_F_features(freq,re_i,im_i); parts.append(np.array(Ff)); names += [f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        Pf=physical_features(freq,re_i,im_i); parts.append(np.array(Pf)); names += PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        Bf=band_stats(freq,re_i,im_i); parts.append(np.array(Bf))\n",
    "        for bi in range(len(BANDS)): names += [f\"band{bi}_mean\", f\"band{bi}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i); parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2): names += [f\"slope_re_seg{i}\", f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        Df=drt_features(freq,re_i,im_i); parts.append(np.array(Df)); names += DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names += [\"Temp_feat\"]  # <-- renamed\n",
    "    vec=np.concatenate(parts).astype(float)\n",
    "    vec=np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if include_names: return vec, names\n",
    "    return vec\n",
    "\n",
    "def shape_normalize(re_i, im_i):\n",
    "    hf = re_i[0] if re_i[0] != 0 else 1.0\n",
    "    return re_i / hf, im_i / hf, hf\n",
    "\n",
    "# ====================================\n",
    "# 6. CAPACITY / CPP\n",
    "# ====================================\n",
    "def load_capacity_info(cap_dir: Path)->pd.DataFrame:\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY):\n",
    "        return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta=parse_cap_metadata(fp.stem)\n",
    "        if not meta: continue\n",
    "        try:\n",
    "            mat=loadmat(fp); arr=_find_matrix(mat)\n",
    "            if arr is None: continue\n",
    "            col=np.argmax(np.abs(arr[-50:, :]).mean(axis=0))\n",
    "            cap=float(np.nanmax(arr[:,col]))\n",
    "            meta[\"MeasuredCapacity_Ah\"]=cap\n",
    "            recs.append(meta)\n",
    "        except Exception:\n",
    "            pass\n",
    "    df=pd.DataFrame(recs)\n",
    "    if df.empty: return df\n",
    "    ref=df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"]=df[\"MeasuredCapacity_Ah\"]/ref\n",
    "    df[\"SoH_percent\"]=df[\"NormCapacity\"]*100.0\n",
    "    return df\n",
    "\n",
    "def estimate_cpp(cap_df: pd.DataFrame, window:int, min_points:int):\n",
    "    cpp={}\n",
    "    for cid,grp in cap_df.groupby(\"CellID\"):\n",
    "        g=grp.sort_values(\"CycleIndex\")\n",
    "        if g.shape[0]<min_points: continue\n",
    "        tail=g.tail(window)\n",
    "        x=tail[\"CycleIndex\"].values.astype(float)\n",
    "        y=tail[\"SoH_percent\"].values.astype(float)\n",
    "        if len(np.unique(x))<2: continue\n",
    "        slope=np.polyfit(x,y,1)[0]\n",
    "        if slope >= -1e-6: continue\n",
    "        cpp[cid]=1.0/abs(slope)\n",
    "    return cpp\n",
    "\n",
    "def build_cpp_map(cap_df: pd.DataFrame):\n",
    "    if cap_df.empty: return {}, cfg.CPP_FALLBACK\n",
    "    cpp_map=estimate_cpp(cap_df, cfg.CPP_ROLLING_WINDOW, cfg.CPP_MIN_POINTS)\n",
    "    if not cpp_map: return {}, cfg.CPP_FALLBACK\n",
    "    return cpp_map, float(np.median(list(cpp_map.values())))\n",
    "\n",
    "def get_cpp(meta: dict, cpp_map: Dict[str,float], global_cpp: float):\n",
    "    if not meta: return global_cpp\n",
    "    return cpp_map.get(meta.get(\"CellID\"), global_cpp)\n",
    "\n",
    "# ====================================\n",
    "# 7. DATASET BUILD\n",
    "# ====================================\n",
    "def load_single_mat(fp: Path):\n",
    "    meta=parse_eis_metadata(fp.stem)\n",
    "    if meta is None: raise ValueError(f\"Pattern mismatch: {fp.name}\")\n",
    "    freq,re_raw,im_raw=load_mat_eis(fp)\n",
    "    re_i=_interp_channel(freq,re_raw,CANON_FREQ)\n",
    "    im_i=_interp_channel(freq,im_raw,CANON_FREQ)\n",
    "    return meta, re_i, im_i\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_df: Optional[pd.DataFrame]):\n",
    "    files=sorted(eis_dir.rglob(\"*.mat\"))\n",
    "    if not files: raise FileNotFoundError(f\"No .mat spectra in {eis_dir}\")\n",
    "\n",
    "    # For feature names\n",
    "    first=None\n",
    "    for f in files:\n",
    "        try:\n",
    "            m,re_i,im_i=load_single_mat(f)\n",
    "            first=(m,re_i,im_i); break\n",
    "        except Exception: continue\n",
    "    if first is None: raise RuntimeError(\"No parsable .mat files.\")\n",
    "    _, r0, i0 = first\n",
    "    _, feature_names = build_feature_vector(r0, i0, 25.0, CANON_FREQ, include_names=True)\n",
    "\n",
    "    rows=[]; raw_feats=[]; shape_feats=[]\n",
    "    for fp in tqdm(files, desc=\"Loading training spectra\"):\n",
    "        try:\n",
    "            meta, re_i, im_i = load_single_mat(fp)\n",
    "            raw_vec = build_feature_vector(re_i, im_i, meta[\"Temp\"], CANON_FREQ)\n",
    "            raw_feats.append(raw_vec); rows.append(meta)\n",
    "            if cfg.INCLUDE_SHAPE_NORMALIZED_BRANCH and cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "                rsh, ish, _ = shape_normalize(re_i, im_i)\n",
    "                shape_vec = build_feature_vector(rsh, ish, meta[\"Temp\"], CANON_FREQ)\n",
    "                shape_feats.append(shape_vec)\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip] {fp.name}: {e}\")\n",
    "\n",
    "    if not rows: raise RuntimeError(\"No usable spectra.\")\n",
    "    X_raw=np.vstack(raw_feats)\n",
    "    X_shape=np.vstack(shape_feats) if (cfg.INCLUDE_SHAPE_NORMALIZED_BRANCH and shape_feats) else None\n",
    "    meta_df=pd.DataFrame(rows)\n",
    "\n",
    "    # SoH refinement\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup=cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        refined=[]\n",
    "        for cid, stage, fallback in zip(meta_df.CellID, meta_df.SOH_stage, meta_df.RealSOH_file):\n",
    "            val=lookup.get((cid,stage))\n",
    "            refined.append(100.0*val if val is not None else fallback)\n",
    "        meta_df[\"SoH_cont\"]=refined\n",
    "    else:\n",
    "        meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "\n",
    "    y_soc=meta_df[\"SOC\"].values\n",
    "    y_soh=meta_df[\"SoH_cont\"].values\n",
    "\n",
    "    unique_soh=np.unique(y_soh)\n",
    "    soh_std=float(np.std(y_soh))\n",
    "    soh_range=(float(np.min(y_soh)), float(np.max(y_soh)))\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[LABELS] SoH unique count={len(unique_soh)} range={soh_range} std={soh_std:.3f}\")\n",
    "        print(f\"[LABELS] Unique SoH values (truncated): {unique_soh[:20]}{'...' if len(unique_soh)>20 else ''}\")\n",
    "    if len(unique_soh) < cfg.MIN_UNIQUE_SOH or soh_std < cfg.MIN_REQUIRED_SOH_STD:\n",
    "        raise RuntimeError(\n",
    "            f\"Insufficient SoH diversity (unique={len(unique_soh)}, std={soh_std:.3f}). \"\n",
    "            f\"Add more varied degradation stages.\"\n",
    "        )\n",
    "\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        feat_df = pd.DataFrame(X_raw, columns=feature_names)\n",
    "        # Defensive duplicate renaming\n",
    "        dup = set(meta_df.columns).intersection(feat_df.columns)\n",
    "        if dup:\n",
    "            rename_map = {c: f\"{c}_feat\" for c in dup}\n",
    "            feat_df = feat_df.rename(columns=rename_map)\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[INFO] Renamed duplicate feature columns: {rename_map}\")\n",
    "        pd.concat([meta_df.reset_index(drop=True), feat_df], axis=1)\\\n",
    "          .to_parquet(cfg.MODEL_DIR/\"training_features.parquet\", index=False)\n",
    "\n",
    "    return meta_df, X_raw, X_shape, y_soc, y_soh, feature_names\n",
    "\n",
    "# ====================================\n",
    "# 8. TRAINING\n",
    "# ====================================\n",
    "def split_mask(meta_df: pd.DataFrame):\n",
    "    cells=meta_df.CellID.unique()\n",
    "    rng=np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test=max(1,int(len(cells)*cfg.TEST_FRAC))\n",
    "    test_cells=rng.choice(cells,size=n_test,replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "def train_models(meta_df, X_raw, X_shape, y_soc, y_soh, feature_names):\n",
    "    mask_test=split_mask(meta_df)\n",
    "\n",
    "    # SoC\n",
    "    soc_scaler=StandardScaler()\n",
    "    X_soc_s=soc_scaler.fit_transform(X_raw)\n",
    "    soc_pca=None\n",
    "    X_soc_model=X_soc_s\n",
    "    if cfg.USE_PCA_SOC:\n",
    "        soc_pca=PCA(n_components=min(cfg.PCA_SOC_COMPONENTS,X_soc_s.shape[1]-1),\n",
    "                    random_state=cfg.RANDOM_STATE)\n",
    "        X_soc_model=soc_pca.fit_transform(X_soc_s)\n",
    "    soc_model=RandomForestClassifier(\n",
    "        n_estimators=600, min_samples_leaf=2, class_weight='balanced',\n",
    "        n_jobs=-1, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    soc_model.fit(X_soc_model[~mask_test], y_soc[~mask_test])\n",
    "    soc_pred=soc_model.predict(X_soc_model[mask_test])\n",
    "    soc_acc=accuracy_score(y_soc[mask_test], soc_pred)\n",
    "    soc_f1=f1_score(y_soc[mask_test], soc_pred, average='macro')\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoC] Acc={soc_acc:.3f} MacroF1={soc_f1:.3f}\")\n",
    "        print(classification_report(y_soc[mask_test], soc_pred, digits=4))\n",
    "\n",
    "    # SoH raw\n",
    "    soh_scaler=StandardScaler()\n",
    "    X_soh_s=soh_scaler.fit_transform(X_raw)\n",
    "    soh_pca=None\n",
    "    X_soh_in=X_soh_s\n",
    "    if cfg.USE_PCA_SOH:\n",
    "        soh_pca=PCA(n_components=min(cfg.PCA_SOH_COMPONENTS,X_soh_s.shape[1]-1),\n",
    "                    random_state=cfg.RANDOM_STATE)\n",
    "        X_soh_in=soh_pca.fit_transform(X_soh_s)\n",
    "\n",
    "    dim_raw=X_soh_in.shape[1]\n",
    "    kernel_raw=RBF(length_scale=np.ones(dim_raw)*3.0,\n",
    "                   length_scale_bounds=(1e-1,1e4)) + \\\n",
    "               WhiteKernel(noise_level=1e-2,\n",
    "                           noise_level_bounds=(1e-6,1e-1))\n",
    "    gpr_raw=GaussianProcessRegressor(\n",
    "        kernel=kernel_raw, alpha=0.0, normalize_y=True,\n",
    "        n_restarts_optimizer=3, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    if X_soh_in.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "        idx=np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "            X_soh_in.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "        gpr_raw.fit(X_soh_in[idx], y_soh[idx])\n",
    "    else:\n",
    "        gpr_raw.fit(X_soh_in, y_soh)\n",
    "    pred_raw=gpr_raw.predict(X_soh_in[mask_test])\n",
    "    r2_raw=r2_score(y_soh[mask_test], pred_raw)\n",
    "    rmse_raw=math.sqrt(mean_squared_error(y_soh[mask_test], pred_raw))\n",
    "\n",
    "    hgb=HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_iter=500,\n",
    "        l2_regularization=1e-3, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    hgb.fit(X_soh_in[~mask_test], y_soh[~mask_test])\n",
    "    pred_hgb=hgb.predict(X_soh_in[mask_test])\n",
    "    r2_hgb=r2_score(y_soh[mask_test], pred_hgb)\n",
    "    rmse_hgb=math.sqrt(mean_squared_error(y_soh[mask_test], pred_hgb))\n",
    "\n",
    "    # Shape branch\n",
    "    shape_bundle=None\n",
    "    if cfg.INCLUDE_SHAPE_NORMALIZED_BRANCH and X_shape is not None:\n",
    "        shape_scaler=StandardScaler()\n",
    "        X_sh_s=shape_scaler.fit_transform(X_shape)\n",
    "        shape_pca=None\n",
    "        X_sh_in=X_sh_s\n",
    "        if cfg.USE_PCA_SOH:\n",
    "            shape_pca=PCA(n_components=min(cfg.PCA_SOH_COMPONENTS,X_sh_s.shape[1]-1),\n",
    "                          random_state=cfg.RANDOM_STATE)\n",
    "            X_sh_in=shape_pca.fit_transform(X_sh_s)\n",
    "        dim_sh=X_sh_in.shape[1]\n",
    "        kernel_sh=RBF(length_scale=np.ones(dim_sh)*3.0,\n",
    "                      length_scale_bounds=(1e-1,1e4)) + \\\n",
    "                  WhiteKernel(noise_level=1e-2,\n",
    "                              noise_level_bounds=(1e-6,1e-1))\n",
    "        gpr_shape=GaussianProcessRegressor(\n",
    "            kernel=kernel_sh, normalize_y=True, alpha=0.0,\n",
    "            n_restarts_optimizer=3, random_state=cfg.RANDOM_STATE\n",
    "        )\n",
    "        if X_sh_in.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "            idxs=np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "                X_sh_in.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "            gpr_shape.fit(X_sh_in[idxs], y_soh[idxs])\n",
    "        else:\n",
    "            gpr_shape.fit(X_sh_in, y_soh)\n",
    "        pred_sh=gpr_shape.predict(X_sh_in[mask_test])\n",
    "        r2_sh=r2_score(y_soh[mask_test], pred_sh)\n",
    "        rmse_sh=math.sqrt(mean_squared_error(y_soh[mask_test], pred_sh))\n",
    "        shape_bundle={\n",
    "            \"shape_scaler\": shape_scaler,\n",
    "            \"shape_pca\": shape_pca,\n",
    "            \"shape_model\": gpr_shape,\n",
    "            \"shape_metrics\": {\"r2\": r2_sh, \"rmse\": rmse_sh}\n",
    "        }\n",
    "    else:\n",
    "        r2_sh=rmse_sh=None\n",
    "\n",
    "    # Select primary\n",
    "    if r2_raw >= r2_hgb:\n",
    "        primary_model=gpr_raw; primary_name=\"gpr_raw\"; primary_r2=r2_raw; primary_rmse=rmse_raw\n",
    "    else:\n",
    "        primary_model=hgb; primary_name=\"hgb_raw\"; primary_r2=r2_hgb; primary_rmse=rmse_hgb\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoH] GPR_raw R2={r2_raw:.3f} RMSE={rmse_raw:.2f}\")\n",
    "        print(f\"[SoH] HGB     R2={r2_hgb:.3f} RMSE={rmse_hgb:.2f}\")\n",
    "        if shape_bundle:\n",
    "            print(f\"[SoH] ShapeGP R2={shape_bundle['shape_metrics']['r2']:.3f} \"\n",
    "                  f\"RMSE={shape_bundle['shape_metrics']['rmse']:.2f}\")\n",
    "        print(f\"[SoH] Selected raw = {primary_name}\")\n",
    "\n",
    "    cov=np.cov(X_soh_s.T)\n",
    "    try:\n",
    "        cov_inv=np.linalg.pinv(cov)\n",
    "    except Exception:\n",
    "        cov_inv=np.eye(cov.shape[0])\n",
    "    center=X_soh_s.mean(axis=0)\n",
    "\n",
    "    bundle={\n",
    "        \"soc_scaler\": soc_scaler,\n",
    "        \"soc_pca\": soc_pca,\n",
    "        \"soc_model\": soc_model,\n",
    "        \"soh_scaler\": soh_scaler,\n",
    "        \"soh_pca\": soh_pca,\n",
    "        \"soh_model\": primary_model,\n",
    "        \"soh_model_name\": primary_name,\n",
    "        \"shape_scaler\": shape_bundle[\"shape_scaler\"] if shape_bundle else None,\n",
    "        \"shape_pca\": shape_bundle[\"shape_pca\"] if shape_bundle else None,\n",
    "        \"shape_model\": shape_bundle[\"shape_model\"] if shape_bundle else None,\n",
    "        \"shape_metrics\": shape_bundle[\"shape_metrics\"] if shape_bundle else None,\n",
    "        \"freq_grid\": CANON_FREQ,\n",
    "        \"feature_version\": cfg.FEATURE_VERSION,\n",
    "        \"feature_manifest\": feature_names,\n",
    "        \"config\": to_jsonable(asdict(cfg)),\n",
    "        \"metrics\": {\n",
    "            \"soc_accuracy\": soc_acc,\n",
    "            \"soc_macro_f1\": soc_f1,\n",
    "            \"soh_primary_r2\": primary_r2,\n",
    "            \"soh_primary_rmse\": primary_rmse,\n",
    "            \"soh_gpr_r2\": r2_raw,\n",
    "            \"soh_hgb_r2\": r2_hgb,\n",
    "            \"soh_shape_r2\": r2_sh\n",
    "        },\n",
    "        \"train_mahal\": {\"center\": center.tolist(), \"cov_inv\": cov_inv.tolist()}\n",
    "    }\n",
    "    out=cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    joblib.dump(bundle,out)\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[MODEL] Saved → {out}\")\n",
    "        print(json.dumps(bundle[\"metrics\"], indent=2))\n",
    "    return bundle\n",
    "\n",
    "# ====================================\n",
    "# 9. LOAD\n",
    "# ====================================\n",
    "def load_bundle():\n",
    "    path=cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    b=joblib.load(path)\n",
    "    required={\"soc_scaler\",\"soc_model\",\"soh_scaler\",\"soh_model\",\"freq_grid\"}\n",
    "    if not required.issubset(b.keys()):\n",
    "        raise RuntimeError(\"Bundle schema mismatch; retrain required.\")\n",
    "    if b.get(\"feature_version\", -1) < cfg.FEATURE_VERSION:\n",
    "        warnings.warn(\"Older feature version detected; consider retraining.\")\n",
    "    return b\n",
    "\n",
    "# ====================================\n",
    "# 10. INFERENCE\n",
    "# ====================================\n",
    "def mahalanobis_distance(x, center, cov_inv):\n",
    "    diff=x-center\n",
    "    return float(np.sqrt(diff @ cov_inv @ diff.T))\n",
    "\n",
    "def gp_ard_norm(Xp, model):\n",
    "    try:\n",
    "        K=model.kernel_\n",
    "        from sklearn.gaussian_process.kernels import RBF\n",
    "        rbf=None\n",
    "        if hasattr(K,\"k1\") and isinstance(K.k1,RBF): rbf=K.k1\n",
    "        elif hasattr(K,\"k2\") and isinstance(K.k2,RBF): rbf=K.k2\n",
    "        if rbf is None: return None\n",
    "        ls=np.atleast_1d(rbf.length_scale)\n",
    "        return float(np.linalg.norm((Xp/ls).ravel()))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def featurize_for_inference(path: Path, bundle):\n",
    "    freq_grid=bundle[\"freq_grid\"]\n",
    "    meta=parse_eis_metadata(path.stem)\n",
    "    freq,re_raw,im_raw=load_any_inference(path)\n",
    "    re_i=_interp_channel(freq,re_raw,freq_grid)\n",
    "    im_i=_interp_channel(freq,im_raw,freq_grid)\n",
    "    temp = meta[\"Temp\"] if meta else (cfg.TEST_TEMPERATURE_OVERRIDE if cfg.TEST_TEMPERATURE_OVERRIDE is not None else -1)\n",
    "    raw_vec=build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "    shape_vec=None\n",
    "    if cfg.INCLUDE_SHAPE_NORMALIZED_BRANCH and bundle.get(\"shape_model\") is not None:\n",
    "        rsh, ish, _ = shape_normalize(re_i, im_i)\n",
    "        shape_vec=build_feature_vector(rsh, ish, temp, freq_grid)\n",
    "    return raw_vec, shape_vec, meta\n",
    "\n",
    "def ensemble_soh(raw_mean, raw_std, shape_mean, shape_std):\n",
    "    if shape_mean is None: return raw_mean, raw_std\n",
    "    if raw_mean is None: return shape_mean, shape_std\n",
    "    m=0.5*(raw_mean+shape_mean)\n",
    "    if cfg.ENSEMBLE_STD_MODE.lower()==\"rms\":\n",
    "        s=math.sqrt(np.mean([raw_std**2, shape_std**2]))\n",
    "    else:\n",
    "        s=0.5*(raw_std+shape_std)\n",
    "    return m,s\n",
    "\n",
    "def predict_file(path: Path, bundle, cpp_map, global_cpp):\n",
    "    raw_vec, shape_vec, meta=featurize_for_inference(path, bundle)\n",
    "\n",
    "    # SoC\n",
    "    soc_scaler=bundle[\"soc_scaler\"]; soc_pca=bundle.get(\"soc_pca\")\n",
    "    soc_model=bundle[\"soc_model\"]\n",
    "    X_soc=soc_scaler.transform(raw_vec.reshape(1,-1))\n",
    "    X_soc_in=soc_pca.transform(X_soc) if soc_pca else X_soc\n",
    "    soc_probs=soc_model.predict_proba(X_soc_in)[0]\n",
    "    soc_classes=soc_model.classes_\n",
    "    soc_pred=int(soc_classes[np.argmax(soc_probs)])\n",
    "\n",
    "    # SoH raw\n",
    "    soh_scaler=bundle[\"soh_scaler\"]; soh_pca=bundle.get(\"soh_pca\")\n",
    "    soh_model=bundle[\"soh_model\"]; model_name=bundle.get(\"soh_model_name\")\n",
    "    X_soh_s=soh_scaler.transform(raw_vec.reshape(1,-1))\n",
    "    X_soh_in=soh_pca.transform(X_soh_s) if soh_pca else X_soh_s\n",
    "    if isinstance(soh_model, GaussianProcessRegressor):\n",
    "        sm, ss=soh_model.predict(X_soh_in, return_std=True)\n",
    "        raw_mean=float(sm[0]); raw_std=float(ss[0])\n",
    "    else:\n",
    "        raw_mean=float(soh_model.predict(X_soh_in)[0])\n",
    "        raw_std=float(b[\"metrics\"].get(\"soh_primary_rmse\",5.0))\n",
    "\n",
    "    # Shape branch\n",
    "    shape_mean=None; shape_std=None\n",
    "    shape_model=bundle.get(\"shape_model\")\n",
    "    if shape_model is not None and shape_vec is not None:\n",
    "        sscaler=bundle.get(\"shape_scaler\"); spca=bundle.get(\"shape_pca\")\n",
    "        X_shape_s=sscaler.transform(shape_vec.reshape(1,-1))\n",
    "        X_shape_in=spca.transform(X_shape_s) if spca else X_shape_s\n",
    "        if isinstance(shape_model, GaussianProcessRegressor):\n",
    "            sm2, ss2=shape_model.predict(X_shape_in, return_std=True)\n",
    "            shape_mean=float(sm2[0]); shape_std=float(ss2[0])\n",
    "        else:\n",
    "            shape_mean=float(shape_model.predict(X_shape_in)[0])\n",
    "            shape_std=float(bundle[\"metrics\"].get(\"soh_primary_rmse\",5.0))\n",
    "\n",
    "    soh_mean, soh_std = ensemble_soh(raw_mean, raw_std, shape_mean, shape_std)\n",
    "\n",
    "    cpp = get_cpp(meta, cpp_map, global_cpp)\n",
    "    cycles_to_target = (soh_mean - cfg.DECISION_SOH_PERCENT)*cpp if soh_mean > cfg.DECISION_SOH_PERCENT else 0.0\n",
    "    cycles_to_lower = (soh_mean - cfg.ILLUSTRATIVE_MIN_SOH)*cpp if soh_mean > cfg.ILLUSTRATIVE_MIN_SOH else 0.0\n",
    "\n",
    "    train_mahal=bundle.get(\"train_mahal\")\n",
    "    mahal=None\n",
    "    if train_mahal:\n",
    "        center=np.array(train_mahal[\"center\"])\n",
    "        cov_inv=np.array(train_mahal[\"cov_inv\"])\n",
    "        mahal=mahalanobis_distance(X_soh_s[0], center, cov_inv)\n",
    "    ard_norm=None\n",
    "    if isinstance(soh_model, GaussianProcessRegressor):\n",
    "        ard_norm=gp_ard_norm(X_soh_in, soh_model)\n",
    "    ood_flag=False\n",
    "    if (mahal is not None and mahal>cfg.MAHAL_THRESHOLD) or \\\n",
    "       (ard_norm is not None and ard_norm>cfg.GP_ARD_NORM_THRESHOLD):\n",
    "        ood_flag=True\n",
    "\n",
    "    return {\n",
    "        \"file\": str(path),\n",
    "        \"parsed_metadata\": meta,\n",
    "        \"predicted_SoC\": soc_pred,\n",
    "        \"SoC_probabilities\": {int(c): float(p) for c,p in zip(soc_classes, soc_probs)},\n",
    "        \"predicted_SoH_percent\": soh_mean,\n",
    "        \"SoH_std_estimate\": soh_std,\n",
    "        \"raw_model_mean\": raw_mean,\n",
    "        \"raw_model_std\": raw_std,\n",
    "        \"shape_model_mean\": shape_mean,\n",
    "        \"shape_model_std\": shape_std,\n",
    "        \"cycles_per_percent_used\": cpp,\n",
    "        \"cycles_to_target\": cycles_to_target,\n",
    "        \"cycles_to_lower\": cycles_to_lower,\n",
    "        \"decision_threshold_percent\": cfg.DECISION_SOH_PERCENT,\n",
    "        \"lower_threshold_percent\": cfg.ILLUSTRATIVE_MIN_SOH,\n",
    "        \"feature_version\": bundle.get(\"feature_version\"),\n",
    "        \"soh_model_chosen\": model_name,\n",
    "        \"OOD_mahal\": mahal,\n",
    "        \"OOD_gp_ard_norm\": ard_norm,\n",
    "        \"OOD_flag\": ood_flag\n",
    "    }\n",
    "\n",
    "# ====================================\n",
    "# 11. PROJECTION PLOT\n",
    "# ====================================\n",
    "def build_projection(soh_current, cpp, lower, exponent=None, n=160):\n",
    "    if soh_current <= lower or cpp <= 0: return np.array([0.0]), np.array([soh_current])\n",
    "    total=(soh_current-lower)*cpp\n",
    "    cycles=np.linspace(0,total,n)\n",
    "    exp=exponent if exponent is not None else cfg.PLOT_EXPONENT\n",
    "    curve=lower + (soh_current-lower)*(1 - cycles/total)**exp\n",
    "    return cycles, curve\n",
    "\n",
    "def plot_projection(base, soh_current, soh_std, cyc_target, cyc_lower, cpp, ood_flag, out_path):\n",
    "    if cyc_lower <= 0: return\n",
    "    cycles, curve=build_projection(soh_current, cpp, cfg.ILLUSTRATIVE_MIN_SOH)\n",
    "    plt.figure(figsize=(6.4,4))\n",
    "    plt.plot(cycles, curve, lw=2, label=\"Projected SoH\")\n",
    "    plt.axhline(cfg.DECISION_SOH_PERCENT, color=\"orange\", ls=\"--\", label=\"Decision\")\n",
    "    plt.axhline(cfg.ILLUSTRATIVE_MIN_SOH, color=\"red\", ls=\":\", label=\"Lower\")\n",
    "    plt.scatter([0],[soh_current], c=\"green\", s=50)\n",
    "    plt.text(0, soh_current+0.6, f\"{soh_current:.2f}±{soh_std:.2f}\", color=\"green\", fontsize=8)\n",
    "    if cyc_target>0:\n",
    "        plt.axvline(cyc_target, color=\"orange\", ls=\"-.\")\n",
    "        plt.text(cyc_target, cfg.DECISION_SOH_PERCENT+1, f\"{cyc_target:.0f} cyc\",\n",
    "                 ha=\"center\", color=\"orange\", fontsize=8)\n",
    "    plt.scatter([cycles[-1]],[cfg.ILLUSTRATIVE_MIN_SOH], c=\"red\", s=45)\n",
    "    plt.text(cycles[-1], cfg.ILLUSTRATIVE_MIN_SOH-2, f\"{cycles[-1]:.0f} cyc\",\n",
    "             ha=\"center\", color=\"red\", fontsize=8)\n",
    "    if ood_flag:\n",
    "        plt.text(0.98,0.05,\"OOD\", transform=plt.gca().transAxes,\n",
    "                 ha=\"right\", va=\"bottom\", color=\"crimson\",\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"crimson\"))\n",
    "    plt.xlabel(\"Remaining Cycles\")\n",
    "    plt.ylabel(\"SoH (%)\")\n",
    "    plt.title(f\"RUL Projection – {base}\")\n",
    "    plt.grid(alpha=0.35)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "# ====================================\n",
    "# 12. MAIN\n",
    "# ====================================\n",
    "def main():\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps({k: to_jsonable(v) for k,v in asdict(cfg).items()}, indent=2))\n",
    "\n",
    "    assert cfg.EIS_DIR.exists(), f\"EIS_DIR missing: {cfg.EIS_DIR}\"\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR)\n",
    "\n",
    "    bundle_path = cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    need_retrain = cfg.FORCE_RETRAIN or (not bundle_path.exists())\n",
    "    bundle=None\n",
    "    if not need_retrain:\n",
    "        try:\n",
    "            bundle=load_bundle()\n",
    "            if cfg.VERBOSE: print(\"[LOAD] Existing bundle loaded (set FORCE_RETRAIN=True to rebuild).\")\n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD] Failed to load bundle ({e}); retraining.\")\n",
    "            need_retrain=True\n",
    "\n",
    "    if need_retrain:\n",
    "        if cfg.VERBOSE: print(\"[TRAIN] Building dataset & training...\")\n",
    "        try:\n",
    "            meta_df, X_raw, X_shape, y_soc, y_soh, feature_names = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[FATAL] {e}\")\n",
    "            return\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[TRAIN] Samples={X_raw.shape[0]} Features={X_raw.shape[1]} Cells={meta_df.CellID.nunique()}\")\n",
    "        bundle=train_models(meta_df, X_raw, X_shape, y_soc, y_soh, feature_names)\n",
    "\n",
    "    # CPP map\n",
    "    if cap_df.empty:\n",
    "        if cfg.VERBOSE: print(\"[CPP] No capacity data; using fallback.\")\n",
    "        cpp_map, global_cpp = {}, cfg.CPP_FALLBACK\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[CPP] dynamic cells={len(cpp_map)} global median cpp={global_cpp:.2f}\")\n",
    "\n",
    "    for tf in cfg.EIS_TEST_FILES:\n",
    "        print(f\"\\n===== TEST: {tf.name} =====\")\n",
    "        if not tf.exists():\n",
    "            print(f\"[WARN] Missing file: {tf}\")\n",
    "            continue\n",
    "        try:\n",
    "            result = predict_file(tf, bundle, cpp_map, global_cpp)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Prediction failed: {e}\")\n",
    "            continue\n",
    "        out_json = cfg.MODEL_DIR / f\"{tf.stem}_prediction.json\"\n",
    "        with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        out_plot = cfg.MODEL_DIR / f\"{tf.stem}_projection.png\"\n",
    "        plot_projection(\n",
    "            tf.stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            result[\"cycles_to_target\"],\n",
    "            result[\"cycles_to_lower\"],\n",
    "            result[\"cycles_per_percent_used\"],\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot\n",
    "        )\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(f\"[JSON] Saved: {out_json}\")\n",
    "        if result[\"cycles_to_lower\"] > 0:\n",
    "            print(f\"[PLOT] Saved: {out_plot}\")\n",
    "        else:\n",
    "            print(\"[PLOT] Skipped (SoH below lower threshold).\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61ee44-167f-4de9-8c40-655fddb59335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
