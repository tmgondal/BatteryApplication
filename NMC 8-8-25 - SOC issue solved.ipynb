{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc41e786-4627-47a5-8190-c3df3aa949ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Cell02_95SOH_15degC_05SOC_9505.mat: none of ['freq', 'frequency', 'f'] found\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 336\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    335\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 336\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[4], line 328\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    325\u001b[0m args, _ \u001b[38;5;241m=\u001b[39m ap\u001b[38;5;241m.\u001b[39mparse_known_args()          \u001b[38;5;66;03m# <= replaces:  args = ap.parse_args()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mretrain \u001b[38;5;129;01mor\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mFORCE_RETRAIN \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (cfg\u001b[38;5;241m.\u001b[39mMODEL_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoc.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 328\u001b[0m     train_and_save()\n\u001b[0;32m    329\u001b[0m infer(Path(args\u001b[38;5;241m.\u001b[39mtest))\n",
      "Cell \u001b[1;32mIn[4], line 294\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[1;34m()\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_and_save\u001b[39m():\n\u001b[1;32m--> 294\u001b[0m     df\u001b[38;5;241m=\u001b[39mbuild_dataset(cfg\u001b[38;5;241m.\u001b[39mEIS_DIR,cfg\u001b[38;5;241m.\u001b[39mCAP_DIR,cfg\u001b[38;5;241m.\u001b[39mINCLUDE_DRT)\n\u001b[0;32m    295\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcycles_remaining\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mfinal_cycle\u001b[38;5;241m-\u001b[39mdf\u001b[38;5;241m.\u001b[39mcycle_idx\n\u001b[0;32m    296\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoc_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39msoc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 189\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[1;34m(eis_dir, cap_dir, include_drt)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fp\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xls\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    188\u001b[0m         meta\u001b[38;5;241m=\u001b[39mparse_eis_metadata(fp)\n\u001b[1;32m--> 189\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m(features\u001b[38;5;241m=\u001b[39mfeaturize_any(fp,include_drt),\n\u001b[0;32m    190\u001b[0m                          cell_id\u001b[38;5;241m=\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    191\u001b[0m                          cycle_idx\u001b[38;5;241m=\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcycle_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    192\u001b[0m                          soc\u001b[38;5;241m=\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    193\u001b[0m                          file_id\u001b[38;5;241m=\u001b[39mfp\u001b[38;5;241m.\u001b[39mstem))\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rows: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo EIS files found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "Cell \u001b[1;32mIn[4], line 176\u001b[0m, in \u001b[0;36mfeaturize_any\u001b[1;34m(fp, include_drt)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeaturize_any\u001b[39m(fp: Path, include_drt: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 176\u001b[0m     f,zr,zi \u001b[38;5;241m=\u001b[39m _read_eis(fp)\n\u001b[0;32m    177\u001b[0m     mag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(zr\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39mzi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    178\u001b[0m     pha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdegrees(np\u001b[38;5;241m.\u001b[39marctan2(zi,zr))\n",
      "Cell \u001b[1;32mIn[4], line 156\u001b[0m, in \u001b[0;36m_read_eis\u001b[1;34m(fp)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(mat[lk[c\u001b[38;5;241m.\u001b[39mlower()]])\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: none of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcands\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m freq \u001b[38;5;241m=\u001b[39m grab([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    157\u001b[0m zre  \u001b[38;5;241m=\u001b[39m grab([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzreal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_real\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    158\u001b[0m zim  \u001b[38;5;241m=\u001b[39m grab([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzimag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_imag\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 155\u001b[0m, in \u001b[0;36m_read_eis.<locals>.grab\u001b[1;34m(cands)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m lk:\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(mat[lk[c\u001b[38;5;241m.\u001b[39mlower()]])\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: none of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcands\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Cell02_95SOH_15degC_05SOC_9505.mat: none of ['freq', 'frequency', 'f'] found\""
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ── std / third-party imports ─────────────────────────────────────────\n",
    "import argparse, json, math, os, pickle, random, warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# ── 0 · GLOBALS  ──────────────────────────────────────────────────────\n",
    "CANON_FREQ = np.logspace(-2, 4, 50)  # 0.01 Hz → 10 kHz\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 17) -> None:\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def to_jsonable(o: Any) -> Any:\n",
    "    if isinstance(o, (np.integer, np.floating)):\n",
    "        return o.item()\n",
    "    if isinstance(o, (np.ndarray, list, tuple, set)):\n",
    "        return [to_jsonable(x) for x in o]\n",
    "    if isinstance(o, dict):\n",
    "        return {k: to_jsonable(v) for k, v in o.items()}\n",
    "    if isinstance(o, Path):\n",
    "        return str(o)\n",
    "    return o\n",
    "\n",
    "\n",
    "# ── 1 · PATH CONFIG  ─────────────────────────────────────────────────\n",
    "@dataclass\n",
    "class Config:\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "    EIS_TEST_FILE: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\TestFile\\Mazda-Battery-Cell5.xlsx\")\n",
    "\n",
    "    FORCE_RETRAIN: bool = False\n",
    "    INCLUDE_DRT: bool = True\n",
    "    USE_PCA_SOC_REG: bool = True\n",
    "    PCA_SOC_REG_COMPONENTS: int = 25\n",
    "    MAX_GP_TRAIN_SAMPLES_SOC: int = 8_000\n",
    "    MAX_GP_TRAIN_SAMPLES_RUL: int = 8_000\n",
    "    SOC_STD_CLAMP: float = 3.0\n",
    "    RANDOM_STATE: int = 17\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "# ── 2 · HELPERS – parsing & feature engineering ──────────────────────\n",
    "import re\n",
    "_EIS_RE = re.compile(r\"(?P<cell>[A-Za-z0-9\\-]+)[_\\-]?(?:Cycl[e]?|CYL)?(?P<cycle>\\d+)[_\\-]?(?:SoC)?(?P<soc>\\d+)?\", re.I)\n",
    "_CAP_RE = re.compile(r\"(?P<cell>[A-Za-z0-9\\-]+)[_\\-]?(?:Cap|CYC|Cycle)?(?P<cycle>\\d+)\", re.I)\n",
    "\n",
    "\n",
    "def parse_eis_metadata(fp: Path) -> Dict[str, Any]:\n",
    "    m = _EIS_RE.search(fp.stem)\n",
    "    return dict(cell_id=fp.stem, cycle_idx=np.nan, soc=np.nan) if not m else \\\n",
    "        dict(cell_id=m[\"cell\"], cycle_idx=int(m[\"cycle\"]), soc=float(m[\"soc\"]) if m[\"soc\"] else np.nan)\n",
    "\n",
    "\n",
    "def parse_cap_metadata(fp: Path) -> Dict[str, Any]:\n",
    "    m = _CAP_RE.search(fp.stem)\n",
    "    return dict(cell_id=fp.stem, cycle_idx=np.nan) if not m else \\\n",
    "        dict(cell_id=m[\"cell\"], cycle_idx=int(m[\"cycle\"]))\n",
    "\n",
    "\n",
    "def _load_one_capacity(fp: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp) if fp.suffix.lower() == \".csv\" else pd.read_excel(fp, engine=\"openpyxl\" if fp.suffix.lower()==\".xlsx\" else None)\n",
    "    df = df.rename(columns=lambda s: s.strip().lower())\n",
    "    if \"capacity\" not in df.columns:\n",
    "        for alt in (\"capacity_ah\", \"cap_ah\", \"capacity (ah)\"):\n",
    "            if alt in df.columns:\n",
    "                df[\"capacity\"] = df[alt]; break\n",
    "    if \"cycle\" not in df.columns:\n",
    "        raise ValueError(f\"{fp.name} missing 'cycle' column\")\n",
    "    meta = parse_cap_metadata(fp)\n",
    "    df = df[[\"cycle\", \"capacity\"]].copy()\n",
    "    df.insert(0, \"cell_id\", meta[\"cell_id\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "    for fp in cap_dir.rglob(\"*\"):\n",
    "        if fp.suffix.lower() in {\".csv\", \".xls\", \".xlsx\"}:\n",
    "            try:\n",
    "                dfs.append(_load_one_capacity(fp))\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Skip capacity file {fp.name}: {e}\")\n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(f\"No capacity files in {cap_dir}\")\n",
    "    cap = pd.concat(dfs, ignore_index=True)\n",
    "    cap[\"init_cap\"] = cap.groupby(\"cell_id\")[\"capacity\"].transform(\"first\")\n",
    "    cap[\"soh_percent\"] = 100 * cap[\"capacity\"] / cap[\"init_cap\"]\n",
    "    return cap\n",
    "\n",
    "\n",
    "def build_cpp_map(cap_dir: Path) -> Dict[float, float]:\n",
    "    cap = load_capacity_info(cap_dir)\n",
    "    cap[\"cycles_remaining\"] = cap.groupby(\"cell_id\")[\"cycle\"].transform(\"max\") - cap[\"cycle\"]\n",
    "    med = cap.groupby(cap[\"soh_percent\"].round())[\"cycles_remaining\"].median()\n",
    "    return med.dropna().to_dict()\n",
    "\n",
    "\n",
    "def get_cpp(cpp: float, cpp_map: Dict[float, float]) -> float:\n",
    "    if not cpp_map or math.isnan(cpp):\n",
    "        return np.nan\n",
    "    keys = np.array(list(cpp_map))\n",
    "    return float(cpp_map[keys[np.argmin(np.abs(keys - cpp))]])\n",
    "\n",
    "\n",
    "# ─── drop-in replacement for `_read_eis` ──────────────────────────\n",
    "def _read_eis(fp: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return (frequency [Hz], Zreal [Ω], Zimag [Ω]) from .csv/.xls/.mat files.\n",
    "\n",
    "    • For .mat we accept several common key variants:\n",
    "        - freq  | frequency  | f\n",
    "        - Zreal | Zre        | Z_real\n",
    "        - Zimag | Zim        | Z_imag\n",
    "      Keys are matched case-insensitively.\n",
    "    \"\"\"\n",
    "    suf = fp.suffix.lower()\n",
    "\n",
    "    # ---------- text / Excel ----------\n",
    "    if suf in {\".csv\", \".txt\"}:\n",
    "        df = pd.read_csv(fp)\n",
    "    elif suf in {\".xls\", \".xlsx\"}:\n",
    "        df = pd.read_excel(fp, engine=\"openpyxl\" if suf == \".xlsx\" else None)\n",
    "    # ---------- MATLAB ----------\n",
    "    elif suf == \".mat\":\n",
    "        mat = loadmat(fp)\n",
    "        # normalise keys to lowercase for easier lookup\n",
    "        lk = {k.lower(): k for k in mat.keys()}\n",
    "        def grab(cands: List[str]) -> np.ndarray:\n",
    "            for c in cands:\n",
    "                if c.lower() in lk:\n",
    "                    return np.squeeze(mat[lk[c.lower()]])\n",
    "            raise KeyError(f\"{fp.name}: none of {cands} found\")\n",
    "        freq = grab([\"freq\", \"frequency\", \"f\"])\n",
    "        zre  = grab([\"zreal\", \"zre\", \"z_real\"])\n",
    "        zim  = grab([\"zimag\", \"zim\", \"z_imag\"])\n",
    "        return freq, zre, zim\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported EIS file type: {fp.suffix}\")\n",
    "\n",
    "    # ---------- tidy dataframe ----------\n",
    "    df = df.rename(columns=lambda s: s.strip().lower())\n",
    "    freq = df.iloc[:, 0].to_numpy(float)\n",
    "    zre  = df.iloc[:, 1].to_numpy(float)\n",
    "    zim  = df.iloc[:, 2].to_numpy(float)\n",
    "    return freq, zre, zim\n",
    "\n",
    "\n",
    "\n",
    "def _interp(freq, arr): return 10**interp1d(np.log10(freq), np.log10(np.abs(arr)), fill_value=\"extrapolate\")(np.log10(CANON_FREQ))\n",
    "\n",
    "\n",
    "def featurize_any(fp: Path, include_drt: bool=True) -> np.ndarray:\n",
    "    f,zr,zi = _read_eis(fp)\n",
    "    mag = np.sqrt(zr**2+zi**2)\n",
    "    pha = np.degrees(np.arctan2(zi,zr))\n",
    "    feats = [_interp(f,mag), _interp(f,pha)]\n",
    "    if include_drt: feats.append(np.zeros_like(CANON_FREQ))  # placeholder DRT\n",
    "    return np.concatenate(feats).astype(np.float32)\n",
    "\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_dir: Path, include_drt=True) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for fp in eis_dir.rglob(\"*\"):\n",
    "        if fp.suffix.lower() in {\".csv\",\".txt\",\".xls\",\".xlsx\",\".mat\"}:\n",
    "            meta=parse_eis_metadata(fp)\n",
    "            rows.append(dict(features=featurize_any(fp,include_drt),\n",
    "                             cell_id=meta[\"cell_id\"],\n",
    "                             cycle_idx=meta[\"cycle_idx\"],\n",
    "                             soc=meta[\"soc\"]/100 if not math.isnan(meta[\"soc\"]) else np.nan,\n",
    "                             file_id=fp.stem))\n",
    "    if not rows: raise FileNotFoundError(\"No EIS files found\")\n",
    "    df=pd.DataFrame(rows)\n",
    "    try:\n",
    "        fin=load_capacity_info(cap_dir).groupby(\"cell_id\")[\"cycle\"].max().rename(\"final_cycle\").reset_index()\n",
    "        df=df.merge(fin,on=\"cell_id\",how=\"left\")\n",
    "    except FileNotFoundError: df[\"final_cycle\"]=np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_vector(test_fp: Path, include_drt=True) -> Dict[str,Any]:\n",
    "    feats=featurize_any(test_fp,include_drt)\n",
    "    meta=parse_eis_metadata(test_fp)\n",
    "    cpp=100.0\n",
    "    try:\n",
    "        caps=load_capacity_info(test_fp.parents[3])\n",
    "        row=caps[(caps.cell_id==meta[\"cell_id\"])&(caps.cycle==meta[\"cycle_idx\"])]\n",
    "        if not row.empty: cpp=float(row.soh_percent.iloc[0])\n",
    "    except Exception: pass\n",
    "    return {\"features\":feats,\"cpp\":cpp}\n",
    "\n",
    "\n",
    "def _cell_soh_curve(cell_id:str,cap_dir:Path)->Tuple[np.ndarray,np.ndarray]:\n",
    "    try:\n",
    "        df=load_capacity_info(cap_dir); sub=df[df.cell_id==cell_id]\n",
    "        if sub.empty: raise ValueError\n",
    "        return sub.cycle.to_numpy(), sub.soh_percent.to_numpy()\n",
    "    except Exception:\n",
    "        cy=np.linspace(0,1200,240); return cy, 80+20*np.exp(-cy/400)\n",
    "\n",
    "\n",
    "def plot_projection(test_fp:Path)->Tuple[np.ndarray,np.ndarray]:\n",
    "    meta=parse_eis_metadata(test_fp)\n",
    "    return _cell_soh_curve(meta[\"cell_id\"], test_fp.parents[2])\n",
    "\n",
    "# ── 3 · MODEL TRAINING  ────────────────────────────────────────────\n",
    "def _subsample(X,y,max_n):\n",
    "    if len(X)<=max_n: return X,y\n",
    "    idx=np.random.default_rng(cfg.RANDOM_STATE).choice(len(X),max_n,replace=False)\n",
    "    return X[idx],y[idx]\n",
    "\n",
    "def _maybe_pca(X,fit,pca=None):\n",
    "    if not cfg.USE_PCA_SOC_REG: return X,None if fit else pca\n",
    "    if fit:\n",
    "        pca=PCA(cfg.PCA_SOC_REG_COMPONENTS,random_state=cfg.RANDOM_STATE); return pca.fit_transform(X),pca\n",
    "    return pca.transform(X),pca\n",
    "\n",
    "def _cv_r2(est,X,y,groups):\n",
    "    gkf=GroupKFold(n_splits=min(5,len(np.unique(groups))))\n",
    "    sc=[]\n",
    "    for tr,te in gkf.split(X,y,groups):\n",
    "        e=pickle.loads(pickle.dumps(est)); e.fit(X[tr],y[tr]); sc.append(r2_score(y[te],e.predict(X[te])))\n",
    "    return float(np.mean(sc))\n",
    "\n",
    "def train_soc(df):\n",
    "    X=np.vstack(df.features.values); y=df.soc_percent.values\n",
    "    groups=df.cell_id.values\n",
    "    X,y=_subsample(X,y,cfg.MAX_GP_TRAIN_SAMPLES_SOC)\n",
    "    Xp,pca=_maybe_pca(X,True)\n",
    "    try:\n",
    "        gp=GaussianProcessRegressor(RBF(np.ones(Xp.shape[1]))+WhiteKernel(),alpha=1e-6,normalize_y=True,random_state=cfg.RANDOM_STATE)\n",
    "        gp.fit(Xp,y); model=(\"gp\",gp)\n",
    "    except MemoryError:\n",
    "        rf=RandomForestRegressor(400,random_state=cfg.RANDOM_STATE); rf.fit(Xp,y); model=(\"rf\",rf)\n",
    "    return {\"model_type\":model[0],\"est\":model[1],\"pca\":pca,\"r2\":_cv_r2(model[1],Xp,y,groups)}\n",
    "\n",
    "def train_rul(df):\n",
    "    df=df.dropna(subset=[\"cycles_remaining\"])\n",
    "    X=np.vstack(df.features.values); y=df.cycles_remaining.values\n",
    "    groups=df.cell_id.values\n",
    "    X,y=_subsample(X,y,cfg.MAX_GP_TRAIN_SAMPLES_RUL)\n",
    "    gp=GaussianProcessRegressor(RBF(np.ones(X.shape[1]))+WhiteKernel(),alpha=1e-4,normalize_y=True,random_state=cfg.RANDOM_STATE)\n",
    "    gp.fit(X,y)\n",
    "    return {\"gp\":gp,\"r2\":_cv_r2(gp,X,y,groups)}\n",
    "\n",
    "# ── 4 · INFERENCE  ────────────────────────────────────────────────\n",
    "def predict_soc(bundle,feat):\n",
    "    X=feat.reshape(1,-1)\n",
    "    if bundle[\"pca\"] is not None: X,_=_maybe_pca(X,False,bundle[\"pca\"])\n",
    "    if bundle[\"model_type\"]==\"gp\":\n",
    "        mean,std=bundle[\"est\"].predict(X,return_std=True)\n",
    "    else:\n",
    "        mean=bundle[\"est\"].predict(X); std=np.std([t.predict(X) for t in bundle[\"est\"].estimators_],ddof=1)\n",
    "    return float(mean),float(np.clip(std,0.0,cfg.SOC_STD_CLAMP))\n",
    "\n",
    "def predict_rul(gp,feat):\n",
    "    mean,std=gp.predict(feat.reshape(1,-1),return_std=True)\n",
    "    return float(mean),float(std)\n",
    "\n",
    "# ── 5 · I/O helpers ───────────────────────────────────────────────\n",
    "def save_json(p,data): p.write_text(json.dumps(to_jsonable(data),indent=2))\n",
    "\n",
    "def save_plot(p,cycles,mean,std,ref):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(cycles,mean,lw=2,label=\"GP mean\")\n",
    "    plt.fill_between(cycles,mean-std,mean+std,alpha=0.3,label=\"GP ±1σ\")\n",
    "    plt.plot(cycles,ref,\"--\",lw=2,label=\"Parametric decay (ref)\")\n",
    "    plt.xlabel(\"Cycle\"); plt.ylabel(\"SoH (%)\"); plt.legend(); plt.tight_layout(); plt.savefig(p,dpi=300); plt.close()\n",
    "\n",
    "# ── 6 · End-to-end routine ────────────────────────────────────────\n",
    "def train_and_save():\n",
    "    df=build_dataset(cfg.EIS_DIR,cfg.CAP_DIR,cfg.INCLUDE_DRT)\n",
    "    df[\"cycles_remaining\"]=df.final_cycle-df.cycle_idx\n",
    "    df[\"soc_percent\"]=df.soc*100\n",
    "    soc=train_soc(df); rul=train_rul(df)\n",
    "    cfg.MODEL_DIR.mkdir(parents=True,exist_ok=True)\n",
    "    joblib.dump(soc,cfg.MODEL_DIR/\"soc.pkl\"); joblib.dump(rul,cfg.MODEL_DIR/\"rul.pkl\")\n",
    "    save_json(cfg.MODEL_DIR/\"metrics.json\",{\"soc_R2\":soc[\"r2\"],\"rul_R2\":rul[\"r2\"]})\n",
    "    print(f\"Training finished · SoC R²={soc['r2']:.3f}  RUL R²={rul['r2']:.3f}\")\n",
    "\n",
    "def infer(test_fp:Path):\n",
    "    soc=joblib.load(cfg.MODEL_DIR/\"soc.pkl\"); rul=joblib.load(cfg.MODEL_DIR/\"rul.pkl\")\n",
    "    d=build_feature_vector(test_fp,cfg.INCLUDE_DRT); feat=d[\"features\"]\n",
    "    soc_m,soc_s=predict_soc(soc,feat); rul_m,rul_s=predict_rul(rul[\"gp\"],feat)\n",
    "    cpp_map=build_cpp_map(cfg.CAP_DIR); cpp_est=get_cpp(d[\"cpp\"],cpp_map)\n",
    "    base=test_fp.with_suffix(\"\")\n",
    "    save_json(Path(f\"{base}_prediction.json\"),\n",
    "              dict(predicted_SoC_percent=soc_m,SoC_std_estimate=soc_s,\n",
    "                   predicted_cycles_remaining=rul_m,cycles_remaining_std=rul_s,\n",
    "                   fallback_cpp_cycles_remaining=cpp_est))\n",
    "    cy,ref=plot_projection(test_fp)\n",
    "    save_plot(Path(f\"{base}_projection.png\"),cy,rul_m-cy,np.full_like(cy,rul_s),ref)\n",
    "    print(\"Inference done → JSON & PNG saved.\")\n",
    "\n",
    "# ── 7 · CLI ───────────────────────────────────────────────────────\n",
    "# ── 7 · CLI ───────────────────────────────────────────────────────\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"One-file EIS training & inference\")\n",
    "    ap.add_argument(\"--test\", type=str, default=str(cfg.EIS_TEST_FILE))\n",
    "    ap.add_argument(\"--retrain\", action=\"store_true\")\n",
    "    \n",
    "    # ↓↓↓  ONE-LINE FIX: ignore any extra flags Jupyter adds (like “-f …json”)\n",
    "    args, _ = ap.parse_known_args()          # <= replaces:  args = ap.parse_args()\n",
    "\n",
    "    if args.retrain or cfg.FORCE_RETRAIN or not (cfg.MODEL_DIR / \"soc.pkl\").exists():\n",
    "        train_and_save()\n",
    "    infer(Path(args.test))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# run automatically in both contexts\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340886c2-5e3e-4cdd-ac66-68cbd1b4b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
