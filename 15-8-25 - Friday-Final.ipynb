{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fefa4c5c-c6de-4c70-8b13-6030a72b8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"EIS_TEST_FILES\": [\n",
      "    \"Mazda-Battery-Cell2.xlsx\"\n",
      "  ],\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"SOH_STD_MAX_OOD\": 2.0,\n",
      "  \"SOC_STD_MAX\": 0.95,\n",
      "  \"SOC_STD_MAX_OOD\": 0.95,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"GROUP_KFOLDS\": 0,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"INCLUDE_NORMALIZED_SHAPE_MODEL\": true,\n",
      "  \"ENSEMBLE_SOH\": true,\n",
      "  \"NORMALIZE_SHAPE_BY_HF_RE\": true,\n",
      "  \"SOC_INCLUDE_SHAPE_MODEL\": true,\n",
      "  \"SOC_MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"SOC_LABEL_JITTER\": 0.6,\n",
      "  \"OOD_SOC_ENABLE\": true,\n",
      "  \"OOD_SOC_Q\": 0.995,\n",
      "  \"OOD_SOC_PRIOR\": 50.0,\n",
      "  \"OOD_SOC_SHRINK_SCALE\": 2.0,\n",
      "  \"OOD_SOC_W_MIN\": 0.3,\n",
      "  \"OOD_SOC_PRIOR_MODE\": \"knn\",\n",
      "  \"SOC_OOD_USE_KNN\": true,\n",
      "  \"SOC_OOD_K\": 40,\n",
      "  \"SOC_CALIBRATE_ON_OOD\": true,\n",
      "  \"OOD_SOC_PRIOR_MAX_WEIGHT\": 0.45,\n",
      "  \"OOD_SOC_SHAPE_MAX_WEIGHT\": 0.3,\n",
      "  \"SOC_CALIBRATION_MODE\": \"auto\",\n",
      "  \"SOC_CAL_MIN_RANGE\": 0.001,\n",
      "  \"SOC_CAL_MIN_UNIQUE\": 4,\n",
      "  \"SOC_CAL_MIN_R2_IMPROVE\": -0.01,\n",
      "  \"ENABLE_CYCLES_MODEL\": true,\n",
      "  \"DECISION_SOH_PERCENT\": 50.0,\n",
      "  \"ILLUSTRATIVE_MIN_SOH\": 40.0,\n",
      "  \"CPP_ROLLING_WINDOW\": 5,\n",
      "  \"CPP_MIN_POINTS\": 6,\n",
      "  \"CPP_FALLBACK\": 20.0,\n",
      "  \"TEST_TEMPERATURE_OVERRIDE\": 25.0,\n",
      "  \"FORCE_RETRAIN\": false,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FEATURE_VERSION\": 97,\n",
      "  \"MAHAL_THRESHOLD\": 10.0,\n",
      "  \"GP_ARD_NORM_THRESHOLD\": 6.0,\n",
      "  \"PLOT_EXPONENT\": 1.25,\n",
      "  \"TARGET_SOH_THRESHOLDS\": [\n",
      "    80.0,\n",
      "    50.0,\n",
      "    40.0\n",
      "  ],\n",
      "  \"CYCLE_SCALE\": 1.0,\n",
      "  \"TARGET_CALIB_CYCLE_AT_80\": 1000.0,\n",
      "  \"CYCLE_TAIL_POINTS\": 4\n",
      "}\n",
      "[CYCLE-SCALE] auto=5.556  (target 80% at 1000.0)  → total scale=5.556\n",
      "[CPP] dynamic cells=19 global_cpp_median=93.40 (scaled)\n",
      "[LOAD] Found bundle. Signature match: True\n",
      "\n",
      "===== TEST: Mazda-Battery-Cell2.xlsx =====\n",
      "[SoH] Mazda-Battery-Cell2.xlsx: mean=91.62 std=2.00  OOD(SoH)=True\n",
      "[SoC] Mazda-Battery-Cell2.xlsx: base=50.00  → final=57.61  OOD=True\n",
      "{\n",
      "  \"file\": \"Mazda-Battery-Cell2.xlsx\",\n",
      "  \"feature_checksum\": \"4f879f3b4297a935a734cc2545a6a461b8a92e45\",\n",
      "  \"parsed_metadata\": null,\n",
      "  \"predicted_SoC_percent\": 57.60944924163232,\n",
      "  \"SoC_std_estimate\": 10.0,\n",
      "  \"soc_model_chosen\": \"ensemble_gpr+hgb+shape\",\n",
      "  \"soc_model_kind\": \"ensemble\",\n",
      "  \"soc_calibration_kind\": \"iso\",\n",
      "  \"predicted_SoH_percent\": 91.61500792387868,\n",
      "  \"SoH_std_estimate\": 2.0,\n",
      "  \"soh_model_chosen\": \"gpr_raw\",\n",
      "  \"predicted_cycle_index\": 1166.3493753297091,\n",
      "  \"predicted_cycles_remaining_to_thresholds\": {\n",
      "    \"80\": 1538.2369834548522,\n",
      "    \"50\": 5441.370111774264,\n",
      "    \"40\": 6799.644613863981\n",
      "  },\n",
      "  \"cycles_per_percent_est\": 131.7377423227762,\n",
      "  \"fallback_cpp_used\": null,\n",
      "  \"used_freq_from_file\": true,\n",
      "  \"freq_range_hz\": {\n",
      "    \"first\": 4973.323,\n",
      "    \"last\": 0.0099768\n",
      "  },\n",
      "  \"decision_threshold_percent\": 50.0,\n",
      "  \"lower_threshold_percent\": 40.0,\n",
      "  \"OOD_flag\": true\n",
      "}\n",
      "[PLOT] Saved: models_eis_phase2_phys\\artifacts\\Mazda-Battery-Cell2_projection.png\n",
      "[JSON] Saved: models_eis_phase2_phys\\artifacts\\Mazda-Battery-Cell2_prediction.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Unified EIS Training + Inference + Cycles-from-Training (v9.7.1 – temp-agnostic SoC ensemble)\n",
    "Fixes SoC collapsing by:\n",
    "  • Making SoC model temperature-agnostic (Temp feature is ignored for SoC)\n",
    "  • Using an ensemble (GPR + HGB + optional shape-GP) with validation-selected weights\n",
    "  • Calibrating SoC uncertainty from validation residuals (no artificial 0.95 clamp)\n",
    "  • Keeping SoH + cycles logic intact\n",
    "  • Adding debug fields in JSON: used_freq_from_file, freq_range_hz\n",
    "\n",
    "v9.7.1: BUGFIX — featurize_any() now calls build_shape_normalized(re_i, im_i) (correct kwarg name)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, random, warnings, joblib, hashlib, uuid, io, sys, os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers: environment & RNG\n",
    "# =========================\n",
    "def _running_in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # noqa\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell in (\"ZMQInteractiveShell\", \"Shell\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIGURATION\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Training data directories (update if needed)\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "\n",
    "    # Test files\n",
    "    EIS_TEST_FILES: List[Path] = None  # assigned after instantiation\n",
    "\n",
    "    # Frequency interpolation grid\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int = 60\n",
    "\n",
    "    # Uncertainty control\n",
    "    SOH_STD_MAX_OOD: float = 2.0\n",
    "\n",
    "    # SoC std caps (kept for compatibility; no hard clamp to 0.95 now)\n",
    "    SOC_STD_MAX: float = 0.95\n",
    "    SOC_STD_MAX_OOD: float = 0.95\n",
    "\n",
    "    # Train / split settings\n",
    "    TEST_FRAC: float = 0.2\n",
    "    GROUP_KFOLDS: int = 0\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    # Feature group toggles\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    # DRT params\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    # Capacity-based refinement\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "\n",
    "    # SoH modeling\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    INCLUDE_NORMALIZED_SHAPE_MODEL: bool = True\n",
    "    ENSEMBLE_SOH: bool = True\n",
    "    NORMALIZE_SHAPE_BY_HF_RE: bool = True\n",
    "\n",
    "    # SoC modeling (ENSEMBLE, temperature-agnostic)\n",
    "    SOC_INCLUDE_SHAPE_MODEL: bool = True\n",
    "    SOC_MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    SOC_LABEL_JITTER: float = 0.6  # helps if SoC labels are discrete bands\n",
    "\n",
    "    # OOD / SoC behavior\n",
    "    OOD_SOC_ENABLE: bool = True\n",
    "    OOD_SOC_Q: float = 0.995\n",
    "    OOD_SOC_PRIOR: float = 50.0\n",
    "    OOD_SOC_SHRINK_SCALE: float = 2.0\n",
    "    OOD_SOC_W_MIN: float = 0.3\n",
    "    OOD_SOC_PRIOR_MODE: str = \"knn\"\n",
    "    SOC_OOD_USE_KNN: bool = True\n",
    "    SOC_OOD_K: int = 40\n",
    "    SOC_CALIBRATE_ON_OOD: bool = True\n",
    "    OOD_SOC_PRIOR_MAX_WEIGHT: float = 0.45\n",
    "    OOD_SOC_SHAPE_MAX_WEIGHT: float = 0.3\n",
    "\n",
    "    # SoC calibration safety controls\n",
    "    SOC_CALIBRATION_MODE: str = \"auto\"        # \"auto\" | \"iso\" | \"linear\" | \"off\"\n",
    "    SOC_CAL_MIN_RANGE: float = 1e-3\n",
    "    SOC_CAL_MIN_UNIQUE: int = 4\n",
    "    SOC_CAL_MIN_R2_IMPROVE: float = -0.01\n",
    "\n",
    "    # Cycles modeling\n",
    "    ENABLE_CYCLES_MODEL: bool = True\n",
    "\n",
    "    # RUL parameters (for plotting)\n",
    "    DECISION_SOH_PERCENT: float = 50.0\n",
    "    ILLUSTRATIVE_MIN_SOH: float = 40.0\n",
    "    CPP_ROLLING_WINDOW: int = 5\n",
    "    CPP_MIN_POINTS: int = 6\n",
    "    CPP_FALLBACK: float = 20.0  # used only if no cycles model & no CPP est\n",
    "\n",
    "    # Inference extras\n",
    "    TEST_TEMPERATURE_OVERRIDE: Optional[float] = 25.0\n",
    "    FORCE_RETRAIN: bool = False\n",
    "\n",
    "    # Saving / logging\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    VERBOSE: bool = True\n",
    "\n",
    "    # ---- feature signature\n",
    "    FEATURE_VERSION: int = 97\n",
    "\n",
    "    # OOD thresholds (SoH)\n",
    "    MAHAL_THRESHOLD: float = 10.0\n",
    "    GP_ARD_NORM_THRESHOLD: float = 6.0\n",
    "\n",
    "    # Projection curve\n",
    "    PLOT_EXPONENT: float = 1.25\n",
    "\n",
    "    # Thresholds to report/plot\n",
    "    TARGET_SOH_THRESHOLDS: Tuple[float, ...] = (80.0, 50.0, 40.0)\n",
    "\n",
    "    # ---- Cycles scaling / extrapolation ----\n",
    "    CYCLE_SCALE: float = 1.0\n",
    "    TARGET_CALIB_CYCLE_AT_80: Optional[float] = 1000.0\n",
    "    CYCLE_TAIL_POINTS: int = 4\n",
    "\n",
    "cfg = Config()\n",
    "if cfg.EIS_TEST_FILES is None:\n",
    "    cfg.EIS_TEST_FILES = [Path(\"Mazda-Battery-Cell2.xlsx\")]\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def set_paths(eis_dir: str | Path, cap_dir: str | Path, model_dir: str | Path | None = None):\n",
    "    cfg.EIS_DIR = Path(eis_dir)\n",
    "    cfg.CAP_DIR = Path(cap_dir)\n",
    "    if model_dir is not None:\n",
    "        cfg.MODEL_DIR = Path(model_dir)\n",
    "        cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path): return str(x)\n",
    "    if isinstance(x, dict): return {k: to_jsonable(v) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "def config_signature(cfg: Config) -> str:\n",
    "    d = asdict(cfg).copy()\n",
    "    d[\"EIS_DIR\"] = str(d[\"EIS_DIR\"]); d[\"CAP_DIR\"]=str(d[\"CAP_DIR\"]); d[\"MODEL_DIR\"]=str(d[\"MODEL_DIR\"])\n",
    "    d.pop(\"EIS_TEST_FILES\", None)\n",
    "    blob = json.dumps(d, sort_keys=True)\n",
    "    return hashlib.sha256(blob.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "# =========================\n",
    "# 3. REGEX\n",
    "# =========================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. PARSERS\n",
    "# =========================\n",
    "def parse_eis_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"SOC\": float(d[\"SOC\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"RealSOH_file\": int(d[\"RealSOH\"])/100.0\n",
    "    }\n",
    "\n",
    "def parse_cap_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"CycleIndex\": int(d[\"Cycle\"])\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 5. LOADERS / INTERPOLATION\n",
    "# =========================\n",
    "def _find_matrix(mat_dict: dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[1] >= 3 and v.shape[0] >= 10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw, y_raw, freq_target):\n",
    "    freq_raw = np.asarray(freq_raw).astype(float)\n",
    "    y_raw = np.asarray(y_raw).astype(float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw = freq_raw[::-1]; y_raw = y_raw[::-1]\n",
    "    uniq, idx = np.unique(freq_raw, return_index=True)\n",
    "    if len(uniq) != len(freq_raw):\n",
    "        order = np.argsort(idx)\n",
    "        freq_raw = uniq[order]; y_raw = y_raw[idx][order]\n",
    "    f = interp1d(freq_raw, y_raw, bounds_error=False,\n",
    "                 fill_value=(y_raw[0], y_raw[-1]), kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "FREQ_CANDS = [\"frequency\",\"freq\",\"f\",\"hz\",\"frequency(hz)\",\"Frequency(Hz)\",\"Frequency\",\"FREQ\",\"Freq\",\"Hz\"]\n",
    "RE_CANDS   = [\"zreal\",\"re(z)\",\"re\",\"real\",\"z_re\",\"zreal(ohm)\",\"re (ohm)\",\"re(z) (ohm)\",\"Zreal\",\"Zreal (ohm)\",\"Zreal(ohm)\",\"Re\",\"Re(Z)\"]\n",
    "IM_CANDS   = [\"-zimag\",\"zimag\",\"im(z)\",\"im\",\"imag\",\"imaginary\",\"z_im\",\"zimg\",\"z_imag\",\" -Zimag (ohm)\",\" -Zimag(ohm)\",\"-Zimag\",\"Zimag\",\"Zimag (ohm)\",\"Im\",\"-Im(Z)\",\"-Zimag\"]\n",
    "\n",
    "def _select_column(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c.lower() in low: return low[c.lower()]\n",
    "    for c in cands:\n",
    "        for col in df.columns:\n",
    "            if c.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def load_mat_eis(path: Path):\n",
    "    mat = loadmat(path)\n",
    "    arr = _find_matrix(mat)\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"No valid EIS matrix in {path.name}\")\n",
    "    return arr[:,0].astype(float), arr[:,1].astype(float), arr[:,2].astype(float)\n",
    "\n",
    "def load_table_eis(path: Path):\n",
    "    # returns (freq, re, im, used_freq_from_file, (first,last))\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty table.\")\n",
    "    fcol = _select_column(df, FREQ_CANDS)\n",
    "    recol = _select_column(df, RE_CANDS)\n",
    "    imcol = _select_column(df, IM_CANDS)\n",
    "    if recol is None or imcol is None:\n",
    "        raise ValueError(f\"Missing Re/Im columns in {path.name}\")\n",
    "    re_vals = pd.to_numeric(df[recol], errors=\"coerce\").to_numpy()\n",
    "    im_vals = pd.to_numeric(df[imcol], errors=\"coerce\").to_numpy()\n",
    "    used_freq = True\n",
    "    first_last = (None, None)\n",
    "    if fcol is not None:\n",
    "        freq_vals = pd.to_numeric(df[fcol], errors=\"coerce\").to_numpy()\n",
    "        if np.isfinite(freq_vals).sum() >= 2:\n",
    "            first_last = (float(freq_vals[0]), float(freq_vals[-1]))\n",
    "    else:\n",
    "        used_freq = False\n",
    "        n = min(len(re_vals), len(im_vals))\n",
    "        freq_vals = np.geomspace(cfg.F_MAX, cfg.F_MIN, n)\n",
    "    n = min(len(freq_vals), len(re_vals), len(im_vals))\n",
    "    freq_vals = freq_vals[:n]; re_vals = re_vals[:n]; im_vals = im_vals[:n]\n",
    "    if np.nanmean(im_vals) > 0:\n",
    "        im_vals = -im_vals\n",
    "    return freq_vals.astype(float), re_vals.astype(float), im_vals.astype(float), used_freq, first_last\n",
    "\n",
    "def load_any_inference(path: Path):\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".mat\":\n",
    "        f,r,i = load_mat_eis(path); used=True; first_last=(None,None)\n",
    "    elif suf in (\".csv\",\".xls\",\".xlsx\"):\n",
    "        f,r,i,used,first_last = load_table_eis(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported test file extension: {suf}\")\n",
    "    return f,r,i,used,first_last\n",
    "\n",
    "# =========================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# =========================\n",
    "def compute_F_features(freq, re_i, im_i):\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    F1 = re_i[0]; F2 = re_i[idx_peak]; F3 = re_i[-1]\n",
    "    sc = np.where(np.sign(im_i[:-1]) != np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k = sc[0]; y0,y1 = im_i[k], im_i[k+1]\n",
    "        w = -y0/(y1 - y0 + 1e-12)\n",
    "        F4 = re_i[k] + w*(re_i[k+1]-re_i[k])\n",
    "    else:\n",
    "        F4 = np.nan\n",
    "    F5 = (re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6 = np.min(im_i)\n",
    "    mid_target = 10.0\n",
    "    idx_mid = int(np.argmin(np.abs(freq-mid_target)))\n",
    "    F7 = re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES = [\n",
    "    \"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "    \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"\n",
    "]\n",
    "\n",
    "def physical_features(freq, re_i, im_i):\n",
    "    freq = np.asarray(freq); re_i = np.asarray(re_i); im_i = np.asarray(im_i)\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    Rs = float(re_i[0]); Rpeak = float(re_i[idx_peak]); Rlow = float(re_i[-1])\n",
    "    Rct = max(Rpeak - Rs, 0.0)\n",
    "    arc_diam = Rlow - Rs\n",
    "    norm_arc = arc_diam / (Rs + 1e-9)\n",
    "    f_peak = float(freq[idx_peak])\n",
    "    tau_peak = 1.0/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K = min(10, len(freq)//3)\n",
    "    if K >= 4:\n",
    "        w_section = (2*np.pi*freq[-K:])**(-0.5)\n",
    "        re_section = re_i[-K:]\n",
    "        if len(np.unique(w_section)) > 2:\n",
    "            warburg_sigma = float(np.polyfit(w_section, re_section, 1)[0])\n",
    "        else:\n",
    "            warburg_sigma = np.nan\n",
    "    else:\n",
    "        warburg_sigma = np.nan\n",
    "    phase = np.arctan2(-im_i, re_i)\n",
    "    mid_mask = (freq>=1) & (freq<=100)\n",
    "    if mid_mask.sum()>2:\n",
    "        phase_mean_mid = float(phase[mid_mask].mean())\n",
    "        phase_std_mid  = float(phase[mid_mask].std())\n",
    "    else:\n",
    "        phase_mean_mid = np.nan; phase_std_mid = np.nan\n",
    "    phase_min = float(phase.min())\n",
    "    lf_mask = (freq<=1.0)\n",
    "    if lf_mask.sum() >= 4:\n",
    "        x = np.log10(freq[lf_mask]+1e-12); y = neg_im[lf_mask]\n",
    "        lf_slope = np.polyfit(x, y, 1)[0]\n",
    "    else:\n",
    "        lf_slope = np.nan\n",
    "    arc_quality = (neg_im.max() - neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS = [(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq, re_i, im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m], im_i[m])\n",
    "            feats += [z.mean(), z.std()]\n",
    "        else:\n",
    "            feats += [np.nan, np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq, re_i, im_i, segments=5):\n",
    "    logf = np.log10(freq)\n",
    "    edges = np.linspace(logf.min(), logf.max(), segments+1)\n",
    "    out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]\n",
    "            out += [np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else:\n",
    "            out += [np.nan, np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES = [\n",
    "    \"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "    \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"\n",
    "]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w = 2*np.pi*freq\n",
    "    tau = np.geomspace(tau_max, tau_min, n_tau)\n",
    "    WT = w[:,None]*tau[None,:]\n",
    "    denom = 1+WT**2\n",
    "    K_re = 1.0/denom\n",
    "    K_im = -WT/denom\n",
    "    R_inf = re_i[0]\n",
    "    y_re = re_i - R_inf\n",
    "    y_im = im_i\n",
    "    Y = np.concatenate([y_re, y_im])\n",
    "    K = np.vstack([K_re, K_im])\n",
    "    A = K.T @ K + lam*np.eye(n_tau)\n",
    "    b = K.T @ Y\n",
    "    gamma = linalg.solve(A,b,assume_a='pos')\n",
    "    gamma = np.clip(gamma,0,None)\n",
    "    return tau, gamma\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    try:\n",
    "        tau,gamma = compute_drt(freq,re_i,im_i,\n",
    "                                 cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,\n",
    "                                 cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau = np.log10(tau)\n",
    "        g_sum = gamma.sum()+1e-12\n",
    "        w_norm = gamma/g_sum\n",
    "        mean_logtau = float((w_norm*log_tau).sum())\n",
    "        var_logtau  = float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p = int(np.argmax(gamma))\n",
    "        peak_tau = float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid = np.median(log_tau)\n",
    "        frac_low = float(w_norm[log_tau<=mid].sum())\n",
    "        frac_high = 1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception:\n",
    "        return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i, im_i, temp, freq, include_names=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts += [re_i, im_i]\n",
    "        names += [f\"Re_{i}\" for i in range(len(re_i))] + [f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z = np.hypot(re_i, im_i)\n",
    "        basics=[re_i[0], re_i[-1], re_i[-1]-re_i[0], z.max(), z.mean(), z.std()]\n",
    "        parts.append(np.array(basics)); names += [\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        Ff=compute_F_features(freq,re_i,im_i); parts.append(np.array(Ff)); names += [f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        Pf=physical_features(freq,re_i,im_i); parts.append(np.array(Pf)); names += PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        Bf=band_stats(freq,re_i,im_i); parts.append(np.array(Bf))\n",
    "        for bi in range(len(BANDS)): names += [f\"band{bi}_mean\", f\"band{bi}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i); parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2): names += [f\"slope_re_seg{i}\", f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        Df=drt_features(freq,re_i,im_i); parts.append(np.array(Df)); names += DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names += [\"Feat_Temp\"]\n",
    "    vec = np.concatenate(parts).astype(float)\n",
    "    vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if include_names: return vec, names\n",
    "    return vec\n",
    "\n",
    "def build_shape_normalized(re_i, im_i, k: int = 5):\n",
    "    hf = float(np.nanmedian(re_i[:max(1, min(k, len(re_i)))]))\n",
    "    if not np.isfinite(hf) or abs(hf) < 1e-9:\n",
    "        hf = 1.0\n",
    "    return re_i / hf, im_i / hf\n",
    "\n",
    "# =========================\n",
    "# 7. CAPACITY & CYCLES TARGETS\n",
    "# =========================\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY):\n",
    "        return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta = parse_cap_metadata(fp.stem)\n",
    "        if not meta:\n",
    "            continue\n",
    "        try:\n",
    "            mat = loadmat(fp, squeeze_me=True, struct_as_record=False)\n",
    "            arr = _find_matrix(mat)\n",
    "            cap = None\n",
    "            if arr is not None:\n",
    "                col = np.argmax(np.abs(arr[-50:, :]).mean(axis=0))\n",
    "                cap = float(np.nanmax(arr[:, col]))\n",
    "            else:\n",
    "                d = mat.get(\"data\", None)\n",
    "                if d is not None:\n",
    "                    def _cell_to_1d(x):\n",
    "                        a = np.array(x, dtype=object).squeeze()\n",
    "                        out=[]\n",
    "                        for e in a.flat:\n",
    "                            if isinstance(e, np.ndarray):\n",
    "                                out.append(float(np.nanmax(e.astype(float))) if e.size else np.nan)\n",
    "                            else:\n",
    "                                try: out.append(float(e))\n",
    "                                except Exception: out.append(np.nan)\n",
    "                        z = np.array(out, dtype=float)\n",
    "                        if z.ndim == 0: z = z[None]\n",
    "                        return z\n",
    "                    if hasattr(d, \"AhAccu\"):\n",
    "                        v = _cell_to_1d(getattr(d, \"AhAccu\"))\n",
    "                        if v.size: cap = float(np.nanmax(v))\n",
    "                    if cap is None and hasattr(d, \"WhAccu\"):\n",
    "                        v = _cell_to_1d(getattr(d, \"WhAccu\"))\n",
    "                        if v.size: cap = float(np.nanmax(v) / 3.7)\n",
    "            if cap is None or not np.isfinite(cap):\n",
    "                continue\n",
    "            meta[\"MeasuredCapacity_Ah\"] = cap\n",
    "            recs.append(meta)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(recs)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    ref = df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"] = df[\"MeasuredCapacity_Ah\"] / ref\n",
    "    df[\"SoH_percent\"] = df[\"NormCapacity\"] * 100.0\n",
    "    return df\n",
    "\n",
    "def _build_soh_to_cycle_interpolators(cap_df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "    maps={}\n",
    "    if cap_df.empty: return maps\n",
    "    for cid, g in cap_df.groupby(\"CellID\"):\n",
    "        g = g.sort_values(\"CycleIndex\")\n",
    "        x = np.asarray(g[\"SoH_percent\"].values, dtype=float)\n",
    "        y = np.asarray(g[\"CycleIndex\"].values, dtype=float)\n",
    "        order = np.argsort(x)\n",
    "        x_sorted = x[order]\n",
    "        y_sorted = y[order]\n",
    "        uniq = np.unique(x_sorted)\n",
    "        cyc_agg=[]\n",
    "        for s in uniq:\n",
    "            cyc_agg.append(float(np.nanmean(y_sorted[x_sorted==s])))\n",
    "        soh = uniq\n",
    "        cyc = np.asarray(cyc_agg, dtype=float)\n",
    "        maps[cid] = {\"soh\": soh, \"cyc\": cyc}\n",
    "    return maps\n",
    "\n",
    "def _tail_cpp_from_map(soh: np.ndarray, cyc: np.ndarray, k: int) -> Optional[float]:\n",
    "    if soh.size < 2: return None\n",
    "    k = max(2, min(k, soh.size))\n",
    "    xs = soh[:k]; ys = cyc[:k]\n",
    "    if len(np.unique(xs)) < 2: return None\n",
    "    slope = np.polyfit(xs, ys, 1)[0]\n",
    "    return abs(float(slope))\n",
    "\n",
    "def _interp_or_extrap_cycle_for_soh(cell_map: Dict[str, Dict[str, Any]],\n",
    "                                    cell_id: str,\n",
    "                                    soh_val: float,\n",
    "                                    k_tail: int,\n",
    "                                    fallback_cpp: float) -> Optional[float]:\n",
    "    m = cell_map.get(cell_id)\n",
    "    if not m: return None\n",
    "    soh = m[\"soh\"]; cyc = m[\"cyc\"]\n",
    "    if soh.size < 2: return None\n",
    "    if soh_val >= soh.min() and soh_val <= soh.max():\n",
    "        return float(np.interp(soh_val, soh, cyc, left=cyc[0], right=cyc[-1]))\n",
    "    if soh_val < soh.min():\n",
    "        cpp = _tail_cpp_from_map(soh, cyc, k_tail) or float(fallback_cpp)\n",
    "        delta = float(soh.min() - soh_val)\n",
    "        return float(cyc[0] + cpp * delta)\n",
    "    return float(cyc[-1])\n",
    "\n",
    "def estimate_cpp_per_cell(capacity_df: pd.DataFrame,\n",
    "                          window:int, min_points:int)->Dict[str,float]:\n",
    "    cpp={}\n",
    "    for cid,grp in capacity_df.groupby(\"CellID\"):\n",
    "        g=grp.sort_values(\"CycleIndex\")\n",
    "        if g.shape[0]<min_points: continue\n",
    "        tail=g.tail(window)\n",
    "        x=tail[\"CycleIndex\"].values.astype(float)\n",
    "        y=tail[\"SoH_percent\"].values.astype(float)\n",
    "        if len(np.unique(x))<2: continue\n",
    "        slope=np.polyfit(x,y,1)[0]\n",
    "        if slope >= -1e-6:\n",
    "            continue\n",
    "        cpp[cid]=1.0/abs(slope)\n",
    "    return cpp\n",
    "\n",
    "def build_cpp_map(cap_df: pd.DataFrame):\n",
    "    if cap_df.empty: return {}, cfg.CPP_FALLBACK\n",
    "    cpp_map=estimate_cpp_per_cell(\n",
    "        cap_df[[\"CellID\",\"CycleIndex\",\"SoH_percent\"]],\n",
    "        cfg.CPP_ROLLING_WINDOW, cfg.CPP_MIN_POINTS\n",
    "    )\n",
    "    if not cpp_map:\n",
    "        return {}, cfg.CPP_FALLBACK\n",
    "    return cpp_map, float(np.median(list(cpp_map.values())))\n",
    "\n",
    "def _calibrate_cycle_scale(cap_df: pd.DataFrame, target_80: Optional[float]) -> float:\n",
    "    if cap_df.empty or target_80 is None: return 1.0\n",
    "    maps = _build_soh_to_cycle_interpolators(cap_df)\n",
    "    vals=[]\n",
    "    for cid in cap_df[\"CellID\"].unique():\n",
    "        m = maps.get(cid)\n",
    "        if not m: continue\n",
    "        c80 = np.interp(80.0, m[\"soh\"], m[\"cyc\"], left=m[\"cyc\"][0], right=m[\"cyc\"][-1])\n",
    "        if np.isfinite(c80) and c80>0: vals.append(float(c80))\n",
    "    if not vals: return 1.0\n",
    "    med = float(np.median(vals))\n",
    "    if med <= 0: return 1.0\n",
    "    return float(target_80/med)\n",
    "\n",
    "def get_cpp(meta: dict, cpp_map: Dict[str,float], global_cpp: float):\n",
    "    if not meta: return global_cpp\n",
    "    return cpp_map.get(meta.get(\"CellID\"), global_cpp)\n",
    "\n",
    "# =========================\n",
    "# 8. DATASET BUILD\n",
    "# =========================\n",
    "def load_single_eis_mat(fp: Path):\n",
    "    meta = parse_eis_metadata(fp.stem)\n",
    "    if meta is None:\n",
    "        raise ValueError(f\"Bad filename: {fp.name}\")\n",
    "    freq,re_z,im_z = load_mat_eis(fp)\n",
    "    re_i=_interp_channel(freq, re_z, CANON_FREQ)\n",
    "    im_i=_interp_channel(freq, im_z, CANON_FREQ)\n",
    "    vec, names = build_feature_vector(re_i, im_i, meta[\"Temp\"], CANON_FREQ, include_names=True)\n",
    "    return vec, names, meta, re_i, im_i\n",
    "\n",
    "def _build_cycles_targets(meta_df: pd.DataFrame,\n",
    "                          cap_df: pd.DataFrame,\n",
    "                          cycle_scale: float,\n",
    "                          k_tail: int) -> Tuple[np.ndarray, Dict[float, np.ndarray]]:\n",
    "    y_cycle_index = np.full(len(meta_df), np.nan, dtype=float)\n",
    "    y_rem_dict: Dict[float, np.ndarray] = {thr: np.full(len(meta_df), np.nan, dtype=float)\n",
    "                                           for thr in cfg.TARGET_SOH_THRESHOLDS}\n",
    "    if cap_df.empty:\n",
    "        return y_cycle_index, y_rem_dict\n",
    "\n",
    "    maps = _build_soh_to_cycle_interpolators(cap_df)\n",
    "    cpp_map, cpp_global = build_cpp_map(cap_df)\n",
    "\n",
    "    for i, row in meta_df.reset_index(drop=True).iterrows():\n",
    "        cid = row[\"CellID\"]\n",
    "        soh_here = float(row[\"SoH_cont\"])\n",
    "\n",
    "        cyc_here = _interp_or_extrap_cycle_for_soh(\n",
    "            maps, cid, soh_here, k_tail=k_tail, fallback_cpp=cpp_map.get(cid, cpp_global)\n",
    "        )\n",
    "        if cyc_here is None:\n",
    "            continue\n",
    "        y_cycle_index[i] = float(max(0.0, cyc_here))\n",
    "\n",
    "        for thr in cfg.TARGET_SOH_THRESHOLDS:\n",
    "            cthr = _interp_or_extrap_cycle_for_soh(\n",
    "                maps, cid, float(thr), k_tail=k_tail, fallback_cpp=cpp_map.get(cid, cpp_global)\n",
    "            )\n",
    "            if cthr is None:\n",
    "                y_rem_dict[thr][i] = np.nan\n",
    "            else:\n",
    "                y_rem_dict[thr][i] = float(max(0.0, cthr - cyc_here))\n",
    "\n",
    "    if cycle_scale and cycle_scale != 1.0:\n",
    "        y_cycle_index *= float(cycle_scale)\n",
    "        for thr in y_rem_dict:\n",
    "            y_rem_dict[thr] *= float(cycle_scale)\n",
    "\n",
    "    return y_cycle_index, y_rem_dict\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_df: Optional[pd.DataFrame], cycle_scale: float):\n",
    "    files = sorted(eis_dir.rglob(\"*.mat\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .mat spectra in {eis_dir}\")\n",
    "\n",
    "    f0,r0,i0 = load_mat_eis(files[0])\n",
    "    re0=_interp_channel(f0,r0,CANON_FREQ); im0=_interp_channel(f0,i0,CANON_FREQ)\n",
    "    _, feature_names = build_feature_vector(re0, im0, 25.0, CANON_FREQ, include_names=True)\n",
    "\n",
    "    feats=[]; rows=[]; shape_feats=[]\n",
    "    for fp in tqdm(files, desc=\"Loading training spectra\"):\n",
    "        try:\n",
    "            v, names, m, rei, imi = load_single_eis_mat(fp)\n",
    "            feats.append(v); rows.append(m)\n",
    "            if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL or cfg.SOC_INCLUDE_SHAPE_MODEL) and cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "                rsh, ish = build_shape_normalized(rei, imi)\n",
    "                shape_vec = build_feature_vector(rsh, ish, m[\"Temp\"], CANON_FREQ)\n",
    "                shape_feats.append(shape_vec)\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip] {fp.name}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid training spectra after filtering.\")\n",
    "\n",
    "    X = np.vstack(feats)\n",
    "    X_shape = np.vstack(shape_feats) if shape_feats else None\n",
    "    meta_df = pd.DataFrame(rows)\n",
    "\n",
    "    # SoH refinement\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup = cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        refined=[]\n",
    "        for cid, stage, fallback in zip(meta_df.CellID, meta_df.SOH_stage, meta_df.RealSOH_file):\n",
    "            nc = lookup.get((cid, stage))\n",
    "            refined.append(100.0*nc if nc is not None else fallback)\n",
    "        meta_df[\"SoH_cont\"]=refined\n",
    "    else:\n",
    "        meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "\n",
    "    # Targets\n",
    "    y_soc = meta_df[\"SOC\"].astype(float).values\n",
    "    y_soh = meta_df[\"SoH_cont\"].values\n",
    "\n",
    "    # Cycles targets\n",
    "    if cfg.ENABLE_CYCLES_MODEL and cap_df is not None:\n",
    "        y_cycle_index, y_rem_dict = _build_cycles_targets(\n",
    "            meta_df, cap_df, cycle_scale=cycle_scale, k_tail=cfg.CYCLE_TAIL_POINTS\n",
    "        )\n",
    "    else:\n",
    "        y_cycle_index = np.full(len(meta_df), np.nan)\n",
    "        y_rem_dict = {thr: np.full(len(meta_df), np.nan) for thr in cfg.TARGET_SOH_THRESHOLDS}\n",
    "\n",
    "    soh_var = float(np.var(y_soh))\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[DATA] SoH range: {y_soh.min():.2f} – {y_soh.max():.2f} (var={soh_var:.3f})\")\n",
    "        if soh_var < 1.0:\n",
    "            print(\"[WARN] Low SoH variance → model may output near-constant SoH.\")\n",
    "\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        pd.concat(\n",
    "            [meta_df.reset_index(drop=True),\n",
    "             pd.DataFrame(X, columns=names)], axis=1\n",
    "        ).to_parquet(cfg.MODEL_DIR/\"training_features.parquet\", index=False)\n",
    "\n",
    "    return meta_df, X, (X_shape, names), y_soc, y_soh, y_cycle_index, y_rem_dict\n",
    "\n",
    "# =========================\n",
    "# 9. SPLITTING\n",
    "# =========================\n",
    "def cell_split_mask(meta_df: pd.DataFrame):\n",
    "    cells = meta_df.CellID.unique()\n",
    "    rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test = max(1, int(len(cells)*cfg.TEST_FRAC))\n",
    "    test_cells = rng.choice(cells, size=n_test, replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "# =========================\n",
    "# 10. MODELS (helpers)\n",
    "# =========================\n",
    "def _fit_gpr(X, y, seed, max_samples):\n",
    "    dim = X.shape[1]\n",
    "    kernel = RBF(length_scale=np.ones(dim)*3.0,\n",
    "                 length_scale_bounds=(1e-1,1e6)) + \\\n",
    "             WhiteKernel(noise_level=1e-2,\n",
    "                         noise_level_bounds=(1e-6,1e-1))\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel, alpha=0.0, normalize_y=True,\n",
    "        random_state=seed, n_restarts_optimizer=3\n",
    "    )\n",
    "    if X.shape[0] > max_samples:\n",
    "        idx = np.random.default_rng(seed).choice(\n",
    "            X.shape[0], size=max_samples, replace=False)\n",
    "        gpr.fit(X[idx], y[idx])\n",
    "    else:\n",
    "        gpr.fit(X, y)\n",
    "    return gpr\n",
    "\n",
    "def _fit_hgb(X, y):\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_iter=600,\n",
    "        l2_regularization=1e-3, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    hgb.fit(X, y)\n",
    "    return hgb\n",
    "\n",
    "def _evaluate(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred), math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def _can_calibrate_soc(val_pred: np.ndarray) -> bool:\n",
    "    if val_pred is None or len(val_pred) < 3:\n",
    "        return False\n",
    "    if np.ptp(val_pred) < cfg.SOC_CAL_MIN_RANGE:\n",
    "        return False\n",
    "    if len(np.unique(np.round(val_pred, 3))) < cfg.SOC_CAL_MIN_UNIQUE:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class _IdentityCalibrator:\n",
    "    def fit(self, x, y): return self\n",
    "    def predict(self, x): return np.asarray(x, dtype=float)\n",
    "\n",
    "def _ensemble_var(means, stds, weights):\n",
    "    \"\"\"Variance of a mixture-of-experts with fixed weights.\"\"\"\n",
    "    m = float(np.sum([w*mu for w, mu in zip(weights, means)]))\n",
    "    var = 0.0\n",
    "    for w, mu, sd in zip(weights, means, stds):\n",
    "        var += w * ((sd if np.isfinite(sd) else 0.0)**2 + (mu - m)**2)\n",
    "    return float(max(var, 1e-9))\n",
    "\n",
    "def _grid_weights(n, step=0.1):\n",
    "    \"\"\"Yield weight vectors of length n that sum to 1 (coarse grid).\"\"\"\n",
    "    if n == 1:\n",
    "        yield [1.0]; return\n",
    "    steps = int(round(1.0/step))\n",
    "    if n == 2:\n",
    "        for i in range(steps+1):\n",
    "            yield [i*step, 1.0 - i*step]\n",
    "        return\n",
    "    for i in range(steps+1):\n",
    "        for j in range(steps+1 - i):\n",
    "            k = steps - i - j\n",
    "            s = i + j + k\n",
    "            if s == steps:\n",
    "                yield [i*step, j*step, k*step]\n",
    "\n",
    "# =========================\n",
    "# 11. TRAINING\n",
    "# =========================\n",
    "def train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict):\n",
    "    X_shape, feature_names = shape_bundle\n",
    "    mask_test = cell_split_mask(meta_df)\n",
    "\n",
    "    # ===== SoC (temperature-agnostic ensemble) =====\n",
    "    temp_idx = feature_names.index(\"Feat_Temp\")\n",
    "    X_soc_raw = X_raw.copy()\n",
    "    X_soc_raw[:, temp_idx] = 0.0\n",
    "\n",
    "    soc_scaler = StandardScaler()\n",
    "    X_soc_s = soc_scaler.fit_transform(X_soc_raw)\n",
    "\n",
    "    # Optional normalized-shape branch for SoC\n",
    "    Xs_shape = None\n",
    "    soc_shape_scaler = None\n",
    "    if cfg.SOC_INCLUDE_SHAPE_MODEL and (X_shape is not None):\n",
    "        X_shape_soc = X_shape.copy()\n",
    "        X_shape_soc[:, temp_idx] = 0.0\n",
    "        soc_shape_scaler = StandardScaler()\n",
    "        Xs_shape = soc_shape_scaler.fit_transform(X_shape_soc)\n",
    "\n",
    "    # Label jitter (helpful if SoC labels are discrete)\n",
    "    y_soc_train = y_soc.copy()\n",
    "    if cfg.SOC_LABEL_JITTER and cfg.SOC_LABEL_JITTER > 0:\n",
    "        rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "        y_soc_train = np.clip(y_soc_train + rng.normal(0.0, cfg.SOC_LABEL_JITTER, size=y_soc_train.shape), 0.0, 100.0)\n",
    "\n",
    "    soc_candidates = {}\n",
    "\n",
    "    # GPR\n",
    "    soc_gpr = _fit_gpr(X_soc_s, y_soc_train, cfg.RANDOM_STATE, cfg.SOC_MAX_GPR_TRAIN_SAMPLES)\n",
    "    gp_val = soc_gpr.predict(X_soc_s[mask_test])\n",
    "    r2_gp, rmse_gp = _evaluate(y_soc[mask_test], gp_val)\n",
    "    soc_candidates[\"gpr\"] = (soc_gpr, r2_gp, rmse_gp)\n",
    "\n",
    "    # HGB\n",
    "    soc_hgb = _fit_hgb(X_soc_s[~mask_test], y_soc_train[~mask_test])\n",
    "    hgb_val = soc_hgb.predict(X_soc_s[mask_test])\n",
    "    r2_hgb, rmse_hgb = _evaluate(y_soc[mask_test], hgb_val)\n",
    "    soc_candidates[\"hgb\"] = (soc_hgb, r2_hgb, rmse_hgb)\n",
    "\n",
    "    # Shape GP (optional)\n",
    "    soc_shape_model = None; r2_shape = None; rmse_shape = None; shp_val = None\n",
    "    if Xs_shape is not None:\n",
    "        soc_shape_model = _fit_gpr(Xs_shape, y_soc_train, cfg.RANDOM_STATE, cfg.SOC_MAX_GPR_TRAIN_SAMPLES)\n",
    "        shp_val = soc_shape_model.predict(Xs_shape[mask_test])\n",
    "        r2_shape, rmse_shape = _evaluate(y_soc[mask_test], shp_val)\n",
    "\n",
    "    preds_on_val = [gp_val, hgb_val] + ([shp_val] if shp_val is not None else [])\n",
    "    stds_on_val  = []\n",
    "    try:\n",
    "        _, gp_std_val = soc_gpr.predict(X_soc_s[mask_test], return_std=True)\n",
    "    except Exception:\n",
    "        gp_std_val = np.full_like(gp_val, float(r2_gp))\n",
    "    stds_on_val.append(gp_std_val.astype(float))\n",
    "    stds_on_val.append(np.full_like(hgb_val, float(rmse_hgb)))\n",
    "    if shp_val is not None:\n",
    "        try:\n",
    "            _, shp_std_val = soc_shape_model.predict(Xs_shape[mask_test], return_std=True)\n",
    "        except Exception:\n",
    "            shp_std_val = np.full_like(shp_val, float(rmse_shape if rmse_shape is not None else rmse_gp))\n",
    "        stds_on_val.append(shp_std_val.astype(float))\n",
    "\n",
    "    P = np.vstack(preds_on_val)\n",
    "    S = np.vstack(stds_on_val)\n",
    "    yv = y_soc[mask_test].astype(float)\n",
    "\n",
    "    names = [\"gpr\", \"hgb\"] + ([\"shape\"] if shp_val is not None else [])\n",
    "    best = None\n",
    "    for w in _grid_weights(len(names), step=0.1):\n",
    "        w = np.array(w, dtype=float)\n",
    "        mix = (w[:,None] * P).sum(0)\n",
    "        r2, rmse = _evaluate(yv, mix)\n",
    "        if (best is None) or (rmse < best[\"rmse\"]):\n",
    "            best = {\"weights\": w, \"rmse\": float(rmse), \"r2\": float(r2)}\n",
    "    soc_weights = best[\"weights\"]\n",
    "\n",
    "    # Uncertainty calibration on validation residuals\n",
    "    mu_mix = (soc_weights[:,None] * P).sum(0)\n",
    "    var_mix = np.zeros_like(mu_mix)\n",
    "    for wi, mu_i, sd_i in zip(soc_weights, P, S):\n",
    "        var_mix += wi * (sd_i**2 + (mu_i - mu_mix)**2)\n",
    "    var_mix = np.clip(var_mix, 1e-9, None)\n",
    "    resid2 = (yv - mu_mix)**2\n",
    "    alpha = float(np.sqrt(np.mean(resid2) / np.mean(var_mix))) if np.isfinite(var_mix).all() else 1.0\n",
    "    alpha = float(np.clip(alpha, 0.3, 3.0))\n",
    "\n",
    "    # Final SoC calibration (isotonic/linear) — only if helpful\n",
    "    soc_calibrator = _IdentityCalibrator()\n",
    "    cal_kind = \"identity\"\n",
    "    r2_before, rmse_before = best[\"r2\"], best[\"rmse\"]\n",
    "    if cfg.SOC_CALIBRATION_MODE != \"off\" and _can_calibrate_soc(mu_mix):\n",
    "        tried = []\n",
    "        if cfg.SOC_CALIBRATION_MODE in (\"auto\",\"iso\"):\n",
    "            try:\n",
    "                iso = IsotonicRegression(y_min=0.0, y_max=100.0, out_of_bounds=\"clip\")\n",
    "                iso.fit(mu_mix, yv)\n",
    "                y_cal = iso.predict(mu_mix)\n",
    "                r2_iso, rmse_iso = _evaluate(yv, y_cal)\n",
    "                tried.append((\"iso\", iso, r2_iso, rmse_iso))\n",
    "            except Exception: pass\n",
    "        if cfg.SOC_CALIBRATION_MODE in (\"auto\",\"linear\"):\n",
    "            try:\n",
    "                lin = LinearRegression()\n",
    "                lin.fit(mu_mix.reshape(-1,1), yv)\n",
    "                y_lin = np.clip(lin.predict(mu_mix.reshape(-1,1)), 0, 100)\n",
    "                r2_lin, rmse_lin = _evaluate(yv, y_lin)\n",
    "                tried.append((\"linear\", lin, r2_lin, rmse_lin))\n",
    "            except Exception: pass\n",
    "        if tried:\n",
    "            name, model, r2_b, rmse_b = max(tried, key=lambda t: t[2])\n",
    "            if r2_b >= r2_before + cfg.SOC_CAL_MIN_R2_IMPROVE:\n",
    "                soc_calibrator = model\n",
    "                cal_kind = name\n",
    "                r2_before, rmse_before = r2_b, rmse_b\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoC] Candidates (R2 | RMSE):\")\n",
    "        print(f\"      - soc_gpr_raw   R2={r2_gp:.3f}  RMSE={rmse_gp:.2f}\")\n",
    "        print(f\"      - soc_hgb_raw   R2={r2_hgb:.3f}  RMSE={rmse_hgb:.2f}\")\n",
    "        if shp_val is not None:\n",
    "            print(f\"      - shapeGP       R2={r2_shape:.3f}  RMSE={rmse_shape:.2f}\")\n",
    "        print(f\"[SoC] Selected base = ensemble({'+'.join(names)}) weights={soc_weights.tolist()}\")\n",
    "        print(f\"[SoC-Cal] kind={cal_kind}  R2_val={r2_before:.3f}  RMSE_val={rmse_before:.2f}\")\n",
    "\n",
    "    # SoC-space OOD stats\n",
    "    soc_center = X_soc_s.mean(axis=0)\n",
    "    try:\n",
    "        soc_cov_inv = np.linalg.pinv(np.cov(X_soc_s.T))\n",
    "        dists = [float(np.sqrt((x - soc_center) @ soc_cov_inv @ (x - soc_center).T)) for x in X_soc_s]\n",
    "        soc_mahal_thresh = float(np.quantile(dists, cfg.OOD_SOC_Q))\n",
    "    except Exception:\n",
    "        soc_cov_inv = np.eye(X_soc_s.shape[1]); soc_mahal_thresh = None\n",
    "\n",
    "    # Save SoC KNN anchor set (SoC space)\n",
    "    X_soc_train_for_knn = X_soc_s[~mask_test]\n",
    "    y_soc_train_vals = y_soc[~mask_test]\n",
    "\n",
    "    soc_bundle = {\n",
    "        \"names\": names,\n",
    "        \"weights\": soc_weights.tolist(),\n",
    "        \"gpr\": soc_gpr,\n",
    "        \"hgb\": soc_hgb,\n",
    "        \"shape_scaler\": soc_shape_scaler,\n",
    "        \"shape_model\": soc_shape_model,\n",
    "        \"calibrator\": soc_calibrator,\n",
    "        \"cal_kind\": cal_kind,\n",
    "        \"std_alpha\": alpha,\n",
    "        \"val_rmse\": float(rmse_before),\n",
    "        \"temp_idx\": int(temp_idx),\n",
    "        \"soc_scaler\": soc_scaler,\n",
    "        \"soc_center\": soc_center.tolist(),\n",
    "        \"soc_cov_inv\": soc_cov_inv.tolist(),\n",
    "        \"soc_mahal_thresh\": None if soc_mahal_thresh is None else float(soc_mahal_thresh),\n",
    "        \"soc_knn_X\": X_soc_train_for_knn,\n",
    "        \"soc_knn_y\": y_soc_train_vals,\n",
    "    }\n",
    "\n",
    "    # ===== SoH =====\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(X_raw)\n",
    "\n",
    "    soh_candidates = {}\n",
    "    soh_gpr = _fit_gpr(X_s, y_soh, cfg.RANDOM_STATE, cfg.MAX_GPR_TRAIN_SAMPLES)\n",
    "    r2g, rmseg = _evaluate(y_soh[mask_test], soh_gpr.predict(X_s[mask_test]))\n",
    "    soh_candidates[\"gpr_raw\"] = (soh_gpr, r2g, rmseg)\n",
    "\n",
    "    soh_hgb = _fit_hgb(X_s[~mask_test], y_soh[~mask_test])\n",
    "    r2h2, rmseh2 = _evaluate(y_soh[mask_test], soh_hgb.predict(X_s[mask_test]))\n",
    "    soh_candidates[\"hgb_raw\"] = (soh_hgb, r2h2, rmseh2)\n",
    "\n",
    "    shape_model=shape_scaler=None\n",
    "    shape_metrics=None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and (X_shape is not None):\n",
    "        shape_scaler = StandardScaler()\n",
    "        X_shape_s = shape_scaler.fit_transform(X_shape)\n",
    "        shape_model = _fit_gpr(X_shape_s, y_soh, cfg.RANDOM_STATE, cfg.MAX_GPR_TRAIN_SAMPLES)\n",
    "        spred = shape_model.predict(X_shape_s[mask_test])\n",
    "        r2s2, rmses2 = _evaluate(y_soh[mask_test], spred)\n",
    "        soh_candidates[\"gpr_shape\"] = (shape_model, r2s2, rmses2)\n",
    "        shape_metrics = {\"r2\": r2s2, \"rmse\": rmses2}\n",
    "\n",
    "    soh_best_name = max([\"gpr_raw\",\"hgb_raw\"], key=lambda k: soh_candidates[k][1])\n",
    "    soh_best_model, soh_best_r2, soh_best_rmse = soh_candidates[soh_best_name]\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoH] GPR_raw:  R2={r2g:.3f} RMSE={rmseg:.2f}\")\n",
    "        print(f\"[SoH] HGB_raw: R2={r2h2:.3f} RMSE={rmseh2:.2f}\")\n",
    "        if shape_metrics:\n",
    "            print(f\"[SoH] ShapeGP: R2={shape_metrics['r2']:.3f} RMSE={shape_metrics['rmse']:.2f}\")\n",
    "        print(f\"[SoH] Selected raw model = {soh_best_name}\")\n",
    "\n",
    "    # SoH OOD stats\n",
    "    cov = np.cov(X_s.T)\n",
    "    try:\n",
    "        cov_inv = np.linalg.pinv(cov)\n",
    "    except Exception:\n",
    "        cov_inv = np.eye(cov.shape[0])\n",
    "    center = X_s.mean(axis=0)\n",
    "\n",
    "    # ----- Cycles models -----\n",
    "    cycles_models = {}\n",
    "    cycles_metrics = {}\n",
    "    if cfg.ENABLE_CYCLES_MODEL:\n",
    "        m_valid = np.isfinite(y_cycle_index)\n",
    "        if m_valid.sum() >= 10:\n",
    "            cyc_hgb = _fit_hgb(X_s[m_valid], y_cycle_index[m_valid])\n",
    "            r2c, rmsec = _evaluate(y_cycle_index[mask_test & m_valid], cyc_hgb.predict(X_s[mask_test & m_valid]) if (mask_test & m_valid).any() else y_cycle_index[m_valid])\n",
    "            cycles_models[\"absolute\"] = {\"model\": cyc_hgb, \"scaler\": scaler}\n",
    "            cycles_metrics[\"absolute\"] = {\"r2\": float(r2c), \"rmse\": float(rmsec), \"n\": int(m_valid.sum())}\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[Cycles] absolute: R2={r2c:.3f} RMSE={rmsec:.2f}  n={m_valid.sum()}\")\n",
    "        for thr, arr in y_rem_dict.items():\n",
    "            mv = np.isfinite(arr)\n",
    "            if mv.sum() < 10:\n",
    "                continue\n",
    "            rem_model = _fit_hgb(X_s[mv], arr[mv])\n",
    "            r2r, rmser = _evaluate(arr[mask_test & mv], rem_model.predict(X_s[mask_test & mv]) if (mask_test & mv).any() else arr[mv])\n",
    "            cycles_models[str(int(thr))] = {\"model\": rem_model, \"scaler\": scaler}\n",
    "            cycles_metrics[str(int(thr))] = {\"r2\": float(r2r), \"rmse\": float(rmser), \"n\": int(mv.sum())}\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[Cycles] remaining→{int(thr)}%: R2={r2r:.3f} RMSE={rmser:.2f}  n={mv.sum()}\")\n",
    "\n",
    "    bundle = {\n",
    "        # SoC (new)\n",
    "        \"soc_ensemble\": soc_bundle,\n",
    "        # SoH\n",
    "        \"shared_scaler\": scaler,\n",
    "        \"soh_model\": soh_best_model,\n",
    "        \"soh_model_name\": soh_best_name,\n",
    "        \"shape_scaler\": shape_scaler,\n",
    "        \"shape_model\": shape_model,\n",
    "        # Cycles\n",
    "        \"cycles_models\": cycles_models,\n",
    "        \"cycles_metrics\": cycles_metrics,\n",
    "        # Meta / persistence\n",
    "        \"freq_grid\": CANON_FREQ,\n",
    "        \"feature_version\": cfg.FEATURE_VERSION,\n",
    "        \"feature_manifest\": feature_names,\n",
    "        \"config_signature\": config_signature(cfg),\n",
    "        \"config\": to_jsonable(asdict(cfg)),\n",
    "        \"cycle_scale\": float(CYCLE_SCALE_GLOBAL),\n",
    "        \"metrics\": {\n",
    "            \"soc_r2_selected\": float(best[\"r2\"]),\n",
    "            \"soc_rmse_selected\": float(best[\"rmse\"]),\n",
    "            \"soh_r2_selected\": soh_best_r2,\n",
    "            \"soh_rmse_selected\": soh_best_rmse\n",
    "        },\n",
    "        \"soh_candidates_metrics\": {\n",
    "            \"gpr_raw\": {\"r2\": r2g, \"rmse\": rmseg},\n",
    "            \"hgb_raw\": {\"r2\": r2h2, \"rmse\": rmseh2},\n",
    "            \"gpr_shape\": shape_metrics\n",
    "        },\n",
    "        \"train_mahal\": {\"center\": center.tolist(), \"cov_inv\": cov_inv.tolist()},\n",
    "    }\n",
    "    out_path = cfg.MODEL_DIR/\"eis_soc_soh_cycles_models.joblib\"\n",
    "    joblib.dump(bundle, out_path)\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[MODEL] Saved bundle → {out_path}\")\n",
    "        print(json.dumps(bundle[\"metrics\"], indent=2))\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 12. LOAD\n",
    "# =========================\n",
    "def load_bundle():\n",
    "    path_new = cfg.MODEL_DIR / \"eis_soc_soh_cycles_models.joblib\"\n",
    "    if not path_new.exists():\n",
    "        raise FileNotFoundError(f\"Bundle not found: {path_new}\")\n",
    "    bundle = joblib.load(path_new)\n",
    "    for key in [\"soc_ensemble\",\"soh_model\",\"freq_grid\"]:\n",
    "        if key not in bundle:\n",
    "            raise KeyError(f\"Bundle missing required key: {key}\")\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 13. INFERENCE FEATURIZATION\n",
    "# =========================\n",
    "def featurize_any(file_path: Path, bundle):\n",
    "    freq_grid = bundle[\"freq_grid\"]\n",
    "    meta = parse_eis_metadata(file_path.stem)\n",
    "    freq,re_raw,im_raw, used_freq, first_last = load_any_inference(file_path)\n",
    "    if not used_freq:\n",
    "        warnings.warn(f\"[{file_path.name}] No frequency column found. Using geometric grid fallback.\")\n",
    "    re_i=_interp_channel(freq, re_raw, freq_grid)\n",
    "    im_i=_interp_channel(freq, im_raw, freq_grid)\n",
    "    if meta is None and cfg.TEST_TEMPERATURE_OVERRIDE is not None:\n",
    "        temp = cfg.TEST_TEMPERATURE_OVERRIDE\n",
    "    else:\n",
    "        temp = meta[\"Temp\"] if meta else -1\n",
    "    vec = build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "    norm_vec=None\n",
    "    if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL or cfg.SOC_INCLUDE_SHAPE_MODEL):\n",
    "        if cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "            # ---- BUGFIX: correct argument name (im_i), not 'imi'\n",
    "            rsh, ish = build_shape_normalized(re_i, im_i)\n",
    "            norm_vec = build_feature_vector(rsh, ish, temp, freq_grid)\n",
    "    checksum = hashlib.sha1(np.ascontiguousarray(vec).tobytes()).hexdigest()\n",
    "    return vec, norm_vec, meta, checksum, used_freq, first_last\n",
    "\n",
    "# =========================\n",
    "# 14. OOD UTILITIES (SoH)\n",
    "# =========================\n",
    "def mahalanobis_distance(x, center, cov_inv):\n",
    "    diff = x - center\n",
    "    return float(np.sqrt(diff @ cov_inv @ diff.T))\n",
    "\n",
    "def gp_ard_norm(Xp, model):\n",
    "    try:\n",
    "        K = model.kernel_\n",
    "        from sklearn.gaussian_process.kernels import RBF\n",
    "        rbf = None\n",
    "        if hasattr(K,\"k1\") and isinstance(K.k1,RBF): rbf=K.k1\n",
    "        elif hasattr(K,\"k2\") and isinstance(K.k2,RBF): rbf=K.k2\n",
    "        if rbf is None: return None\n",
    "        ls = np.atleast_1d(rbf.length_scale)\n",
    "        z = (Xp / ls).ravel()\n",
    "        return float(np.linalg.norm(z))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# 15. PROJECTION PLOT\n",
    "# =========================\n",
    "def _estimate_cpp_from_predictions(soh_current: float, cycles_to_map: Dict[float, float]) -> float:\n",
    "    usable = [(thr, c) for thr, c in cycles_to_map.items() if c and c > 0 and soh_current > thr]\n",
    "    if not usable:\n",
    "        return cfg.CPP_FALLBACK\n",
    "    thr, cyc = sorted(usable, key=lambda x: x[0])[0]\n",
    "    delta = max(1e-6, soh_current - float(thr))\n",
    "    return float(cyc / delta)\n",
    "\n",
    "def build_projection(soh_current, cpp, lower, exponent=None, n=160):\n",
    "    if soh_current <= lower or cpp <= 0:\n",
    "        return np.array([0.0]), np.array([soh_current])\n",
    "    total = (soh_current - lower) * cpp\n",
    "    cycles = np.linspace(0, total, n)\n",
    "    S0 = soh_current; Smin=lower\n",
    "    if exponent is None: exponent = cfg.PLOT_EXPONENT\n",
    "    soh_curve = Smin + (S0 - Smin)*(1 - cycles/total)**exponent\n",
    "    return cycles, soh_curve\n",
    "\n",
    "def plot_projection(file_base, soh_current, soh_std, cycles_to_map, cpp_hint, ood_flag, out_path, thresholds):\n",
    "    if not thresholds: thresholds = (50.0, 40.0)\n",
    "    min_thr = min(thresholds)\n",
    "    if soh_current <= min_thr:\n",
    "        return\n",
    "\n",
    "    cpp = _estimate_cpp_from_predictions(soh_current, cycles_to_map)\n",
    "    if not np.isfinite(cpp) or cpp <= 0:\n",
    "        cpp = cpp_hint if (cpp_hint and cpp_hint > 0) else cfg.CPP_FALLBACK\n",
    "\n",
    "    cycles, curve = build_projection(soh_current, cpp, min_thr)\n",
    "    plt.figure(figsize=(6.4,4))\n",
    "    plt.plot(cycles, curve, lw=2, label=\"Projected SoH (approx)\")\n",
    "\n",
    "    for thr in thresholds:\n",
    "        style = \"--\" if thr >= 50 else \":\"\n",
    "        color = \"orange\" if thr >= 50 else \"red\"\n",
    "        plt.axhline(thr, color=color, ls=style, label=f\"{int(thr)}%\")\n",
    "        x = float(cycles_to_map.get(thr, 0.0) or 0.0)\n",
    "        if x > 0:\n",
    "            plt.axvline(x, color=color, ls=\"-.\" if thr>=50 else \":\")\n",
    "            plt.scatter([x],[thr], s=45)\n",
    "            txty = thr + (1.0 if thr>=50 else -2.0)\n",
    "            plt.text(x, txty, f\"{x:.0f} cyc\", ha=\"center\", fontsize=8, color=color)\n",
    "\n",
    "    plt.scatter([0],[soh_current], c=\"green\", s=55, label=f\"Current {soh_current:.2f}%\")\n",
    "    plt.text(0, soh_current+0.7, f\"±{soh_std:.2f}\", color=\"green\", fontsize=8)\n",
    "\n",
    "    if ood_flag:\n",
    "        plt.text(0.98,0.05,\"OOD\", transform=plt.gca().transAxes,\n",
    "                 ha=\"right\", va=\"bottom\", color=\"crimson\", fontsize=11,\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"crimson\"))\n",
    "\n",
    "    plt.xlabel(\"Remaining Cycles\")\n",
    "    plt.ylabel(\"SoH (%)\")\n",
    "    plt.title(f\"RUL Projection – {file_base}\")\n",
    "    plt.grid(alpha=0.35)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 16. INFERENCE (single file)\n",
    "# =========================\n",
    "def predict_file(file_path: Path, bundle, cpp_map, global_cpp):\n",
    "    vec, norm_vec, meta, checksum, used_freq, first_last_freq = featurize_any(file_path, bundle)\n",
    "\n",
    "    # ----- SoC (temp-agnostic ensemble) -----\n",
    "    socb = bundle[\"soc_ensemble\"]\n",
    "    temp_idx = int(socb[\"temp_idx\"])\n",
    "\n",
    "    # SoC features (zero temperature)\n",
    "    vec_soc = vec.copy()\n",
    "    vec_soc[temp_idx] = 0.0\n",
    "    soc_scaler = socb[\"soc_scaler\"]\n",
    "    X_soc = soc_scaler.transform(vec_soc.reshape(1,-1))\n",
    "\n",
    "    means = []; stds = []\n",
    "\n",
    "    # GPR\n",
    "    gp = socb[\"gpr\"]\n",
    "    if isinstance(gp, GaussianProcessRegressor):\n",
    "        mu, sd = gp.predict(X_soc, return_std=True)\n",
    "        means.append(float(mu[0])); stds.append(float(sd[0]))\n",
    "    else:\n",
    "        means.append(float(gp.predict(X_soc)[0])); stds.append(float(socb[\"val_rmse\"]))\n",
    "\n",
    "    # HGB\n",
    "    hgb = socb[\"hgb\"]\n",
    "    means.append(float(hgb.predict(X_soc)[0])); stds.append(float(socb[\"val_rmse\"]))\n",
    "\n",
    "    # Shape GP (optional)\n",
    "    if socb.get(\"shape_model\") is not None and norm_vec is not None:\n",
    "        norm_vec_soc = norm_vec.copy()\n",
    "        norm_vec_soc[temp_idx] = 0.0\n",
    "        Xs = socb[\"shape_scaler\"].transform(norm_vec_soc.reshape(1,-1))\n",
    "        shp = socb[\"shape_model\"]\n",
    "        if isinstance(shp, GaussianProcessRegressor):\n",
    "            mu2, sd2 = shp.predict(Xs, return_std=True)\n",
    "            means.append(float(mu2[0])); stds.append(float(sd2[0]))\n",
    "        else:\n",
    "            means.append(float(shp.predict(Xs)[0])); stds.append(float(socb[\"val_rmse\"]))\n",
    "\n",
    "    w = np.array(socb[\"weights\"], dtype=float)\n",
    "    mu_mix = float(np.sum(w * np.array(means)))\n",
    "    var_mix = _ensemble_var(means, stds, w)\n",
    "    sd_mix = float(np.sqrt(var_mix)) * float(socb.get(\"std_alpha\", 1.0))\n",
    "\n",
    "    # Calibrate mean\n",
    "    cal = socb.get(\"calibrator\")\n",
    "    if cal is not None:\n",
    "        try:\n",
    "            mu_mix = float(np.clip(np.asarray(cal.predict([mu_mix]))[0], 0.0, 100.0))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # OOD detection in SoC space\n",
    "    soc_oob = False; soc_mahal = None\n",
    "    if socb.get(\"soc_center\") is not None and socb.get(\"soc_cov_inv\") is not None:\n",
    "        c = np.array(socb[\"soc_center\"]); inv = np.array(socb[\"soc_cov_inv\"])\n",
    "        diff = (X_soc[0] - c)\n",
    "        try:\n",
    "            soc_mahal = float(np.sqrt(diff @ inv @ diff.T))\n",
    "        except Exception:\n",
    "            soc_mahal = None\n",
    "        thr = socb.get(\"soc_mahal_thresh\", None)\n",
    "        if thr is not None:\n",
    "            soc_oob = (soc_mahal is not None and soc_mahal > float(thr))\n",
    "\n",
    "    # If no frequency column in file, force OOD fallback behavior\n",
    "    if not used_freq:\n",
    "        soc_oob = True\n",
    "\n",
    "    mu_final = mu_mix\n",
    "    sd_final = sd_mix\n",
    "\n",
    "    if cfg.OOD_SOC_ENABLE and soc_oob:\n",
    "        # Prior via KNN in SoC-space\n",
    "        Xtr = socb.get(\"soc_knn_X\", None)\n",
    "        ytr = socb.get(\"soc_knn_y\", None)\n",
    "        prior_val = cfg.OOD_SOC_PRIOR\n",
    "        if Xtr is not None and ytr is not None and Xtr.shape[0] >= 5:\n",
    "            d = np.linalg.norm(Xtr - X_soc[0], axis=1)\n",
    "            k = min(cfg.SOC_OOD_K, Xtr.shape[0])\n",
    "            idx = np.argpartition(d, k-1)[:k]\n",
    "            wdist = 1.0 / (d[idx] + 1e-6)\n",
    "            prior_val = float(np.sum(wdist * ytr[idx]) / np.sum(wdist))\n",
    "\n",
    "        thr = socb.get(\"soc_mahal_thresh\", np.inf)\n",
    "        delta = max(0.0, (soc_mahal or 0.0) - (thr if np.isfinite(thr) else 0.0))\n",
    "        s = max(1e-6, cfg.OOD_SOC_SHRINK_SCALE)\n",
    "        severity = float(max(0.0, min(1.0, delta/(s*6.0))))\n",
    "\n",
    "        w_prior = min(float(cfg.OOD_SOC_PRIOR_MAX_WEIGHT), 0.15 + 0.85*severity)\n",
    "        w_base  = 1.0 - w_prior\n",
    "        mu_final = float(w_base*mu_mix + w_prior*prior_val)\n",
    "\n",
    "        # Slightly inflate uncertainty when OOD\n",
    "        sd_final = float(min(max(sd_mix*(1.0 + 1.25*severity), sd_mix), 10.0))\n",
    "\n",
    "    soc_mean = float(np.clip(mu_final, 0.0, 100.0))\n",
    "    soc_std  = float(min(sd_final, 10.0))  # allow real variation\n",
    "\n",
    "    # ----- SoH -----\n",
    "    scaler = bundle[\"shared_scaler\"]\n",
    "    X = scaler.transform(vec.reshape(1,-1))\n",
    "    soh_model=bundle[\"soh_model\"]; model_name=bundle.get(\"soh_model_name\",\"unknown\")\n",
    "    if isinstance(soh_model, GaussianProcessRegressor):\n",
    "        sm, ss = soh_model.predict(X, return_std=True)\n",
    "        soh_mean_raw = float(sm[0]); soh_std_raw=float(ss[0])\n",
    "    else:\n",
    "        soh_mean_raw = float(soh_model.predict(X)[0])\n",
    "        soh_std_raw  = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    shape_model = bundle.get(\"shape_model\"); shape_scaler = bundle.get(\"shape_scaler\")\n",
    "    shape_soh_mean=None; shape_soh_std=None\n",
    "    if shape_model is not None and norm_vec is not None:\n",
    "        X_shape_s = shape_scaler.transform(norm_vec.reshape(1,-1))\n",
    "        if isinstance(shape_model, GaussianProcessRegressor):\n",
    "            sm2, ss2 = shape_model.predict(X_shape_s, return_std=True)\n",
    "            shape_soh_mean=float(sm2[0]); shape_soh_std=float(ss2[0])\n",
    "        else:\n",
    "            shape_soh_mean=float(shape_model.predict(X_shape_s)[0])\n",
    "            shape_soh_std=float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    if cfg.ENSEMBLE_SOH and shape_soh_mean is not None:\n",
    "        soh_mean = 0.5*(soh_mean_raw + shape_soh_mean)\n",
    "        stds = [soh_std_raw]\n",
    "        if shape_soh_std is not None: stds.append(shape_soh_std)\n",
    "        soh_std = float(np.sqrt(np.mean(np.array(stds)**2)))\n",
    "    else:\n",
    "        soh_mean, soh_std = soh_mean_raw, soh_std_raw\n",
    "\n",
    "    train_mahal = bundle.get(\"train_mahal\")\n",
    "    mahal_dist=None\n",
    "    if train_mahal:\n",
    "        cov_inv = np.array(train_mahal[\"cov_inv\"])\n",
    "        center = np.array(train_mahal[\"center\"])\n",
    "        mahal_dist = mahalanobis_distance(X[0], center, cov_inv)\n",
    "    ard_norm=None\n",
    "    if \"gpr\" in model_name:\n",
    "        ard_norm = gp_ard_norm(X, soh_model)\n",
    "    ood_flag=False\n",
    "    if (mahal_dist is not None and mahal_dist > cfg.MAHAL_THRESHOLD) or \\\n",
    "       (ard_norm is not None and ard_norm > cfg.GP_ARD_NORM_THRESHOLD):\n",
    "        ood_flag=True\n",
    "\n",
    "    soh_val_rmse = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "    if ood_flag:\n",
    "        soh_std = min(soh_std, cfg.SOH_STD_MAX_OOD)\n",
    "    else:\n",
    "        soh_std = min(soh_std, soh_val_rmse)\n",
    "\n",
    "    # ----- Cycles predictions (learned; already scaled) -----\n",
    "    cycles_models = bundle.get(\"cycles_models\", {}) or {}\n",
    "    cycles_to = {}\n",
    "    cycles_abs = None\n",
    "    if \"absolute\" in cycles_models:\n",
    "        m = cycles_models[\"absolute\"][\"model\"]\n",
    "        cyc_pred = float(max(0.0, m.predict(X)[0]))\n",
    "        cycles_abs = cyc_pred\n",
    "    for thr in cfg.TARGET_SOH_THRESHOLDS:\n",
    "        key = str(int(thr))\n",
    "        if key in cycles_models:\n",
    "            m = cycles_models[key][\"model\"]\n",
    "            rem = float(max(0.0, m.predict(X)[0]))\n",
    "            cycles_to[thr] = rem\n",
    "        else:\n",
    "            cycles_to[thr] = 0.0\n",
    "\n",
    "    # Fallback if needed\n",
    "    used_cpp = None\n",
    "    if not any(v > 0 for v in cycles_to.values()):\n",
    "        cpp = get_cpp(meta, cpp_map, global_cpp)\n",
    "        used_cpp = float(cpp)\n",
    "        for thr_val in cfg.TARGET_SOH_THRESHOLDS:\n",
    "            cycles_to[thr_val] = float((soh_mean - thr_val) * cpp) if soh_mean > thr_val else 0.0\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoH] {Path(file_path).name}: mean={soh_mean:.2f} std={soh_std:.2f}  OOD(SoH)={bool(ood_flag)}\")\n",
    "        if cfg.OOD_SOC_ENABLE:\n",
    "            print(f\"[SoC] {Path(file_path).name}: base={mu_mix:.2f}  → final={soc_mean:.2f}  OOD={soc_oob}\")\n",
    "\n",
    "    result={\n",
    "        \"file\": str(file_path),\n",
    "        \"feature_checksum\": checksum,\n",
    "        \"parsed_metadata\": meta,\n",
    "        # SoC\n",
    "        \"predicted_SoC_percent\": float(soc_mean),\n",
    "        \"SoC_std_estimate\": float(soc_std),\n",
    "        \"soc_model_chosen\": \"ensemble_\" + \"+\".join(socb[\"names\"]),\n",
    "        \"soc_model_kind\": \"ensemble\",\n",
    "        \"soc_calibration_kind\": socb.get(\"cal_kind\", \"identity\"),\n",
    "        # SoH\n",
    "        \"predicted_SoH_percent\": float(soh_mean),\n",
    "        \"SoH_std_estimate\": float(soh_std),\n",
    "        \"soh_model_chosen\": model_name,\n",
    "        # Cycles\n",
    "        \"predicted_cycle_index\": None if cycles_abs is None else float(cycles_abs),\n",
    "        \"predicted_cycles_remaining_to_thresholds\": {str(int(k)): float(v) for k,v in cycles_to.items()},\n",
    "        \"cycles_per_percent_est\": None if not any(v>0 for v in cycles_to.values()) else float(_estimate_cpp_from_predictions(soh_mean, cycles_to)),\n",
    "        \"fallback_cpp_used\": None if used_cpp is None else float(used_cpp),\n",
    "        # Debug\n",
    "        \"used_freq_from_file\": bool(used_freq),\n",
    "        \"freq_range_hz\": {\n",
    "            \"first\": None if first_last_freq[0] is None else float(first_last_freq[0]),\n",
    "            \"last\":  None if first_last_freq[1] is None else float(first_last_freq[1]),\n",
    "        },\n",
    "        # thresholds\n",
    "        \"decision_threshold_percent\": cfg.DECISION_SOH_PERCENT,\n",
    "        \"lower_threshold_percent\": cfg.ILLUSTRATIVE_MIN_SOH,\n",
    "        # OOD\n",
    "        \"OOD_flag\": bool(ood_flag)\n",
    "    }\n",
    "    return result, ood_flag, {float(k): float(v) for k,v in cycles_to.items()}\n",
    "\n",
    "# =========================\n",
    "# 17. MAIN (batch mode)\n",
    "# =========================\n",
    "def main():\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps(to_jsonable(asdict(cfg)), indent=2))\n",
    "\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR)\n",
    "\n",
    "    # compute global cycle scale used in this run\n",
    "    global CYCLE_SCALE_GLOBAL\n",
    "    CYCLE_SCALE_GLOBAL = float(cfg.CYCLE_SCALE)\n",
    "    if not cap_df.empty and cfg.TARGET_CALIB_CYCLE_AT_80 is not None:\n",
    "        auto = _calibrate_cycle_scale(cap_df, cfg.TARGET_CALIB_CYCLE_AT_80)\n",
    "        CYCLE_SCALE_GLOBAL *= float(auto)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[CYCLE-SCALE] auto={auto:.3f}  (target 80% at {cfg.TARGET_CALIB_CYCLE_AT_80})  → total scale={CYCLE_SCALE_GLOBAL:.3f}\")\n",
    "\n",
    "    # CPP map (scaled) for fallback and plotting\n",
    "    if cap_df.empty:\n",
    "        if cfg.VERBOSE: print(\"[INFO] No / empty capacity data.\")\n",
    "        cpp_map, global_cpp = {}, float(cfg.CPP_FALLBACK)\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "    cpp_map = {k: v*CYCLE_SCALE_GLOBAL for k,v in cpp_map.items()}\n",
    "    global_cpp *= CYCLE_SCALE_GLOBAL\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[CPP] dynamic cells={len(cpp_map)} global_cpp_median={global_cpp:.2f} (scaled)\")\n",
    "\n",
    "    # ---- Load or retrain\n",
    "    bundle_path_new = cfg.MODEL_DIR / \"eis_soc_soh_cycles_models.joblib\"\n",
    "    need_retrain = bool(cfg.FORCE_RETRAIN) or (not bundle_path_new.exists())\n",
    "    bundle = None\n",
    "\n",
    "    if not need_retrain:\n",
    "        try:\n",
    "            bundle = load_bundle()\n",
    "            same_sig = (bundle.get(\"config_signature\") == config_signature(cfg)) and \\\n",
    "                       (bundle.get(\"feature_version\") == cfg.FEATURE_VERSION)\n",
    "            need_retrain = not same_sig\n",
    "            if cfg.VERBOSE:\n",
    "                print(f\"[LOAD] Found bundle. Signature match: {same_sig}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD] Could not load existing bundle cleanly: {e}\")\n",
    "            need_retrain = True\n",
    "\n",
    "    if need_retrain:\n",
    "        if not cfg.EIS_DIR.exists():\n",
    "            raise FileNotFoundError(f\"EIS_DIR missing: {cfg.EIS_DIR}.\")\n",
    "        if cfg.REFINE_SOH_WITH_CAPACITY and not cfg.CAP_DIR.exists():\n",
    "            print(f\"[WARN] CAP_DIR missing: {cfg.CAP_DIR}. Proceeding without capacity refinement.\")\n",
    "            cfg.REFINE_SOH_WITH_CAPACITY = False\n",
    "            cap_df = pd.DataFrame()\n",
    "        if cfg.VERBOSE: print(\"[TRAIN] Building dataset & training models...\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict = build_dataset(cfg.EIS_DIR, cap_df, cycle_scale=CYCLE_SCALE_GLOBAL)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[TRAIN] Samples={X_raw.shape[0]} Features={X_raw.shape[1]} Cells={meta_df.CellID.nunique()}\")\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh, y_cycle_index, y_rem_dict)\n",
    "    else:\n",
    "        if bundle is None:\n",
    "            bundle = load_bundle()\n",
    "\n",
    "    # ---- Inference (batch)\n",
    "    artifacts = cfg.MODEL_DIR / \"artifacts\"\n",
    "    artifacts.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for test_fp in cfg.EIS_TEST_FILES:\n",
    "        print(f\"\\n===== TEST: {Path(test_fp).name} =====\")\n",
    "        if not Path(test_fp).exists():\n",
    "            print(f\"[WARN] Test file not found: {test_fp}\")\n",
    "            continue\n",
    "        try:\n",
    "            result, ood_flag, cycles_to_map = predict_file(Path(test_fp), bundle, cpp_map, global_cpp)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Prediction failed for {Path(test_fp).name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        out_plot = artifacts / f\"{Path(test_fp).stem}_projection.png\"\n",
    "        cpp_hint = result.get(\"fallback_cpp_used\", None)\n",
    "        plot_projection(\n",
    "            Path(test_fp).stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            {float(k): float(v) for k,v in result[\"predicted_cycles_remaining_to_thresholds\"].items()},\n",
    "            cpp_hint,\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot,\n",
    "            thresholds=cfg.TARGET_SOH_THRESHOLDS\n",
    "        )\n",
    "\n",
    "        out_json = artifacts / f\"{Path(test_fp).stem}_prediction.json\"\n",
    "        with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(f\"[PLOT] Saved: {out_plot}\")\n",
    "        print(f\"[JSON] Saved: {out_json}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "# =========================\n",
    "# 18. ENTRYPOINT\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ui\", action=\"store_true\", help=\"(UI removed in this trimmed script)\")\n",
    "    parser.add_argument(\"--share\", action=\"store_true\", help=\"(no-op)\")\n",
    "    parser.add_argument(\"--host\", default=\"127.0.0.1\", help=\"Server host (default: 127.0.0.1)\")\n",
    "    parser.add_argument(\"--port\", type=int, default=7860, help=\"Server port (default: 7860)\")\n",
    "    parser.add_argument(\"--inbrowser\", action=\"store_true\", help=\"(no-op)\")\n",
    "    # NEW: paths + toggles\n",
    "    parser.add_argument(\"--eis_dir\", type=str, default=None, help=\"Path to training EIS .mat directory\")\n",
    "    parser.add_argument(\"--cap_dir\", type=str, default=None, help=\"Path to capacity .mat directory\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=None, help=\"Path to save/load models and artifacts\")\n",
    "    parser.add_argument(\"--no-capacity\", action=\"store_true\", help=\"Disable capacity refinement\")\n",
    "    parser.add_argument(\"--force-retrain\", action=\"store_true\", help=\"Force retrain even if a bundle exists/matches\")\n",
    "    # Cycle scale override\n",
    "    parser.add_argument(\"--cycle-scale\", type=float, default=None, help=\"Override CYCLE_SCALE (global multiplier)\")\n",
    "    parser.add_argument(\"--target-80\", type=float, default=None, help=\"Override TARGET_CALIB_CYCLE_AT_80 (auto-scale)\")\n",
    "    # SoC toggles\n",
    "    parser.add_argument(\"--soc-ood\", action=\"store_true\", help=\"Enable SoC OOD blending (ON by default here)\")\n",
    "    parser.add_argument(\"--soc-cal\", type=str, default=None, help=\"SoC calibration mode: auto|iso|linear|off\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.eis_dir or args.cap_dir or args.model_dir:\n",
    "        set_paths(\n",
    "            args.eis_dir if args.eis_dir else cfg.EIS_DIR,\n",
    "            args.cap_dir if args.cap_dir else cfg.CAP_DIR,\n",
    "            args.model_dir if args.model_dir else cfg.MODEL_DIR,\n",
    "        )\n",
    "    if args.no_capacity:\n",
    "        cfg.REFINE_SOH_WITH_CAPACITY = False\n",
    "    if args.force_retrain:\n",
    "        cfg.FORCE_RETRAIN = True\n",
    "    if args.cycle_scale is not None:\n",
    "        cfg.CYCLE_SCALE = float(args.cycle_scale)\n",
    "    if args.target_80 is not None:\n",
    "        cfg.TARGET_CALIB_CYCLE_AT_80 = float(args.target_80)\n",
    "    if args.soc_ood:\n",
    "        cfg.OOD_SOC_ENABLE = True\n",
    "    if args.soc_cal is not None:\n",
    "        cfg.SOC_CALIBRATION_MODE = str(args.soc_cal).lower()\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e7bbf-b67e-4ad0-8858-683c3b1245c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f7d87-a3a1-424b-964d-20c043558edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15c08d-b11d-4857-8837-6ac9853477f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
