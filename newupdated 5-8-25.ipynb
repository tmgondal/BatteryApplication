{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533f3ad1-50da-4863-aca8-d0b879b68b07",
   "metadata": {},
   "source": [
    "┌─[1] Config & constants  → cfg\n",
    "\n",
    "│\n",
    "├─[2] Low-level loaders (MAT, CSV/XLSX, interpolation)\n",
    "\n",
    "│\n",
    "├─[3] Feature-engineering helpers\n",
    "│     • raw Re/Im          • “F” heuristics\n",
    "│     • physical features  • band statistics\n",
    "│     • diff-segment slopes• DRT features  ←★\n",
    "│\n",
    "\n",
    "├─[4] Capacity parser  → cycles-per-percent (CPP) map\n",
    "│\n",
    "\n",
    "├─[5] Dataset builder  → X_raw, y_SoC, y_SoH (+ optional shape model)\n",
    "│\n",
    "\n",
    "├─[6] Model trainer\n",
    "│     • RandomForest        → SoC (classification)\n",
    "│     • GPR / HGB ensemble  → SoH (regression)\n",
    "│     • optional shape-normalised GPR for SoH\n",
    "│\n",
    "├─[7] Bundle saver/loader (.joblib)\n",
    "│\n",
    "├─[8] featurize_any()       ← handles *any* test file ext.\n",
    "│\n",
    "├─[9] predict_file()        ← single-spectrum inference\n",
    "│\n",
    "└─[10] main()\n",
    "       ↳ build/load bundle → predict test_fp\n",
    "       ↳ save JSON + RUL projection plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09526afd-c48e-41d1-83e7-9f45953f4cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"EIS_TEST_FILE\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\TestFile\\\\Mazda-Battery-Cell5.xlsx\",\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"USE_PCA_SOC\": true,\n",
      "  \"USE_PCA_SOH\": false,\n",
      "  \"PCA_SOC_COMPONENTS\": 25,\n",
      "  \"PCA_SOH_COMPONENTS\": 30,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"INCLUDE_NORMALIZED_SHAPE_MODEL\": true,\n",
      "  \"ENSEMBLE_SOH\": true,\n",
      "  \"NORMALIZE_SHAPE_BY_HF_RE\": true,\n",
      "  \"DECISION_SOH_PERCENT\": 50.0,\n",
      "  \"ILLUSTRATIVE_MIN_SOH\": 40.0,\n",
      "  \"CPP_ROLLING_WINDOW\": 5,\n",
      "  \"CPP_MIN_POINTS\": 6,\n",
      "  \"CPP_FALLBACK\": 20.0,\n",
      "  \"TEST_TEMPERATURE_OVERRIDE\": 25.0,\n",
      "  \"FORCE_RETRAIN\": false,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FEATURE_VERSION\": 8,\n",
      "  \"MAHAL_THRESHOLD\": 10.0,\n",
      "  \"GP_ARD_NORM_THRESHOLD\": 6.0,\n",
      "  \"PLOT_EXPONENT\": 1.25\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator GaussianProcessRegressor from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Using bundle → models_eis_phase2_phys\\eis_soc_soh_phys_models.joblib\n",
      "{\n",
      "  \"file\": \"C:\\\\Users\\\\tgondal0\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\TestFile\\\\Mazda-Battery-Cell5.xlsx\",\n",
      "  \"parsed_metadata\": null,\n",
      "  \"predicted_SoC\": 5,\n",
      "  \"SoC_probabilities\": {\n",
      "    \"5\": 0.4346845238095238,\n",
      "    \"20\": 0.17007440476190472,\n",
      "    \"50\": 0.14090327380952378,\n",
      "    \"70\": 0.1471875,\n",
      "    \"95\": 0.10715029761904762\n",
      "  },\n",
      "  \"predicted_SoH_percent\": 90.34750000003794,\n",
      "  \"SoH_std_estimate\": 5.0,\n",
      "  \"cycles_per_percent_used\": 20.0,\n",
      "  \"cycles_to_target\": 806.9500000007588,\n",
      "  \"cycles_to_lower\": 1006.9500000007588,\n",
      "  \"decision_threshold_percent\": 50.0,\n",
      "  \"lower_threshold_percent\": 40.0,\n",
      "  \"feature_version\": 9,\n",
      "  \"soh_model_chosen\": \"gpr_raw\"\n",
      "}\n",
      "[PLOT]  models_eis_phase2_phys\\Mazda-Battery-Cell5_projection.png\n",
      "[JSON]  models_eis_phase2_phys\\Mazda-Battery-Cell5_prediction.json\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Unified EIS Training + Inference + Dynamic RUL  (v8 – single file)\n",
    "# ================================================================\n",
    "#   • Accepts ONE EIS test file (cfg.EIS_TEST_FILE or --test path)\n",
    "#   • Caps SoH uncertainty to ±5 percentage-points\n",
    "#   • Outputs the single most-likely SoC class\n",
    "#   • Back-compatible with legacy model bundles\n",
    "# ================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, argparse, json, math, random, re, warnings, joblib\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIGURATION\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- local folders -------------------------------------------------\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "    EIS_TEST_FILE: Path = Path(r\"C:\\Users\\tgondal0\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\TestFile\\Mazda-Battery-Cell5.xlsx\")\n",
    "\n",
    "    # --- spectrum grid -------------------------------------------------\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int  = 60\n",
    "\n",
    "    # --- split & random ------------------------------------------------\n",
    "    TEST_FRAC: float = 0.2\n",
    "\n",
    "    # --- PCA flags -----------------------------------------------------\n",
    "    USE_PCA_SOC: bool = True\n",
    "    USE_PCA_SOH: bool = False\n",
    "    PCA_SOC_COMPONENTS: int = 25\n",
    "    PCA_SOH_COMPONENTS: int = 30\n",
    "\n",
    "    # --- feature toggles ----------------------------------------------\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    # --- DRT params ----------------------------------------------------\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    # --- SoH / RUL -----------------------------------------------------\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    INCLUDE_NORMALIZED_SHAPE_MODEL: bool = True\n",
    "    ENSEMBLE_SOH: bool = True\n",
    "    NORMALIZE_SHAPE_BY_HF_RE: bool = True\n",
    "    DECISION_SOH_PERCENT: float = 50.0\n",
    "    ILLUSTRATIVE_MIN_SOH: float = 40.0\n",
    "    CPP_ROLLING_WINDOW: int = 5\n",
    "    CPP_MIN_POINTS: int = 6\n",
    "    CPP_FALLBACK: float = 20.0\n",
    "\n",
    "    # --- extras --------------------------------------------------------\n",
    "    TEST_TEMPERATURE_OVERRIDE: Optional[float] = 25.0\n",
    "    FORCE_RETRAIN: bool = False\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    VERBOSE: bool = True\n",
    "    FEATURE_VERSION: int = 8           # ← bumped\n",
    "    MAHAL_THRESHOLD: float = 10.0\n",
    "    GP_ARD_NORM_THRESHOLD: float = 6.0\n",
    "    PLOT_EXPONENT: float = 1.25\n",
    "\n",
    "cfg = Config()\n",
    "cfg.FORCE_RETRAIN = False\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 2. UTILITIES\n",
    "# =========================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path): return str(x)\n",
    "    if isinstance(x, dict): return {k: to_jsonable(v) for k, v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "# =========================\n",
    "# 3. REGEX & METADATA PARSERS\n",
    "# =========================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "def parse_eis_metadata(stem: str)->Optional[Dict[str,Any]]:\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\"CellID\":f\"Cell{d['CellID']}\", \"SOH_stage\":int(d[\"SOH\"]),\n",
    "            \"SOC\":int(d[\"SOC\"]), \"Temp\":int(d[\"Temp\"]),\n",
    "            \"RealSOH_file\":int(d[\"RealSOH\"])/100.0}\n",
    "\n",
    "def parse_cap_metadata(stem:str)->Optional[Dict[str,Any]]:\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d=m.groupdict()\n",
    "    return {\"CellID\":f\"Cell{d['CellID']}\", \"SOH_stage\":int(d[\"SOH\"]),\n",
    "            \"Temp\":int(d[\"Temp\"]), \"CycleIndex\":int(d[\"Cycle\"])}\n",
    "\n",
    "# =========================\n",
    "# 4. LOW-LEVEL LOADERS\n",
    "# =========================\n",
    "def _find_matrix(mat_dict:dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v,np.ndarray) and v.ndim==2 and v.shape[1]>=3 and v.shape[0]>=10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw,y_raw,freq_target):\n",
    "    freq_raw=np.asarray(freq_raw,float); y_raw=np.asarray(y_raw,float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw=freq_raw[::-1]; y_raw=y_raw[::-1]\n",
    "    uniq,idx=np.unique(freq_raw,return_index=True)\n",
    "    if len(uniq)!=len(freq_raw):\n",
    "        order=np.argsort(idx); freq_raw=uniq[order]; y_raw=y_raw[idx][order]\n",
    "    f=interp1d(freq_raw,y_raw,bounds_error=False,\n",
    "               fill_value=(y_raw[0],y_raw[-1]),kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "FREQ_CANDS=[\"frequency\",\"freq\",\"f\",\"hz\",\"frequency(hz)\",\"Frequency(Hz)\"]\n",
    "RE_CANDS  =[\"zreal\",\"re(z)\",\"re\",\"real\",\"z_re\",\"zreal(ohm)\",\"Re (ohm)\",\"re(z) (ohm)\",\"Zreal\",\"Zreal (ohm)\"]\n",
    "IM_CANDS  =[\"-zimag\",\"zimag\",\"im(z)\",\"im\",\"imag\",\"imaginary\",\"z_im\",\"zimg\",\" -Zimag (ohm)\",\"-Zimag\",\"Zimag\",\"Zimag (ohm)\"]\n",
    "\n",
    "def _select_column(df:pd.DataFrame,cands:List[str])->Optional[str]:\n",
    "    low={c.lower():c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c.lower() in low: return low[c.lower()]\n",
    "    for c in cands:\n",
    "        for col in df.columns:\n",
    "            if c.lower() in col.lower(): return col\n",
    "    return None\n",
    "\n",
    "def load_mat_eis(path:Path):\n",
    "    mat=loadmat(path); arr=_find_matrix(mat)\n",
    "    if arr is None: raise ValueError(f\"No EIS matrix in {path.name}\")\n",
    "    return arr[:,0].astype(float),arr[:,1].astype(float),arr[:,2].astype(float)\n",
    "\n",
    "def load_table_eis(path:Path):\n",
    "    df=pd.read_csv(path) if path.suffix.lower()==\".csv\" else pd.read_excel(path)\n",
    "    if df.empty: raise ValueError(\"Empty table\")\n",
    "    fcol=_select_column(df,FREQ_CANDS); recol=_select_column(df,RE_CANDS); imcol=_select_column(df,IM_CANDS)\n",
    "    if recol is None or imcol is None: raise ValueError(f\"Missing Re/Im columns in {path.name}\")\n",
    "    re_vals=pd.to_numeric(df[recol],errors=\"coerce\").to_numpy()\n",
    "    im_vals=pd.to_numeric(df[imcol],errors=\"coerce\").to_numpy()\n",
    "    freq_vals=pd.to_numeric(df[fcol],errors=\"coerce\").to_numpy() if fcol else \\\n",
    "               np.geomspace(cfg.F_MAX,cfg.F_MIN,min(len(re_vals),len(im_vals)))\n",
    "    n=min(len(freq_vals),len(re_vals),len(im_vals))\n",
    "    freq_vals,re_vals,im_vals=freq_vals[:n],re_vals[:n],im_vals[:n]\n",
    "    if np.nanmean(im_vals)>0: im_vals=-im_vals\n",
    "    return freq_vals,re_vals.astype(float),im_vals.astype(float)\n",
    "\n",
    "def load_any_inference(path:Path):\n",
    "    suf=path.suffix.lower()\n",
    "    if suf==\".mat\": return load_mat_eis(path)\n",
    "    if suf in(\".csv\",\".xls\",\".xlsx\"): return load_table_eis(path)\n",
    "    raise ValueError(f\"Unsupported test file ext: {suf}\")\n",
    "\n",
    "# =========================\n",
    "# 5. FEATURE ENGINEERING\n",
    "# =========================\n",
    "def compute_F_features(freq,re_i,im_i):\n",
    "    neg_im=-im_i; idx_peak=int(np.argmax(neg_im))\n",
    "    F1,F2,F3=re_i[0],re_i[idx_peak],re_i[-1]\n",
    "    sc=np.where(np.sign(im_i[:-1])!=np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k=sc[0]; y0,y1=im_i[k],im_i[k+1]; w=-y0/(y1-y0+1e-12)\n",
    "        F4=re_i[k]+w*(re_i[k+1]-re_i[k])\n",
    "    else: F4=np.nan\n",
    "    F5=(re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6=np.min(im_i)\n",
    "    idx_mid=int(np.argmin(np.abs(freq-10.0))); F7=re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES=[\"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "                        \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"]\n",
    "\n",
    "def physical_features(freq,re_i,im_i):\n",
    "    neg_im=-im_i; idx_peak=int(np.argmax(neg_im))\n",
    "    Rs=float(re_i[0]); Rpeak=float(re_i[idx_peak]); Rlow=float(re_i[-1])\n",
    "    Rct=max(Rpeak-Rs,0.0); arc_diam=Rlow-Rs; norm_arc=arc_diam/(Rs+1e-9)\n",
    "    f_peak=float(freq[idx_peak]); tau_peak=1.0/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K=min(10,len(freq)//3)\n",
    "    warburg_sigma=np.nan\n",
    "    if K>=4:\n",
    "        w_section=(2*np.pi*freq[-K:])**(-0.5); re_section=re_i[-K:]\n",
    "        if len(np.unique(w_section))>2: warburg_sigma=float(np.polyfit(w_section,re_section,1)[0])\n",
    "    phase=np.arctan2(-im_i,re_i); mid_mask=(freq>=1)&(freq<=100)\n",
    "    phase_mean_mid=float(phase[mid_mask].mean()) if mid_mask.sum()>2 else np.nan\n",
    "    phase_std_mid=float(phase[mid_mask].std())  if mid_mask.sum()>2 else np.nan\n",
    "    phase_min=float(phase.min())\n",
    "    lf_mask=(freq<=1.0); lf_slope=np.nan\n",
    "    if lf_mask.sum()>=4:\n",
    "        x=np.log10(freq[lf_mask]+1e-12); y=neg_im[lf_mask]; lf_slope=np.polyfit(x,y,1)[0]\n",
    "    arc_quality=(neg_im.max()-neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS=[(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq,re_i,im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m],im_i[m]); feats+=[z.mean(),z.std()]\n",
    "        else: feats+=[np.nan,np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq,re_i,im_i,segments=5):\n",
    "    logf=np.log10(freq); edges=np.linspace(logf.min(),logf.max(),segments+1); out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]; out+=[np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else: out+=[np.nan,np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES=[\"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "                   \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w=2*np.pi*freq; tau=np.geomspace(tau_max,tau_min,n_tau); WT=w[:,None]*tau[None,:]\n",
    "    K_re=1.0/(1+WT**2); K_im=-WT/(1+WT**2)\n",
    "    R_inf=re_i[0]; y_re=re_i-R_inf; y_im=im_i\n",
    "    Y=np.concatenate([y_re,y_im]); K=np.vstack([K_re,K_im])\n",
    "    A=K.T@K + lam*np.eye(n_tau); b=K.T@Y\n",
    "    gamma=linalg.solve(A,b,assume_a='pos'); return tau,np.clip(gamma,0,None)\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    try:\n",
    "        tau,gamma=compute_drt(freq,re_i,im_i,\n",
    "                              cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau=np.log10(tau); g_sum=gamma.sum()+1e-12; w_norm=gamma/g_sum\n",
    "        mean_logtau=float((w_norm*log_tau).sum())\n",
    "        var_logtau=float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p=int(np.argmax(gamma)); peak_tau=float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid=np.median(log_tau); frac_low=float(w_norm[log_tau<=mid].sum()); frac_high=1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception: return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i,im_i,temp,freq,include_names=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts+=[re_i,im_i]\n",
    "        names+=[f\"Re_{i}\" for i in range(len(re_i))]+[f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z=np.hypot(re_i,im_i)\n",
    "        basics=[re_i[0],re_i[-1],re_i[-1]-re_i[0],z.max(),z.mean(),z.std()]\n",
    "        parts.append(np.array(basics)); names+=[\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        parts.append(np.array(compute_F_features(freq,re_i,im_i))); names+=[f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        parts.append(np.array(physical_features(freq,re_i,im_i))); names+=PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        parts.append(np.array(band_stats(freq,re_i,im_i)))\n",
    "        for i in range(len(BANDS)): names+=[f\"band{i}_mean\",f\"band{i}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i); parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2): names+=[f\"slope_re_seg{i}\",f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        parts.append(np.array(drt_features(freq,re_i,im_i))); names+=DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names+=[\"Temp_feat\"]\n",
    "    vec=np.concatenate(parts).astype(float); vec=np.nan_to_num(vec,0,0,0)\n",
    "    return (vec,names) if include_names else vec\n",
    "\n",
    "def build_shape_normalized(re_i,im_i):\n",
    "    hf=re_i[0] if re_i[0]!=0 else 1.0\n",
    "    return re_i/hf, im_i/hf\n",
    "\n",
    "# =========================\n",
    "# 6. CAPACITY → CPP\n",
    "# =========================\n",
    "def load_capacity_info(cap_dir:Path)->pd.DataFrame:\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY): return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta=parse_cap_metadata(fp.stem); \n",
    "        if not meta: continue\n",
    "        try:\n",
    "            mat=loadmat(fp); arr=_find_matrix(mat); \n",
    "            if arr is None: continue\n",
    "            col=np.argmax(np.abs(arr[-50:,:]).mean(axis=0)); cap=float(np.nanmax(arr[:,col]))\n",
    "            meta[\"MeasuredCapacity_Ah\"]=cap; recs.append(meta)\n",
    "        except Exception: pass\n",
    "    df=pd.DataFrame(recs)\n",
    "    if df.empty: return df\n",
    "    ref=df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"]=df[\"MeasuredCapacity_Ah\"]/ref; df[\"SoH_percent\"]=df[\"NormCapacity\"]*100.0\n",
    "    return df\n",
    "\n",
    "def estimate_cpp_per_cell(cap_df:pd.DataFrame,window:int,min_pts:int)->Dict[str,float]:\n",
    "    cpp={}\n",
    "    for cid,grp in cap_df.groupby(\"CellID\"):\n",
    "        g=grp.sort_values(\"CycleIndex\")\n",
    "        if g.shape[0]<min_pts: continue\n",
    "        tail=g.tail(window); x=tail[\"CycleIndex\"].values; y=tail[\"SoH_percent\"].values\n",
    "        if len(np.unique(x))<2: continue\n",
    "        slope=np.polyfit(x,y,1)[0]\n",
    "        if slope>=-1e-6: continue\n",
    "        cpp[cid]=1.0/abs(slope)\n",
    "    return cpp\n",
    "\n",
    "def build_cpp_map(df:pd.DataFrame):\n",
    "    if df.empty: return {},cfg.CPP_FALLBACK\n",
    "    cpp_map=estimate_cpp_per_cell(df[[\"CellID\",\"CycleIndex\",\"SoH_percent\"]],\n",
    "                                  cfg.CPP_ROLLING_WINDOW,cfg.CPP_MIN_POINTS)\n",
    "    return (cpp_map,float(np.median(list(cpp_map.values())) if cpp_map else cfg.CPP_FALLBACK))\n",
    "\n",
    "def get_cpp(meta:dict,cpp_map:Dict[str,float],global_cpp:float)->float:\n",
    "    if not meta: return global_cpp\n",
    "    return cpp_map.get(meta.get(\"CellID\"),global_cpp)\n",
    "\n",
    "# =========================\n",
    "# 7. DATASET BUILD & TRAIN\n",
    "# =========================\n",
    "def load_single_eis_mat(fp:Path):\n",
    "    meta=parse_eis_metadata(fp.stem); \n",
    "    if meta is None: raise ValueError(f\"Bad filename {fp.name}\")\n",
    "    f,r,i=load_mat_eis(fp); re_i=_interp_channel(f,r,CANON_FREQ); im_i=_interp_channel(f,i,CANON_FREQ)\n",
    "    vec=build_feature_vector(re_i,im_i,meta[\"Temp\"],CANON_FREQ)\n",
    "    return vec,meta,re_i,im_i\n",
    "\n",
    "def build_dataset(eis_dir:Path,cap_df:Optional[pd.DataFrame]):\n",
    "    files=sorted(eis_dir.rglob(\"*.mat\")); \n",
    "    if not files: raise FileNotFoundError(\"No .mat spectra in training dir\")\n",
    "    f0,r0,i0=load_mat_eis(files[0]); re0=_interp_channel(f0,r0,CANON_FREQ); im0=_interp_channel(f0,i0,CANON_FREQ)\n",
    "    _,feat_names=build_feature_vector(re0,im0,25.0,CANON_FREQ,include_names=True)\n",
    "    feats,rows,shape_feats=[],[],[]\n",
    "    for fp in tqdm(files,desc=\"Loading\"):\n",
    "        try:\n",
    "            v,m,rei,imi=load_single_eis_mat(fp); feats.append(v); rows.append(m)\n",
    "            if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "                rsh,ish=build_shape_normalized(rei,imi)\n",
    "                shape_feats.append(build_feature_vector(rsh,ish,m[\"Temp\"],CANON_FREQ))\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip]{fp.name}: {e}\")\n",
    "    if not rows: raise RuntimeError(\"No valid spectra\")\n",
    "    X=np.vstack(feats); X_shape=np.vstack(shape_feats) if shape_feats else None\n",
    "    meta_df=pd.DataFrame(rows)\n",
    "    # SoH refinement\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup=cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        meta_df[\"SoH_cont\"]=[\n",
    "            100.0*lookup.get((cid,stage), fallback)\n",
    "            for cid,stage,fallback in zip(meta_df.CellID,meta_df.SOH_stage,meta_df.RealSOH_file)\n",
    "        ]\n",
    "    else: meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "    y_soc=meta_df[\"SOC\"].values; y_soh=meta_df[\"SoH_cont\"].values\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        pd.concat([meta_df.reset_index(drop=True),\n",
    "                   pd.DataFrame(X,columns=feat_names)],axis=1).to_parquet(cfg.MODEL_DIR/\"training_features.parquet\",index=False)\n",
    "    return meta_df,X,(X_shape,feat_names),y_soc,y_soh\n",
    "\n",
    "def cell_split_mask(meta_df:pd.DataFrame):\n",
    "    cells=meta_df.CellID.unique(); rng=np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test=max(1,int(len(cells)*cfg.TEST_FRAC)); test_cells=rng.choice(cells,size=n_test,replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "def train_models(meta_df,X_raw,shape_bundle,y_soc,y_soh):\n",
    "    X_shape,feat_names=shape_bundle; mask_test=cell_split_mask(meta_df)\n",
    "    # --- SoC pipeline -------------------------------------------\n",
    "    soc_scaler=StandardScaler(); X_soc_s=soc_scaler.fit_transform(X_raw)\n",
    "    soc_pca=None; X_soc_in=X_soc_s\n",
    "    if cfg.USE_PCA_SOC:\n",
    "        soc_pca=PCA(n_components=min(cfg.PCA_SOC_COMPONENTS,X_soc_s.shape[1]-1),random_state=cfg.RANDOM_STATE)\n",
    "        X_soc_in=soc_pca.fit_transform(X_soc_s)\n",
    "    soc_model=RandomForestClassifier(n_estimators=600,min_samples_leaf=2,class_weight='balanced',\n",
    "                                     n_jobs=-1,random_state=cfg.RANDOM_STATE)\n",
    "    soc_model.fit(X_soc_in[~mask_test],y_soc[~mask_test])\n",
    "    if cfg.VERBOSE:\n",
    "        preds=soc_model.predict(X_soc_in[mask_test])\n",
    "        print(f\"[SoC] Acc={accuracy_score(y_soc[mask_test],preds):.3f} F1={f1_score(y_soc[mask_test],preds,average='macro'):.3f}\")\n",
    "    # --- SoH raw pipeline ---------------------------------------\n",
    "    soh_scaler=StandardScaler(); X_soh_s=soh_scaler.fit_transform(X_raw)\n",
    "    soh_pca=None; X_soh_in=X_soh_s\n",
    "    if cfg.USE_PCA_SOH:\n",
    "        soh_pca=PCA(n_components=min(cfg.PCA_SOH_COMPONENTS,X_soh_s.shape[1]-1),random_state=cfg.RANDOM_STATE)\n",
    "        X_soh_in=soh_pca.fit_transform(X_soh_s)\n",
    "    kernel=RBF(length_scale=np.ones(X_soh_in.shape[1])*3.0,length_scale_bounds=(1e-1,1e4))+\\\n",
    "           WhiteKernel(noise_level=1e-2,noise_level_bounds=(1e-6,1e-1))\n",
    "    gpr=GaussianProcessRegressor(kernel=kernel,alpha=0.0,normalize_y=True,\n",
    "                                 random_state=cfg.RANDOM_STATE,n_restarts_optimizer=3)\n",
    "    idx=np.random.default_rng(cfg.RANDOM_STATE).choice(X_soh_in.shape[0],\n",
    "            size=min(cfg.MAX_GPR_TRAIN_SAMPLES,X_soh_in.shape[0]),replace=False)\n",
    "    gpr.fit(X_soh_in[idx],y_soh[idx])\n",
    "    r2_gpr=r2_score(y_soh[mask_test],gpr.predict(X_soh_in[mask_test]))\n",
    "    # Alt model\n",
    "    hgb=HistGradientBoostingRegressor(learning_rate=0.05,max_iter=500,l2_regularization=1e-3,\n",
    "                                      random_state=cfg.RANDOM_STATE)\n",
    "    hgb.fit(X_soh_in[~mask_test],y_soh[~mask_test])\n",
    "    r2_hgb=r2_score(y_soh[mask_test],hgb.predict(X_soh_in[mask_test]))\n",
    "    best_model,model_name=(gpr,\"gpr_raw\") if r2_gpr>=r2_hgb else (hgb,\"hgb_raw\")\n",
    "    # shape model\n",
    "    shape_scaler=shape_pca=shape_model=shape_metrics=None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and X_shape is not None:\n",
    "        shape_scaler=StandardScaler(); Xs=shape_scaler.fit_transform(X_shape)\n",
    "        shape_pca=None; Xs_in=Xs\n",
    "        if cfg.USE_PCA_SOH:\n",
    "            shape_pca=PCA(n_components=min(cfg.PCA_SOH_COMPONENTS,Xs.shape[1]-1),random_state=cfg.RANDOM_STATE)\n",
    "            Xs_in=shape_pca.fit_transform(Xs)\n",
    "        kernel_s=RBF(length_scale=np.ones(Xs_in.shape[1])*3.0,length_scale_bounds=(1e-1,1e4))+\\\n",
    "                 WhiteKernel(noise_level=1e-2,noise_level_bounds=(1e-6,1e-1))\n",
    "        shape_model=GaussianProcessRegressor(kernel=kernel_s,alpha=0.0,normalize_y=True,\n",
    "                                             random_state=cfg.RANDOM_STATE,n_restarts_optimizer=3)\n",
    "        idx_s=np.random.default_rng(cfg.RANDOM_STATE).choice(Xs_in.shape[0],\n",
    "              size=min(cfg.MAX_GPR_TRAIN_SAMPLES,Xs_in.shape[0]),replace=False)\n",
    "        shape_model.fit(Xs_in[idx_s],y_soh[idx_s])\n",
    "        r2_shape=r2_score(y_soh[mask_test],shape_model.predict(Xs_in[mask_test]))\n",
    "        shape_metrics={\"r2\":r2_shape}\n",
    "    # Mahalanobis stats\n",
    "    cov=np.cov(X_soh_s.T); cov_inv=np.linalg.pinv(cov); center=X_soh_s.mean(axis=0)\n",
    "    bundle={\n",
    "        \"soc_scaler\":soc_scaler,\"soc_pca\":soc_pca,\"soc_model\":soc_model,\n",
    "        \"soh_scaler\":soh_scaler,\"soh_pca\":soh_pca,\"soh_model\":best_model,\"soh_model_name\":model_name,\n",
    "        \"shape_scaler\":shape_scaler,\"shape_pca\":shape_pca,\"shape_model\":shape_model,\n",
    "        \"freq_grid\":CANON_FREQ,\"feature_version\":cfg.FEATURE_VERSION,\"feature_manifest\":feat_names,\n",
    "        \"train_mahal\":{\"center\":center.tolist(),\"cov_inv\":cov_inv.tolist()}\n",
    "    }\n",
    "    joblib.dump(bundle,cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\")\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 8. LOAD (with legacy shim)\n",
    "# =========================\n",
    "def load_bundle():\n",
    "    path=cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"; b=joblib.load(path)\n",
    "    if \"soc_scaler\" not in b:   # legacy\n",
    "        b[\"soc_scaler\"]=b[\"scaler\"]; b[\"soh_scaler\"]=b[\"scaler\"]\n",
    "        b[\"soc_pca\"]=b.get(\"pca\"); b[\"soh_pca\"]=b.get(\"pca\")\n",
    "    return b\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8½.  INFERENCE FEATURISATION  (missing helper)\n",
    "# =========================\n",
    "def featurize_any(file_path: Path, bundle):\n",
    "    \"\"\"\n",
    "    • Loads ANY test file (.mat, .csv, .xls, .xlsx)\n",
    "    • Interpolates it onto the model’s canonical frequency grid\n",
    "    • Builds the feature vector (+ optional shape-normalised vector)\n",
    "    • Returns: (raw_vec, shape_vec_or_None, parsed_metadata_or_None)\n",
    "    \"\"\"\n",
    "    freq_grid = bundle[\"freq_grid\"]          # grid saved inside the bundle\n",
    "    meta = parse_eis_metadata(file_path.stem)\n",
    "\n",
    "    # raw spectrum\n",
    "    freq, re_raw, im_raw = load_any_inference(file_path)\n",
    "    re_i = _interp_channel(freq, re_raw, freq_grid)\n",
    "    im_i = _interp_channel(freq, im_raw, freq_grid)\n",
    "\n",
    "    # temperature (use override if no metadata)\n",
    "    if meta is None:\n",
    "        temp = cfg.TEST_TEMPERATURE_OVERRIDE if cfg.TEST_TEMPERATURE_OVERRIDE is not None else -1\n",
    "    else:\n",
    "        temp = meta[\"Temp\"]\n",
    "\n",
    "    # main feature vector\n",
    "    vec = build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "\n",
    "    # shape-normalised vector (for the ensemble GP)\n",
    "    norm_vec = None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and bundle.get(\"shape_model\") is not None:\n",
    "        rsh, ish = build_shape_normalized(re_i, im_i)\n",
    "        norm_vec = build_feature_vector(rsh, ish, temp, freq_grid)\n",
    "\n",
    "    return vec, norm_vec, meta\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9. PROJECTION PLOT\n",
    "# =========================\n",
    "def build_projection(soh,cpp,lower,exp=None,n=160):\n",
    "    if soh<=lower or cpp<=0: return np.array([0.0]),np.array([soh])\n",
    "    total=(soh-lower)*cpp; cycs=np.linspace(0,total,n)\n",
    "    S0=soh; exp=cfg.PLOT_EXPONENT if exp is None else exp\n",
    "    curve=lower+(S0-lower)*(1-cycs/total)**exp\n",
    "    return cycs,curve\n",
    "\n",
    "def plot_projection(name,soh,soh_std,cyc_t,cyc_l,cpp,ood,out):\n",
    "    if cyc_l<=0: return\n",
    "    cycs,curve=build_projection(soh,cpp,cfg.ILLUSTRATIVE_MIN_SOH)\n",
    "    plt.figure(figsize=(6.4,4)); plt.plot(cycs,curve,lw=2,label=\"Projected SoH\")\n",
    "    plt.axhline(cfg.DECISION_SOH_PERCENT,color=\"orange\",ls=\"--\"); plt.axhline(cfg.ILLUSTRATIVE_MIN_SOH,color=\"red\",ls=\":\")\n",
    "    plt.scatter([0],[soh],c=\"green\",s=55); plt.text(0,soh+0.7,f\"±{soh_std:.2f}\",color=\"green\",fontsize=8)\n",
    "    if cyc_t>0:\n",
    "        plt.axvline(cyc_t,color=\"orange\",ls=\"-.\"); plt.scatter([cyc_t],[cfg.DECISION_SOH_PERCENT],c=\"orange\",s=45)\n",
    "    plt.scatter([cycs[-1]],[cfg.ILLUSTRATIVE_MIN_SOH],c=\"red\",s=50)\n",
    "    plt.xlabel(\"Remaining Cycles\"); plt.ylabel(\"SoH (%)\"); plt.title(f\"RUL Projection – {name}\")\n",
    "    plt.grid(alpha=0.35); plt.tight_layout(); plt.savefig(out,dpi=140); plt.close()\n",
    "\n",
    "# =========================\n",
    "# 10. INFERENCE (single file)\n",
    "# =========================\n",
    "def mahalanobis_distance(x,center,cov_inv):\n",
    "    d=x-center; return float(np.sqrt(d@cov_inv@d.T))\n",
    "\n",
    "def predict_file(fp:Path,bundle,cpp_map,global_cpp):\n",
    "    vec,norm_vec,meta=featurize_any(fp,bundle)\n",
    "    # SoC\n",
    "    soc_scaler,b_soc_pca,b_soc_model=bundle[\"soc_scaler\"],bundle.get(\"soc_pca\"),bundle[\"soc_model\"]\n",
    "    X_soc=soc_scaler.transform(vec.reshape(1,-1)); X_soc=b_soc_pca.transform(X_soc) if b_soc_pca else X_soc\n",
    "    probs=b_soc_model.predict_proba(X_soc)[0]; soc=int(b_soc_model.classes_[probs.argmax()])\n",
    "    # SoH\n",
    "    s_scaler,s_pca,s_model=bundle[\"soh_scaler\"],bundle.get(\"soh_pca\"),bundle[\"soh_model\"]\n",
    "    Xs=s_scaler.transform(vec.reshape(1,-1)); Xs=s_pca.transform(Xs) if s_pca else Xs\n",
    "    if isinstance(s_model,GaussianProcessRegressor):\n",
    "        mu,sigma=s_model.predict(Xs,return_std=True); soh,sd=float(mu[0]),float(sigma[0])\n",
    "    else: soh=float(s_model.predict(Xs)[0]); sd=float(bundle[\"train_mahal\"][\"center\"][0])*0+5.0\n",
    "    sd=min(sd,5.0)\n",
    "    # ensemble\n",
    "    if cfg.ENSEMBLE_SOH and bundle.get(\"shape_model\") and norm_vec is not None:\n",
    "        sh_scl,sh_pca,sh_mdl=bundle[\"shape_scaler\"],bundle.get(\"shape_pca\"),bundle[\"shape_model\"]\n",
    "        Xn=sh_scl.transform(norm_vec.reshape(1,-1)); Xn=sh_pca.transform(Xn) if sh_pca else Xn\n",
    "        mu2=float(sh_mdl.predict(Xn)[0]); soh=0.5*(soh+mu2)\n",
    "    cpp=get_cpp(meta,cpp_map,global_cpp)\n",
    "    cyc_t=max((soh-cfg.DECISION_SOH_PERCENT)*cpp,0.0); cyc_l=max((soh-cfg.ILLUSTRATIVE_MIN_SOH)*cpp,0.0)\n",
    "    return {\"file\":str(fp),\"parsed_metadata\":meta,\"predicted_SoC\":soc,\n",
    "            \"SoC_probabilities\":{int(c):float(p) for c,p in zip(b_soc_model.classes_,probs)},\n",
    "            \"predicted_SoH_percent\":soh,\"SoH_std_estimate\":sd,\"cycles_per_percent_used\":cpp,\n",
    "            \"cycles_to_target\":cyc_t,\"cycles_to_lower\":cyc_l,\n",
    "            \"decision_threshold_percent\":cfg.DECISION_SOH_PERCENT,\n",
    "            \"lower_threshold_percent\":cfg.ILLUSTRATIVE_MIN_SOH,\n",
    "            \"feature_version\":bundle[\"feature_version\"],\n",
    "            \"soh_model_chosen\":bundle.get(\"soh_model_name\",\"raw\")}, cyc_t, cyc_l\n",
    "\n",
    "# =========================\n",
    "# 11. MAIN\n",
    "# =========================\n",
    "def main(argv=None):\n",
    "    p=argparse.ArgumentParser(add_help=False)\n",
    "    p.add_argument(\"--test\",dest=\"test_file\"); args,_=p.parse_known_args([] if argv is None else argv)\n",
    "    if args.test_file:                       # ← finish the truncated line\n",
    "        cfg.EIS_TEST_FILE = Path(args.test_file)\n",
    "\n",
    "    # ---------- sanity checks on folders ----------------------------\n",
    "    assert cfg.EIS_DIR.exists(), f\"EIS_DIR missing: {cfg.EIS_DIR}\"\n",
    "    if cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        assert cfg.CAP_DIR.exists(), f\"CAP_DIR missing: {cfg.CAP_DIR}\"\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps(to_jsonable(asdict(cfg)), indent=2))\n",
    "\n",
    "    # ---------- capacity ⇒ cycles-per-percent map -------------------\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR)\n",
    "    cpp_map, global_cpp = build_cpp_map(cap_df) if not cap_df.empty else ({}, cfg.CPP_FALLBACK)\n",
    "\n",
    "    # ---------- train or load model bundle --------------------------\n",
    "    bundle_path = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "    if bundle_path.exists() and not cfg.FORCE_RETRAIN:\n",
    "        bundle = load_bundle()\n",
    "        if cfg.VERBOSE: print(f\"[LOAD] Using bundle → {bundle_path}\")\n",
    "    else:\n",
    "        if cfg.VERBOSE: print(\"[TRAIN] Building dataset & training models …\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh)\n",
    "\n",
    "    # ---------- single-file inference -------------------------------\n",
    "    test_fp = cfg.EIS_TEST_FILE\n",
    "    if not test_fp.exists():\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_fp}\")\n",
    "\n",
    "    result, cyc_target, cyc_lower = predict_file(test_fp, bundle, cpp_map, global_cpp)\n",
    "\n",
    "    # ---------- save outputs (plot + JSON) --------------------------\n",
    "    out_plot = cfg.MODEL_DIR / f\"{test_fp.stem}_projection.png\"\n",
    "    plot_projection(\n",
    "        test_fp.stem,\n",
    "        result[\"predicted_SoH_percent\"],\n",
    "        result[\"SoH_std_estimate\"],\n",
    "        result[\"cycles_to_target\"],\n",
    "        result[\"cycles_to_lower\"],\n",
    "        result[\"cycles_per_percent_used\"],\n",
    "        False,\n",
    "        out_plot\n",
    "    )\n",
    "\n",
    "    out_json = cfg.MODEL_DIR / f\"{test_fp.stem}_prediction.json\"\n",
    "    cfg.MODEL_DIR.mkdir(exist_ok=True)\n",
    "    with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    print(json.dumps(result, indent=2))\n",
    "    print(f\"[PLOT]  {out_plot}\")\n",
    "    print(f\"[JSON]  {out_json}\\nDone.\")\n",
    "\n",
    "# =========================\n",
    "# 12. RUN (works in notebooks & terminal)\n",
    "# =========================\n",
    "main([])          # pass [] so Jupyter’s hidden “-f …” flag is ignored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b33917-6976-4134-ac58-b1a34771fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\tgondal0\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator GaussianProcessRegressor from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRADIO] Loaded bundle → models_eis_phase2_phys\\eis_soc_soh_phys_models.joblib\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# Gradio demo – upload ONE EIS file → projection plot + predicted SoC\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "import gradio as gr\n",
    "import tempfile, shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# ---------- 1.  one-time model / CPP setup -----------------------------\n",
    "cap_df   = load_capacity_info(cfg.CAP_DIR)\n",
    "cpp_map, global_cpp = build_cpp_map(cap_df) if not cap_df.empty else ({}, cfg.CPP_FALLBACK)\n",
    "\n",
    "bundle_path = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "if bundle_path.exists() and not cfg.FORCE_RETRAIN:\n",
    "    bundle = load_bundle()\n",
    "    print(f\"[GRADIO] Loaded bundle → {bundle_path}\")\n",
    "else:\n",
    "    print(\"[GRADIO] Training bundle – first-run only …\")\n",
    "    meta_df, X_raw, shape_bundle, y_soc, y_soh = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "    bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh)\n",
    "\n",
    "# ---------- 2.  inference wrapper for Gradio ---------------------------\n",
    "def run_inference(uploaded_file):\n",
    "    \"\"\"Gradio callback → returns (PIL.Image, int)\"\"\"\n",
    "    # keep the original filename & extension\n",
    "    tmp_path = Path(tempfile.gettempdir()) / Path(uploaded_file.name).name\n",
    "    shutil.copy(uploaded_file.name, tmp_path)\n",
    "\n",
    "    result, *_ = predict_file(tmp_path, bundle, cpp_map, global_cpp)\n",
    "\n",
    "    # regenerate a fresh projection plot (saved under MODEL_DIR)\n",
    "    plot_path = cfg.MODEL_DIR / f\"{tmp_path.stem}_projection.png\"\n",
    "    plot_projection(\n",
    "        tmp_path.stem,\n",
    "        result[\"predicted_SoH_percent\"],\n",
    "        result[\"SoH_std_estimate\"],\n",
    "        result[\"cycles_to_target\"],\n",
    "        result[\"cycles_to_lower\"],\n",
    "        result[\"cycles_per_percent_used\"],\n",
    "        False,\n",
    "        plot_path\n",
    "    )\n",
    "    return Image.open(plot_path), int(result[\"predicted_SoC\"])\n",
    "\n",
    "# ---------- 3.  build & launch the UI ----------------------------------\n",
    "demo = gr.Interface(\n",
    "    fn=run_inference,\n",
    "    inputs=gr.File(label=\"Upload EIS test file\"),\n",
    "    outputs=[\n",
    "        gr.Image(type=\"pil\", label=\"RUL projection\"),\n",
    "        gr.Number(label=\"Predicted SoC (%)\")\n",
    "    ],\n",
    "    title=\"EIS RUL / SoC predictor\",\n",
    "    description=\"Upload a single EIS spectrum. The model returns the projected RUL chart and the most-likely SoC class.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b1e92-7888-4f64-870a-f121c84f9a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2d515-7aa9-4a89-9dc1-679800a10b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "557bc796-a443-4c10-8907-8abd1a01c8d7",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd879a-10e7-43c1-be3e-ad4dc2bbb8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
