{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c66c8d2-de8c-4076-ad4c-dfe7ae7baf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "#  eis_helpers.py  ·  companion module for “Unified EIS v9”\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "#\n",
    "#  This **stand-alone** helper library implements every symbol that the\n",
    "#  new v9 training / inference script imports.  Drop this file next to\n",
    "#  your v9 main script (or add its directory to PYTHONPATH) and you’re\n",
    "#  good to go.  All external dependencies are limited to NumPy / Pandas /\n",
    "#  SciPy (for .mat I/O and simple interpolation).\n",
    "#\n",
    "#  Key capabilities\n",
    "#  ----------------\n",
    "#  • Canonical frequency grid + feature engineering  (magnitude, phase,\n",
    "#    optional DRT placeholder, room for custom shape-model features)\n",
    "#  • Robust filename metadata parsing for both EIS and capacity files\n",
    "#  • Capacity-vs-cycle loader that derives SoH and builds a CPP→cycles\n",
    "#    heuristic map\n",
    "#  • Dataset assembly (“long table” with one feature vector per EIS file)\n",
    "#  • Minimal plotting helper for projection curves (keeps PNG output)\n",
    "#  • JSON-friendly serialisation helpers\n",
    "#\n",
    "#  Anything not needed by v9 but retained from earlier versions\n",
    "#  (e.g. `train_models`, `load_bundle`) is stubbed so import errors vanish.\n",
    "#  Replace the stubs with your own advanced logic whenever required,\n",
    "#  without touching the main v9 script.\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# ╭───────────────────────────────╮\n",
    "# │ 1 · GLOBAL CONSTANTS & SEEDS  │\n",
    "# ╰───────────────────────────────╯\n",
    "# Canonical 50-point log-spaced frequency grid (0.01 Hz → 10 kHz)\n",
    "CANON_FREQ: np.ndarray = np.logspace(-2, 4, 50)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 123) -> None:  # noqa: D401\n",
    "    \"\"\"Set global RNG seeds for reproducibility.\"\"\"\n",
    "    import os\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "# ╭─────────────────────────╮\n",
    "# │ 2 · JSON SERIALISATION  │\n",
    "# ╰─────────────────────────╯\n",
    "def to_jsonable(obj: Any) -> Any:  # noqa: D401\n",
    "    \"\"\"Recursively convert NumPy / Path / set → vanilla JSON types.\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.floating)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, (np.ndarray, set, tuple, list)):\n",
    "        return [to_jsonable(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_jsonable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "    return obj\n",
    "\n",
    "\n",
    "# ╭────────────────────────────╮\n",
    "# │ 3 · FILENAME META PARSERS │\n",
    "# ╰────────────────────────────╯\n",
    "_EIS_RE = re.compile(\n",
    "    r\"(?P<cell>[A-Za-z0-9\\-]+)[_\\-]?(?:Cycle|CYL)?(?P<cycle>\\d+)[_\\-]?(?:SoC)?(?P<soc>\\d+)?\",\n",
    "    re.I,\n",
    ")\n",
    "_CAP_RE = re.compile(\n",
    "    r\"(?P<cell>[A-Za-z0-9\\-]+)[_\\-]?(?:Cap|CYC|Cycle)?(?P<cycle>\\d+)\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "\n",
    "def parse_eis_metadata(fp: Path) -> Dict[str, Any]:\n",
    "    m = _EIS_RE.search(fp.stem)\n",
    "    if not m:\n",
    "        return dict(cell_id=fp.stem, cycle_idx=np.nan, soc=np.nan)\n",
    "    d = m.groupdict()\n",
    "    return dict(\n",
    "        cell_id=d[\"cell\"],\n",
    "        cycle_idx=int(d[\"cycle\"]) if d[\"cycle\"] else np.nan,\n",
    "        soc=float(d[\"soc\"]) if d[\"soc\"] else np.nan,\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_cap_metadata(fp: Path) -> Dict[str, Any]:\n",
    "    m = _CAP_RE.search(fp.stem)\n",
    "    if not m:\n",
    "        return dict(cell_id=fp.stem, cycle_idx=np.nan)\n",
    "    d = m.groupdict()\n",
    "    return dict(cell_id=d[\"cell\"], cycle_idx=int(d[\"cycle\"]) if d[\"cycle\"] else np.nan)\n",
    "\n",
    "\n",
    "# ╭───────────────────────────────────────────────────╮\n",
    "# │ 4 · CAPACITY FILES  →  SoH & CPP HEURISTICS      │\n",
    "# ╰───────────────────────────────────────────────────╯\n",
    "def _load_one_capacity(fp: Path) -> pd.DataFrame:\n",
    "    \"\"\"Supports .csv, .xls, .xlsx with columns ≈ [Cycle, Capacity].\"\"\"\n",
    "    if fp.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(fp)\n",
    "    else:\n",
    "        df = pd.read_excel(fp, engine=\"openpyxl\" if fp.suffix.lower() == \".xlsx\" else None)\n",
    "\n",
    "    df = df.rename(columns=lambda s: s.strip().lower())\n",
    "    # normalise column names\n",
    "    if \"capacity\" not in df.columns:\n",
    "        for alt in (\"capacity_ah\", \"cap_ah\", \"capacity (ah)\"):\n",
    "            if alt in df.columns:\n",
    "                df[\"capacity\"] = df[alt]\n",
    "                break\n",
    "    if \"cycle\" not in df.columns:\n",
    "        raise ValueError(f\"File {fp.name} has no 'Cycle' column\")\n",
    "\n",
    "    meta = parse_cap_metadata(fp)\n",
    "    df = df[[\"cycle\", \"capacity\"]].copy()\n",
    "    df.insert(0, \"cell_id\", meta[\"cell_id\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Concatenate all capacity-check files into tidy DataFrame.\"\"\"\n",
    "    dfs: list[pd.DataFrame] = []\n",
    "    for fp in cap_dir.rglob(\"*\"):\n",
    "        if fp.suffix.lower() not in {\".csv\", \".xls\", \".xlsx\"}:\n",
    "            continue\n",
    "        try:\n",
    "            dfs.append(_load_one_capacity(fp))\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            warnings.warn(f\"Skipping capacity file {fp.name}: {exc}\")\n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(f\"No capacity files found under {cap_dir}\")\n",
    "    cap_df = pd.concat(dfs, ignore_index=True)\n",
    "    cap_df[\"init_cap\"] = cap_df.groupby(\"cell_id\")[\"capacity\"].transform(\"first\")\n",
    "    cap_df[\"soh_percent\"] = 100 * cap_df[\"capacity\"] / cap_df[\"init_cap\"]\n",
    "    return cap_df\n",
    "\n",
    "\n",
    "def build_cpp_map(cap_dir: Path) -> Dict[float, float]:\n",
    "    \"\"\"\n",
    "    Build *capacity-percent → median cycles-remaining* mapping across dataset.\n",
    "\n",
    "    Used as heuristic fallback during inference.\n",
    "    \"\"\"\n",
    "    cap_df = load_capacity_info(cap_dir)\n",
    "    final_cycle = cap_df.groupby(\"cell_id\")[\"cycle\"].transform(\"max\")\n",
    "    cap_df[\"cycles_remaining\"] = final_cycle - cap_df[\"cycle\"]\n",
    "    cap_df[\"cap_bin\"] = cap_df[\"soh_percent\"].round()  # 1 ppt bins\n",
    "    med = cap_df.groupby(\"cap_bin\")[\"cycles_remaining\"].median().dropna()\n",
    "    return med.to_dict()\n",
    "\n",
    "\n",
    "def get_cpp(cap_pct: float, cpp_map: Dict[float, float]) -> float:\n",
    "    \"\"\"Return cycles-remaining via nearest-neighbour lookup in *cpp_map*.\"\"\"\n",
    "    if not cpp_map or math.isnan(cap_pct):\n",
    "        return float(\"nan\")\n",
    "    keys = np.array(list(cpp_map))\n",
    "    nearest = keys[(np.abs(keys - cap_pct)).argmin()]\n",
    "    return float(cpp_map[nearest])\n",
    "\n",
    "\n",
    "# ╭──────────────────────────────╮\n",
    "# │ 5 · EIS → FEATURE VECTORS    │\n",
    "# ╰──────────────────────────────╯\n",
    "def _read_eis(fp: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (freq [Hz], Zreal [Ω], Zimag [Ω]).\"\"\"\n",
    "    suf = fp.suffix.lower()\n",
    "    if suf in {\".csv\", \".txt\"}:\n",
    "        df = pd.read_csv(fp)\n",
    "    elif suf in {\".xls\", \".xlsx\"}:\n",
    "        df = pd.read_excel(fp, engine=\"openpyxl\" if suf == \".xlsx\" else None)\n",
    "    elif suf == \".mat\":\n",
    "        mdict = loadmat(fp)\n",
    "        return np.squeeze(mdict[\"freq\"]), np.squeeze(mdict[\"Zreal\"]), np.squeeze(mdict[\"Zimag\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported EIS file type: {fp.suffix}\")\n",
    "    df = df.rename(columns=lambda s: s.strip().lower())\n",
    "    freq = df.iloc[:, 0].to_numpy(float)\n",
    "    zre = df.iloc[:, 1].to_numpy(float)\n",
    "    zim = df.iloc[:, 2].to_numpy(float)\n",
    "    return freq, zre, zim\n",
    "\n",
    "\n",
    "def _interp_to_grid(freq: np.ndarray, arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log-log interpolation onto CANON_FREQ.\"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        f = interp1d(np.log10(freq), np.log10(np.abs(arr)), bounds_error=False, fill_value=\"extrapolate\")\n",
    "    return 10 ** f(np.log10(CANON_FREQ))\n",
    "\n",
    "\n",
    "def _drt_stub(freq: np.ndarray, zre: np.ndarray, zim: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Placeholder for DRT features (returns zeros).  Replace with your own\n",
    "    distribution-of-relaxation-times calculator if available.\n",
    "    \"\"\"\n",
    "    return np.zeros_like(CANON_FREQ)\n",
    "\n",
    "\n",
    "def featurize_any(fp: Path, include_drt: bool = True) -> np.ndarray:\n",
    "    \"\"\"Generate one concatenated feature vector.\"\"\"\n",
    "    freq, zre, zim = _read_eis(fp)\n",
    "    mag = np.sqrt(zre**2 + zim**2)\n",
    "    pha = np.degrees(np.arctan2(zim, zre))\n",
    "    vecs = [\n",
    "        _interp_to_grid(freq, mag),\n",
    "        _interp_to_grid(freq, pha),\n",
    "    ]\n",
    "    if include_drt:\n",
    "        vecs.append(_drt_stub(freq, zre, zim))\n",
    "    return np.concatenate(vecs).astype(np.float32)\n",
    "\n",
    "\n",
    "# ╭─────────────────────────────╮\n",
    "# │ 6 · DATASET CONSTRUCTION    │\n",
    "# ╰─────────────────────────────╯\n",
    "def build_dataset(eis_dir: Path, cap_dir: Path, include_drt: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return DataFrame with columns features | cell_id | cycle_idx | soc | file_id | final_cycle.\"\"\"\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    for fp in eis_dir.rglob(\"*\"):\n",
    "        if fp.suffix.lower() not in {\".csv\", \".txt\", \".xls\", \".xlsx\", \".mat\"}:\n",
    "            continue\n",
    "        meta = parse_eis_metadata(fp)\n",
    "        rows.append(\n",
    "            dict(\n",
    "                features=featurize_any(fp, include_drt),\n",
    "                cell_id=meta[\"cell_id\"],\n",
    "                cycle_idx=meta[\"cycle_idx\"],\n",
    "                soc=meta[\"soc\"] / 100.0 if not math.isnan(meta[\"soc\"]) else np.nan,\n",
    "                file_id=fp.stem,\n",
    "            )\n",
    "        )\n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"No EIS files found under {eis_dir}\")\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # merge final_cycle from capacity data (if available)\n",
    "    try:\n",
    "        caps = load_capacity_info(cap_dir)\n",
    "        fin = caps.groupby(\"cell_id\")[\"cycle\"].max().rename(\"final_cycle\").reset_index()\n",
    "        df = df.merge(fin, on=\"cell_id\", how=\"left\")\n",
    "    except FileNotFoundError:\n",
    "        df[\"final_cycle\"] = np.nan\n",
    "    return df\n",
    "\n",
    "\n",
    "# ╭──────────────────────────────────────────╮\n",
    "# │ 7 · LEGACY STUBS (v7/v8 compatibility)   │\n",
    "# ╰──────────────────────────────────────────╯\n",
    "def train_models(*args, **kwargs):  # noqa: D401\n",
    "    raise NotImplementedError(\"train_models() not used in v9\")\n",
    "\n",
    "\n",
    "def load_bundle(*args, **kwargs):  # noqa: D401\n",
    "    raise NotImplementedError(\"load_bundle() not used in v9\")\n",
    "\n",
    "\n",
    "# ╭──────────────────────────────╮\n",
    "# │ 8 · PROJECTION PLOT HELPER   │\n",
    "# ╰──────────────────────────────╯\n",
    "@lru_cache(None)\n",
    "def _cell_soh_curve(cell_id: str, cap_dir: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (cycles, SoH%) for one cell, else generic decay if unknown.\"\"\"\n",
    "    try:\n",
    "        df = load_capacity_info(cap_dir)\n",
    "        sub = df[df[\"cell_id\"] == cell_id]\n",
    "        if sub.empty:\n",
    "            raise ValueError\n",
    "        return sub[\"cycle\"].to_numpy(), sub[\"soh_percent\"].to_numpy()\n",
    "    except Exception:  # noqa: BLE001\n",
    "        cycles = np.linspace(0, 1_200, 240)\n",
    "        soh = 80 + 20 * np.exp(-cycles / 400)\n",
    "        return cycles, soh\n",
    "\n",
    "\n",
    "def plot_projection(test_fp: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return reference projection curve for the cell in *test_fp*.\"\"\"\n",
    "    meta = parse_eis_metadata(test_fp)\n",
    "    cycles, soh = _cell_soh_curve(meta[\"cell_id\"], test_fp.parents[2])\n",
    "    return cycles, soh\n",
    "\n",
    "\n",
    "# ╭──────────────────────────────────────────────────╮\n",
    "# │ 9 · SINGLE-FILE FEATURE BUILDER (FOR INFERENCE) │\n",
    "# ╰──────────────────────────────────────────────────╯\n",
    "def build_feature_vector(test_fp: Path, include_drt: bool = True) -> Dict[str, Any]:\n",
    "    feats = featurize_any(test_fp, include_drt)\n",
    "    meta = parse_eis_metadata(test_fp)\n",
    "    cpp = 100.0  # default if no capacity data\n",
    "    try:\n",
    "        cap_dir = test_fp.parents[3]\n",
    "        caps = load_capacity_info(cap_dir)\n",
    "        row = caps[(caps[\"cell_id\"] == meta[\"cell_id\"]) & (caps[\"cycle\"] == meta[\"cycle_idx\"])]\n",
    "        if not row.empty:\n",
    "            cpp = float(row[\"soh_percent\"].iloc[0])\n",
    "    except Exception:  # noqa: BLE001\n",
    "        pass\n",
    "    return {\"features\": feats, \"cpp\": cpp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94015b7e-d514-4f8d-8c77-9f19fea7ba51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
