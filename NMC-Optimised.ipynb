{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c082129-a005-43b9-adbf-9e26c60afd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tmgon\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tmgon\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"GROUP_KFOLDS\": 0,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"USE_PCA\": true,\n",
      "  \"PCA_COMPONENTS\": 25,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 2500,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"FEATURE_VERSION\": 4,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FORCE_RETRAIN\": true,\n",
      "  \"LOW_SOH_VAR_EPS\": 0.0\n",
      "}\n",
      "[TRAIN] Building dataset & fitting models...\n",
      "[INFO] Capacity file set empty → using filename RealSOH only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectra: 100%|██████████| 360/360 [00:01<00:00, 215.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] SoH stats: min=80.46 max=100.00 mean=90.35 var=52.0277\n",
      "Training set: 360 spectra | feat_dim=173 | cells=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 13 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 15 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 19 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 20 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 22 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 23 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tmgon\\anaconda3\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 24 of parameter k1__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoC] holdout classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5     1.0000    0.9167    0.9565        12\n",
      "          20     1.0000    1.0000    1.0000        12\n",
      "          50     0.9231    1.0000    0.9600        12\n",
      "          70     1.0000    1.0000    1.0000        12\n",
      "          95     1.0000    1.0000    1.0000        12\n",
      "\n",
      "    accuracy                         0.9833        60\n",
      "   macro avg     0.9846    0.9833    0.9833        60\n",
      "weighted avg     0.9846    0.9833    0.9833        60\n",
      "\n",
      "[SoH] holdout RMSE=1.066 R2=0.963\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models_eis_phase2_phys\\\\eis_soc_soh_phys_models.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 591\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample prediction:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(demo, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 591\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[14], line 583\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mVERBOSE:\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m spectra | feat_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | cells=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta_df\u001b[38;5;241m.\u001b[39mCellID\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 583\u001b[0m     train_models(X,y_soc,y_soh,meta_df,feature_names)\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# Demonstration prediction on first file\u001b[39;00m\n\u001b[0;32m    586\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mEIS_DIR\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[14], line 515\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(X, y_soc, y_soh, meta_df, feature_names)\u001b[0m\n\u001b[0;32m    506\u001b[0m bundle\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m:scaler,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m:pca,\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoc_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:soc_model,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoh_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:soh_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m:metrics\n\u001b[0;32m    513\u001b[0m }\n\u001b[0;32m    514\u001b[0m out_path\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mMODEL_DIR\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meis_soc_soh_phys_models.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 515\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(bundle,out_path)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mVERBOSE:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model bundle → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models_eis_phase2_phys\\\\eis_soc_soh_phys_models.joblib'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "EIS → SoC (classification) + SoH (regression) unified training & inference (Phase 2 core)\n",
    "----------------------------------------------------------------------------------------\n",
    "Features:\n",
    "  - Raw interpolated Re/Im (canonical frequency grid)\n",
    "  - Basic magnitude stats\n",
    "  - F-features (peak & heuristic points)\n",
    "  - Physical features (Rs, Rct, tau_peak, Warburg proxy, etc.)\n",
    "  - Band statistics over log-frequency ranges\n",
    "  - Differential log-frequency slopes\n",
    "  - DRT-based regularized gamma summary features\n",
    "  - Temperature feature (stored as Temp_feat to avoid collision)\n",
    "\n",
    "Enhancements vs earlier snippet:\n",
    "  * Robust recursive to_jsonable to print config (fixes Path JSON error)\n",
    "  * Optional variance inflation when SoH variance is extremely low\n",
    "  * Clear training / validation split & logging\n",
    "  * Graceful handling of missing capacity refinement\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, joblib, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIG\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tmgon\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tmgon\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "\n",
    "    # Frequency grid\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int = 60\n",
    "\n",
    "    # Split / CV\n",
    "    TEST_FRAC: float = 0.2\n",
    "    GROUP_KFOLDS: int = 0\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    # PCA\n",
    "    USE_PCA: bool = True\n",
    "    PCA_COMPONENTS: int = 25\n",
    "\n",
    "    # GP limit\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 2500\n",
    "\n",
    "    # Feature toggles\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    # DRT\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    # Capacity refinement\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "\n",
    "    # Saving & meta\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    FEATURE_VERSION: int = 4\n",
    "    VERBOSE: bool = True\n",
    "    FORCE_RETRAIN: bool = True          # set False to reuse existing bundle\n",
    "    LOW_SOH_VAR_EPS: float = 0.0        # set >0 (e.g. 0.5) to inflate very low variance labels slightly\n",
    "\n",
    "cfg = Config()\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 2. UTILITIES\n",
    "# =========================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path):\n",
    "        return str(x)\n",
    "    if isinstance(x, dict):\n",
    "        return {k: to_jsonable(v) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "# =========================\n",
    "# 3. REGEX\n",
    "# =========================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "def parse_eis_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"SOC\": int(d[\"SOC\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"RealSOH_file\": int(d[\"RealSOH\"]) / 100.0\n",
    "    }\n",
    "\n",
    "def parse_cap_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"CycleIndex\": int(d[\"Cycle\"])\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 4. LOADERS\n",
    "# =========================\n",
    "def _find_matrix(mat_dict: dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v, np.ndarray) and v.ndim==2 and v.shape[1]>=3 and v.shape[0]>=10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw, y_raw, freq_target):\n",
    "    freq_raw = np.asarray(freq_raw).astype(float)\n",
    "    y_raw = np.asarray(y_raw).astype(float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw = freq_raw[::-1]; y_raw = y_raw[::-1]\n",
    "    uniq, idx = np.unique(freq_raw, return_index=True)\n",
    "    if len(uniq)!=len(freq_raw):\n",
    "        order = np.argsort(idx)\n",
    "        freq_raw = uniq[order]; y_raw = y_raw[idx][order]\n",
    "    f = interp1d(freq_raw, y_raw, bounds_error=False,\n",
    "                 fill_value=(y_raw[0], y_raw[-1]), kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "# =========================\n",
    "# 5. FEATURE COMPONENTS\n",
    "# =========================\n",
    "def compute_F_features(freq, re_i, im_i):\n",
    "    neg_im=-im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    F1=re_i[0]; F2=re_i[idx_peak]; F3=re_i[-1]\n",
    "    sc=np.where(np.sign(im_i[:-1]) != np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k=sc[0]; y0,y1=im_i[k],im_i[k+1]; w=-y0/(y1-y0+1e-12)\n",
    "        F4 = re_i[k] + w*(re_i[k+1]-re_i[k])\n",
    "    else:\n",
    "        F4 = np.nan\n",
    "    F5 = (re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6 = np.min(im_i)\n",
    "    mid_target=10.0\n",
    "    idx_mid = int(np.argmin(np.abs(freq-mid_target)))\n",
    "    F7 = re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES = [\n",
    "    \"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "    \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"\n",
    "]\n",
    "\n",
    "def physical_features(freq, re_i, im_i):\n",
    "    freq=np.asarray(freq); re_i=np.asarray(re_i); im_i=np.asarray(im_i)\n",
    "    neg_im=-im_i\n",
    "    idx_peak=int(np.argmax(neg_im))\n",
    "    Rs=float(re_i[0]); Rpeak=float(re_i[idx_peak]); Rlow=float(re_i[-1])\n",
    "    Rct=max(Rpeak - Rs,0.0)\n",
    "    arc_diam=Rlow - Rs\n",
    "    norm_arc = arc_diam/(Rs+1e-9)\n",
    "    f_peak=float(freq[idx_peak])\n",
    "    tau_peak = 1/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K=min(10,len(freq)//3)\n",
    "    if K>=4:\n",
    "        w_section=(2*np.pi*freq[-K:])**(-0.5)\n",
    "        re_section=re_i[-K:]\n",
    "        warburg_sigma=float(np.polyfit(w_section, re_section,1)[0]) if len(np.unique(w_section))>2 else np.nan\n",
    "    else:\n",
    "        warburg_sigma=np.nan\n",
    "    phase=np.arctan2(-im_i, re_i)\n",
    "    mid=(freq>=1)&(freq<=100)\n",
    "    if mid.sum()>2:\n",
    "        phase_mean_mid=float(phase[mid].mean())\n",
    "        phase_std_mid=float(phase[mid].std())\n",
    "    else:\n",
    "        phase_mean_mid=np.nan; phase_std_mid=np.nan\n",
    "    phase_min=float(phase.min())\n",
    "    lf_mask=(freq<=1.0)\n",
    "    if lf_mask.sum()>=4:\n",
    "        x=np.log10(freq[lf_mask]+1e-12); y=neg_im[lf_mask]\n",
    "        lf_slope=np.polyfit(x,y,1)[0]\n",
    "    else:\n",
    "        lf_slope=np.nan\n",
    "    arc_quality=(neg_im.max()-neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS=[(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq, re_i, im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m], im_i[m])\n",
    "            feats+=[z.mean(), z.std()]\n",
    "        else:\n",
    "            feats+=[np.nan,np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq, re_i, im_i, segments=5):\n",
    "    logf=np.log10(freq)\n",
    "    edges=np.linspace(logf.min(), logf.max(), segments+1)\n",
    "    out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]\n",
    "            out += [np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else:\n",
    "            out += [np.nan, np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES=[\n",
    "    \"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "    \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"\n",
    "]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w=2*np.pi*freq\n",
    "    tau=np.geomspace(tau_max,tau_min,n_tau)\n",
    "    WT=w[:,None]*tau[None,:]\n",
    "    denom=1+WT**2\n",
    "    K_re=1.0/denom\n",
    "    K_im=-WT/denom\n",
    "    R_inf=re_i[0]\n",
    "    y_re=re_i - R_inf\n",
    "    y_im=im_i\n",
    "    Y=np.concatenate([y_re,y_im])\n",
    "    K=np.vstack([K_re,K_im])\n",
    "    A=K.T@K + lam*np.eye(n_tau)\n",
    "    b=K.T@Y\n",
    "    gamma=linalg.solve(A,b,assume_a='pos')\n",
    "    gamma=np.clip(gamma,0,None)\n",
    "    return tau,gamma\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    if not cfg.INCLUDE_DRT:\n",
    "        return []\n",
    "    try:\n",
    "        tau,gamma=compute_drt(freq,re_i,im_i,\n",
    "                               cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,\n",
    "                               cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau=np.log10(tau)\n",
    "        g_sum=gamma.sum()+1e-12\n",
    "        w_norm=gamma/g_sum\n",
    "        mean_logtau=float((w_norm*log_tau).sum())\n",
    "        var_logtau=float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p=int(np.argmax(gamma))\n",
    "        peak_tau=float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid=np.median(log_tau)\n",
    "        frac_low=float(w_norm[log_tau<=mid].sum())\n",
    "        frac_high=1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception:\n",
    "        return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i, im_i, temp, freq, include_manifest=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts += [re_i, im_i]\n",
    "        names += [f\"Re_{i}\" for i in range(len(re_i))] + [f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z=np.hypot(re_i, im_i)\n",
    "        basics=[re_i[0], re_i[-1], re_i[-1]-re_i[0], z.max(), z.mean(), z.std()]\n",
    "        parts.append(np.array(basics)); names += [\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        Ff=compute_F_features(freq,re_i,im_i)\n",
    "        parts.append(np.array(Ff)); names += [f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        Pf=physical_features(freq,re_i,im_i)\n",
    "        parts.append(np.array(Pf)); names += PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        Bf=band_stats(freq,re_i,im_i)\n",
    "        parts.append(np.array(Bf))\n",
    "        for bi in range(len(BANDS)):\n",
    "            names += [f\"band{bi}_mean\", f\"band{bi}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i)\n",
    "        parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2):\n",
    "            names += [f\"slope_re_seg{i}\", f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        Df=drt_features(freq,re_i,im_i)\n",
    "        parts.append(np.array(Df)); names += DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names += [\"Temp_feat\"]\n",
    "    vec=np.concatenate(parts).astype(float)\n",
    "    vec=np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if include_manifest:\n",
    "        return vec, names\n",
    "    return vec\n",
    "\n",
    "# =========================\n",
    "# 6. CAPACITY\n",
    "# =========================\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY):\n",
    "        return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta=parse_cap_metadata(fp.stem)\n",
    "        if not meta: continue\n",
    "        try:\n",
    "            mat=loadmat(fp); arr=_find_matrix(mat)\n",
    "            if arr is None: continue\n",
    "            col=np.argmax(np.abs(arr[-50:, :]).mean(axis=0))\n",
    "            cap=float(np.nanmax(arr[:,col]))\n",
    "            meta[\"MeasuredCapacity_Ah\"]=cap\n",
    "            recs.append(meta)\n",
    "        except Exception:\n",
    "            pass\n",
    "    df=pd.DataFrame(recs)\n",
    "    if df.empty: return df\n",
    "    ref=df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"]=df[\"MeasuredCapacity_Ah\"]/ref\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 7. DATASET\n",
    "# =========================\n",
    "def load_single_eis(fp: Path):\n",
    "    meta=parse_eis_metadata(fp.stem)\n",
    "    if meta is None:\n",
    "        raise ValueError(f\"Bad filename: {fp.name}\")\n",
    "    mat=loadmat(fp); arr=_find_matrix(mat)\n",
    "    if arr is None: raise ValueError(f\"No EIS matrix in {fp.name}\")\n",
    "    freq_raw=arr[:,0].astype(float); re_raw=arr[:,1].astype(float); im_raw=arr[:,2].astype(float)\n",
    "    re_i=_interp_channel(freq_raw, re_raw, CANON_FREQ)\n",
    "    im_i=_interp_channel(freq_raw, im_raw, CANON_FREQ)\n",
    "    vec=build_feature_vector(re_i, im_i, meta[\"Temp\"], CANON_FREQ)\n",
    "    return vec, meta\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_df: Optional[pd.DataFrame]):\n",
    "    files=sorted(eis_dir.rglob(\"*.mat\"))\n",
    "    if not files: raise FileNotFoundError(f\"No .mat files in {eis_dir}\")\n",
    "    # Feature names\n",
    "    arr0=_find_matrix(loadmat(files[0]))\n",
    "    f0=arr0[:,0].astype(float); r0=arr0[:,1].astype(float); i0=arr0[:,2].astype(float)\n",
    "    re0=_interp_channel(f0,r0,CANON_FREQ); im0=_interp_channel(f0,i0,CANON_FREQ)\n",
    "    _, feature_names = build_feature_vector(re0, im0, 25.0, CANON_FREQ, include_manifest=True)\n",
    "\n",
    "    feats=[]; rows=[]\n",
    "    for fp in tqdm(files, desc=\"Loading spectra\"):\n",
    "        try:\n",
    "            v,m=load_single_eis(fp)\n",
    "            feats.append(v); rows.append(m)\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip] {fp.name}: {e}\")\n",
    "    if not rows: raise RuntimeError(\"No valid spectra\")\n",
    "    X=np.vstack(feats)\n",
    "    meta_df=pd.DataFrame(rows)\n",
    "\n",
    "    # SoH refinement\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup=cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        refined=[]\n",
    "        for cid,stage,fallback in zip(meta_df.CellID, meta_df.SOH_stage, meta_df.RealSOH_file):\n",
    "            nc=lookup.get((cid,stage))\n",
    "            refined.append(100.0*nc if nc is not None else fallback)\n",
    "        meta_df[\"SoH_cont\"]=refined\n",
    "    else:\n",
    "        meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "\n",
    "    y_soc=meta_df[\"SOC\"].values\n",
    "    y_soh=meta_df[\"SoH_cont\"].values.astype(float)\n",
    "\n",
    "    soh_var=float(np.var(y_soh))\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[DATA] SoH stats: min={y_soh.min():.2f} max={y_soh.max():.2f} mean={y_soh.mean():.2f} var={soh_var:.4f}\")\n",
    "    if soh_var < 1.0 and cfg.LOW_SOH_VAR_EPS > 0:\n",
    "        # mild variance inflation (center + noise) — optional\n",
    "        rng=np.random.default_rng(cfg.RANDOM_STATE+7)\n",
    "        noise=rng.normal(0, cfg.LOW_SOH_VAR_EPS, size=y_soh.shape)\n",
    "        y_soh = y_soh + noise\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[INFO] Applied variance inflation (epsilon={cfg.LOW_SOH_VAR_EPS}). New var={np.var(y_soh):.4f}\")\n",
    "\n",
    "    # Save feature table\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        feature_df=pd.DataFrame(X, columns=feature_names)\n",
    "        dup = set(meta_df.columns).intersection(feature_df.columns)\n",
    "        if dup:\n",
    "            feature_df=feature_df.rename(columns={c: f\"{c}_feat\" for c in dup})\n",
    "        pd.concat([meta_df.reset_index(drop=True), feature_df], axis=1)\\\n",
    "          .to_parquet(cfg.MODEL_DIR/\"training_features.parquet\", index=False)\n",
    "\n",
    "    return meta_df, X, y_soc, y_soh, feature_names\n",
    "\n",
    "# =========================\n",
    "# 8. SPLIT\n",
    "# =========================\n",
    "def simple_cell_split(meta_df: pd.DataFrame):\n",
    "    cells=meta_df.CellID.unique()\n",
    "    rng=np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test=max(1,int(len(cells)*cfg.TEST_FRAC))\n",
    "    test_cells=rng.choice(cells,size=n_test,replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "# =========================\n",
    "# 9. TRAIN\n",
    "# =========================\n",
    "def train_models(X,y_soc,y_soh,meta_df,feature_names):\n",
    "    scaler=StandardScaler()\n",
    "    Xs=scaler.fit_transform(X)\n",
    "    pca=None; Xm=Xs\n",
    "    if cfg.USE_PCA:\n",
    "        pca=PCA(n_components=min(cfg.PCA_COMPONENTS, Xs.shape[1]-1),\n",
    "                random_state=cfg.RANDOM_STATE)\n",
    "        Xm=pca.fit_transform(Xs)\n",
    "\n",
    "    def fit_once(Xt, y_soc_t, y_soh_t):\n",
    "        soc=RandomForestClassifier(\n",
    "            n_estimators=600, min_samples_leaf=2, class_weight='balanced',\n",
    "            n_jobs=-1, random_state=cfg.RANDOM_STATE\n",
    "        )\n",
    "        soc.fit(Xt, y_soc_t)\n",
    "        dim=Xt.shape[1]\n",
    "        kernel=RBF(length_scale=np.ones(dim),\n",
    "                   length_scale_bounds=(1e-2,1e3)) + \\\n",
    "               WhiteKernel(noise_level=1e-3,\n",
    "                           noise_level_bounds=(1e-6,1e-1))\n",
    "        gpr=GaussianProcessRegressor(\n",
    "            kernel=kernel, alpha=0.0, normalize_y=True,\n",
    "            random_state=cfg.RANDOM_STATE, n_restarts_optimizer=3\n",
    "        )\n",
    "        if Xt.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "            idx=np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "                Xt.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "            gpr.fit(Xt[idx], y_soh_t[idx])\n",
    "        else:\n",
    "            gpr.fit(Xt, y_soh_t)\n",
    "        return soc,gpr\n",
    "\n",
    "    if cfg.GROUP_KFOLDS and cfg.GROUP_KFOLDS>1:\n",
    "        gkf=GroupKFold(n_splits=cfg.GROUP_KFOLDS)\n",
    "        groups=meta_df.CellID.values\n",
    "        r2s=[]; fold_models=[]\n",
    "        for i,(tr,te) in enumerate(gkf.split(Xm,y_soc,groups)):\n",
    "            soc,gpr=fit_once(Xm[tr], y_soc[tr], y_soh[tr])\n",
    "            pred=gpr.predict(Xm[te]); r2=r2_score(y_soh[te], pred)\n",
    "            r2s.append(r2); fold_models.append((soc,gpr))\n",
    "            if cfg.VERBOSE: print(f\"[Fold {i}] SoH R2={r2:.3f}\")\n",
    "        best=int(np.argmax(r2s))\n",
    "        soc_model,soh_model=fold_models[best]\n",
    "        metrics={\"cv_soh_r2_mean\":float(np.mean(r2s)),\"chosen_fold\":best}\n",
    "    else:\n",
    "        mask_test=simple_cell_split(meta_df)\n",
    "        Xt,Xv = Xm[~mask_test], Xm[mask_test]\n",
    "        yst,ysv = y_soc[~mask_test], y_soc[mask_test]\n",
    "        yht,yhv = y_soh[~mask_test], y_soh[mask_test]\n",
    "        soc_model,soh_model=fit_once(Xt, yst, yht)\n",
    "        soc_pred=soc_model.predict(Xv)\n",
    "        acc=accuracy_score(ysv,soc_pred); f1m=f1_score(ysv,soc_pred,average='macro')\n",
    "        soh_pred=soh_model.predict(Xv)\n",
    "        rmse=math.sqrt(mean_squared_error(yhv,soh_pred)); r2=r2_score(yhv,soh_pred)\n",
    "        if cfg.VERBOSE:\n",
    "            print(\"[SoC] holdout classification report:\")\n",
    "            print(classification_report(ysv,soc_pred,digits=4))\n",
    "            print(f\"[SoH] holdout RMSE={rmse:.3f} R2={r2:.3f}\")\n",
    "        metrics={\"soc_accuracy\":acc,\"soc_macro_f1\":f1m,\"soh_rmse\":rmse,\"soh_r2\":r2}\n",
    "\n",
    "    bundle={\n",
    "        \"scaler\":scaler,\"pca\":pca,\n",
    "        \"soc_model\":soc_model,\"soh_model\":soh_model,\n",
    "        \"freq_grid\":CANON_FREQ,\"feature_version\":cfg.FEATURE_VERSION,\n",
    "        \"feature_manifest\":feature_names,\n",
    "        \"config\":to_jsonable(asdict(cfg)),\n",
    "        \"metrics\":metrics\n",
    "    }\n",
    "    out_path=cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    joblib.dump(bundle,out_path)\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"Saved model bundle → {out_path}\")\n",
    "        print(\"Metrics:\", json.dumps(metrics, indent=2))\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 10. INFERENCE\n",
    "# =========================\n",
    "def load_bundle()->dict:\n",
    "    return joblib.load(cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\")\n",
    "\n",
    "def featurize_new_eis(file_path:Path, bundle=None):\n",
    "    if bundle is None:\n",
    "        bundle=load_bundle()\n",
    "    freq_grid=bundle[\"freq_grid\"]\n",
    "    meta=parse_eis_metadata(file_path.stem)\n",
    "    mat=loadmat(file_path); arr=_find_matrix(mat)\n",
    "    if arr is None: raise ValueError(\"No valid EIS matrix in file.\")\n",
    "    fr=arr[:,0].astype(float); re_raw=arr[:,1].astype(float); im_raw=arr[:,2].astype(float)\n",
    "    re_i=_interp_channel(fr,re_raw,freq_grid)\n",
    "    im_i=_interp_channel(fr,im_raw,freq_grid)\n",
    "    temp=meta[\"Temp\"] if meta else -1\n",
    "    vec=build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "    return vec, meta\n",
    "\n",
    "def predict_from_file(file_path:Path)->Dict[str,Any]:\n",
    "    bundle=load_bundle()\n",
    "    scaler=bundle[\"scaler\"]; pca=bundle[\"pca\"]\n",
    "    soc_model=bundle[\"soc_model\"]; soh_model=bundle[\"soh_model\"]\n",
    "    feat_vec, meta=featurize_new_eis(file_path, bundle)\n",
    "    Xs=scaler.transform(feat_vec.reshape(1,-1))\n",
    "    Xp=pca.transform(Xs) if pca is not None else Xs\n",
    "    soc_probs=soc_model.predict_proba(Xp)[0]\n",
    "    soc_classes=soc_model.classes_\n",
    "    soc_pred=int(soc_classes[np.argmax(soc_probs)])\n",
    "    soh_mean_arr, soh_std_arr=soh_model.predict(Xp, return_std=True)\n",
    "    return {\n",
    "        \"file\": file_path.name,\n",
    "        \"metadata\": meta,\n",
    "        \"predicted_SoC\": soc_pred,\n",
    "        \"SoC_probabilities\": {int(c): float(p) for c,p in zip(soc_classes, soc_probs)},\n",
    "        \"predicted_SoH_percent\": float(soh_mean_arr[0]),\n",
    "        \"SoH_std_estimate\": float(soh_std_arr[0]),\n",
    "        \"feature_version\": bundle.get(\"feature_version\")\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 11. MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps(to_jsonable(asdict(cfg)), indent=2))\n",
    "    assert cfg.EIS_DIR.exists(), f\"EIS_DIR missing: {cfg.EIS_DIR}\"\n",
    "    if cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        assert cfg.CAP_DIR.exists(), f\"CAP_DIR missing: {cfg.CAP_DIR}\"\n",
    "\n",
    "    bundle_path=cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    if bundle_path.exists() and not cfg.FORCE_RETRAIN:\n",
    "        if cfg.VERBOSE: print(f\"[LOAD] Reusing existing model: {bundle_path}\")\n",
    "    else:\n",
    "        if cfg.VERBOSE: print(\"[TRAIN] Building dataset & fitting models...\")\n",
    "        cap_df=load_capacity_info(cfg.CAP_DIR)\n",
    "        if cfg.REFINE_SOH_WITH_CAPACITY and cap_df.empty and cfg.VERBOSE:\n",
    "            print(\"[INFO] Capacity file set empty → using filename RealSOH only.\")\n",
    "        meta_df, X, y_soc, y_soh, feature_names = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"Training set: {X.shape[0]} spectra | feat_dim={X.shape[1]} | cells={meta_df.CellID.nunique()}\")\n",
    "        train_models(X,y_soc,y_soh,meta_df,feature_names)\n",
    "\n",
    "    # Demonstration prediction on first file\n",
    "    sample = sorted(cfg.EIS_DIR.rglob(\"*.mat\"))[0]\n",
    "    demo = predict_from_file(sample)\n",
    "    print(\"\\nExample prediction:\\n\", json.dumps(demo, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1666f4b4-9b5d-49c7-b103-4269e6eb2b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      " {\n",
      "  \"EIS_DIR\": \"C:\\\\Users\\\\tmgon\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\EIS_Test\",\n",
      "  \"CAP_DIR\": \"C:\\\\Users\\\\tmgon\\\\OneDrive - Edith Cowan University\\\\00 - Megallan Power\\\\NMC Batteries Warwick Station\\\\NMC\\\\DIB_Data\\\\.matfiles\\\\Capacity_Check\",\n",
      "  \"MODEL_DIR\": \"models_eis_phase2_phys\",\n",
      "  \"EIS_TEST_FILES\": [\n",
      "    \"Mazda-Battery-Cell1.xlsx\",\n",
      "    \"Mazda-Battery-Cell2.xlsx\"\n",
      "  ],\n",
      "  \"F_MIN\": 0.01,\n",
      "  \"F_MAX\": 10000.0,\n",
      "  \"N_FREQ\": 60,\n",
      "  \"TEST_FRAC\": 0.2,\n",
      "  \"GROUP_KFOLDS\": 0,\n",
      "  \"RANDOM_STATE\": 42,\n",
      "  \"USE_PCA_SOC\": true,\n",
      "  \"USE_PCA_SOH\": false,\n",
      "  \"PCA_SOC_COMPONENTS\": 25,\n",
      "  \"PCA_SOH_COMPONENTS\": 30,\n",
      "  \"INCLUDE_RAW_RE_IM\": true,\n",
      "  \"INCLUDE_BASICS\": true,\n",
      "  \"INCLUDE_F_FEATS\": true,\n",
      "  \"INCLUDE_PHYSICAL\": true,\n",
      "  \"INCLUDE_DRT\": true,\n",
      "  \"INCLUDE_BAND_STATS\": true,\n",
      "  \"INCLUDE_DIFF_SLOPES\": true,\n",
      "  \"DRT_POINTS\": 60,\n",
      "  \"DRT_TAU_MIN\": 0.0001,\n",
      "  \"DRT_TAU_MAX\": 10000.0,\n",
      "  \"DRT_LAMBDA\": 0.01,\n",
      "  \"REFINE_SOH_WITH_CAPACITY\": true,\n",
      "  \"MAX_GPR_TRAIN_SAMPLES\": 3500,\n",
      "  \"INCLUDE_NORMALIZED_SHAPE_MODEL\": true,\n",
      "  \"ENSEMBLE_SOH\": true,\n",
      "  \"NORMALIZE_SHAPE_BY_HF_RE\": true,\n",
      "  \"DECISION_SOH_PERCENT\": 50.0,\n",
      "  \"ILLUSTRATIVE_MIN_SOH\": 40.0,\n",
      "  \"CPP_ROLLING_WINDOW\": 5,\n",
      "  \"CPP_MIN_POINTS\": 6,\n",
      "  \"CPP_FALLBACK\": 20.0,\n",
      "  \"TEST_TEMPERATURE_OVERRIDE\": 25.0,\n",
      "  \"FORCE_RETRAIN\": false,\n",
      "  \"SAVE_FEATURE_TABLE\": true,\n",
      "  \"VERBOSE\": true,\n",
      "  \"FEATURE_VERSION\": 7,\n",
      "  \"MAHAL_THRESHOLD\": 10.0,\n",
      "  \"GP_ARD_NORM_THRESHOLD\": 6.0,\n",
      "  \"PLOT_EXPONENT\": 1.25\n",
      "}\n",
      "[INFO] No / empty capacity data -> fallback Cpp.\n",
      "[LOAD] Using existing model bundle: models_eis_phase2_phys\\eis_soc_soh_phys_models.joblib\n",
      "\n",
      "===== TEST: Mazda-Battery-Cell1.xlsx =====\n",
      "{\n",
      "  \"file\": \"Mazda-Battery-Cell1.xlsx\",\n",
      "  \"parsed_metadata\": null,\n",
      "  \"predicted_SoC\": 20,\n",
      "  \"SoC_probabilities\": {\n",
      "    \"5\": 0.31517658730158726,\n",
      "    \"20\": 0.40203174603174613,\n",
      "    \"50\": 0.05611111111111112,\n",
      "    \"70\": 0.13527777777777777,\n",
      "    \"95\": 0.09140277777777779\n",
      "  },\n",
      "  \"predicted_SoH_percent\": 90.6675,\n",
      "  \"SoH_std_estimate\": 5.0,\n",
      "  \"raw_model_mean\": 90.6675,\n",
      "  \"raw_model_std\": 5.0,\n",
      "  \"shape_model_mean\": null,\n",
      "  \"shape_model_std\": null,\n",
      "  \"cycles_per_percent_used\": 20.0,\n",
      "  \"cycles_to_target\": 813.3500000000001,\n",
      "  \"cycles_to_lower\": 1013.3500000000001,\n",
      "  \"decision_threshold_percent\": 50.0,\n",
      "  \"lower_threshold_percent\": 40.0,\n",
      "  \"feature_version\": 3,\n",
      "  \"soh_model_chosen\": \"legacy_model\",\n",
      "  \"OOD_mahal\": 205.59520383113002,\n",
      "  \"OOD_gp_ard_norm\": null,\n",
      "  \"OOD_flag\": true\n",
      "}\n",
      "[PLOT] Saved: models_eis_phase2_phys\\Mazda-Battery-Cell1_projection.png\n",
      "[JSON] Saved: models_eis_phase2_phys\\Mazda-Battery-Cell1_prediction.json\n",
      "\n",
      "===== TEST: Mazda-Battery-Cell2.xlsx =====\n",
      "{\n",
      "  \"file\": \"Mazda-Battery-Cell2.xlsx\",\n",
      "  \"parsed_metadata\": null,\n",
      "  \"predicted_SoC\": 95,\n",
      "  \"SoC_probabilities\": {\n",
      "    \"5\": 0.29520105820105824,\n",
      "    \"20\": 0.07914682539682542,\n",
      "    \"50\": 0.16102777777777777,\n",
      "    \"70\": 0.11950925925925927,\n",
      "    \"95\": 0.34511507936507935\n",
      "  },\n",
      "  \"predicted_SoH_percent\": 90.6675,\n",
      "  \"SoH_std_estimate\": 5.0,\n",
      "  \"raw_model_mean\": 90.6675,\n",
      "  \"raw_model_std\": 5.0,\n",
      "  \"shape_model_mean\": null,\n",
      "  \"shape_model_std\": null,\n",
      "  \"cycles_per_percent_used\": 20.0,\n",
      "  \"cycles_to_target\": 813.3500000000001,\n",
      "  \"cycles_to_lower\": 1013.3500000000001,\n",
      "  \"decision_threshold_percent\": 50.0,\n",
      "  \"lower_threshold_percent\": 40.0,\n",
      "  \"feature_version\": 3,\n",
      "  \"soh_model_chosen\": \"legacy_model\",\n",
      "  \"OOD_mahal\": 855.2860988699334,\n",
      "  \"OOD_gp_ard_norm\": null,\n",
      "  \"OOD_flag\": true\n",
      "}\n",
      "[PLOT] Saved: models_eis_phase2_phys\\Mazda-Battery-Cell2_projection.png\n",
      "[JSON] Saved: models_eis_phase2_phys\\Mazda-Battery-Cell2_prediction.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Optimized Unified EIS Training + Inference + Dynamic RUL (v7 with Legacy Bundle Compatibility)\n",
    "----------------------------------------------------------------------------------------------\n",
    "Features:\n",
    "  * Extensive impedance feature set: raw Re/Im, basics, F-features, physical (Rs, Rct, etc.),\n",
    "    DRT-derived descriptors, band statistics, log-frequency differential slopes, temperature.\n",
    "  * Optional shape-normalized GP (normalizes spectrum by high-frequency Re) + ensemble with raw model.\n",
    "  * Multiple SoH regressors (Gaussian Process, HistGradientBoosting); automatic selection by validation R².\n",
    "  * Dynamic cycles-per-percent (Cpp) estimation from capacity .mat files (rolling linear fit); fallback heuristic.\n",
    "  * Multi-format inference (.mat, .csv, .xls, .xlsx) with column auto-detection & freq interpolation.\n",
    "  * Out-of-distribution (OOD) diagnostics: Mahalanobis distance in scaled feature space + GP ARD norm heuristic.\n",
    "  * Backward compatibility loader to use older bundles (with keys: \"scaler\",\"pca\",\"soc_model\",\"soh_model\").\n",
    "  * Per-test JSON output + projection plot annotated with OOD flag.\n",
    "  * Configurable PCA for SoC and SoH pipelines (decoupled).\n",
    "\n",
    "Outputs in MODEL_DIR:\n",
    "  - eis_soc_soh_phys_models.joblib\n",
    "  - training_features.parquet (if SAVE_FEATURE_TABLE)\n",
    "  - <TestFile>_prediction.json\n",
    "  - <TestFile>_projection.png\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, random, warnings, joblib\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from scipy import linalg\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIGURATION\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Training data directories (update if needed)\n",
    "    EIS_DIR: Path = Path(r\"C:\\Users\\tmgon\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\EIS_Test\")\n",
    "    CAP_DIR: Path = Path(r\"C:\\Users\\tmgon\\OneDrive - Edith Cowan University\\00 - Megallan Power\\NMC Batteries Warwick Station\\NMC\\DIB_Data\\.matfiles\\Capacity_Check\")\n",
    "    MODEL_DIR: Path = Path(\"models_eis_phase2_phys\")\n",
    "\n",
    "    # Test files (multi-format allowed)\n",
    "    EIS_TEST_FILES: List[Path] = None  # assigned after instantiation\n",
    "\n",
    "    # Frequency interpolation grid\n",
    "    F_MIN: float = 1e-2\n",
    "    F_MAX: float = 1e4\n",
    "    N_FREQ: int = 60\n",
    "\n",
    "    # Train / split settings\n",
    "    TEST_FRAC: float = 0.2        # fraction of cells for hold-out test (if GROUP_KFOLDS==0)\n",
    "    GROUP_KFOLDS: int = 0         # >1 to enable grouped CV\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    # PCA toggles (separate for SoC & SoH)\n",
    "    USE_PCA_SOC: bool = True\n",
    "    USE_PCA_SOH: bool = False\n",
    "    PCA_SOC_COMPONENTS: int = 25\n",
    "    PCA_SOH_COMPONENTS: int = 30\n",
    "\n",
    "    # Feature group toggles\n",
    "    INCLUDE_RAW_RE_IM: bool = True\n",
    "    INCLUDE_BASICS: bool = True\n",
    "    INCLUDE_F_FEATS: bool = True\n",
    "    INCLUDE_PHYSICAL: bool = True\n",
    "    INCLUDE_DRT: bool = True\n",
    "    INCLUDE_BAND_STATS: bool = True\n",
    "    INCLUDE_DIFF_SLOPES: bool = True\n",
    "\n",
    "    # DRT params\n",
    "    DRT_POINTS: int = 60\n",
    "    DRT_TAU_MIN: float = 1e-4\n",
    "    DRT_TAU_MAX: float = 1e4\n",
    "    DRT_LAMBDA: float = 1e-2\n",
    "\n",
    "    # Capacity-based refinement\n",
    "    REFINE_SOH_WITH_CAPACITY: bool = True\n",
    "\n",
    "    # SoH modeling\n",
    "    MAX_GPR_TRAIN_SAMPLES: int = 3500\n",
    "    INCLUDE_NORMALIZED_SHAPE_MODEL: bool = True\n",
    "    ENSEMBLE_SOH: bool = True\n",
    "    NORMALIZE_SHAPE_BY_HF_RE: bool = True\n",
    "\n",
    "    # RUL parameters\n",
    "    DECISION_SOH_PERCENT: float = 50.0\n",
    "    ILLUSTRATIVE_MIN_SOH: float = 40.0\n",
    "    CPP_ROLLING_WINDOW: int = 5\n",
    "    CPP_MIN_POINTS: int = 6\n",
    "    CPP_FALLBACK: float = 20.0  # fallback cycles-per-percent\n",
    "\n",
    "    # Inference extras\n",
    "    TEST_TEMPERATURE_OVERRIDE: Optional[float] = 25.0  # applied if metadata absent\n",
    "    FORCE_RETRAIN: bool = False  # force retraining even if bundle exists\n",
    "\n",
    "    # Saving / logging\n",
    "    SAVE_FEATURE_TABLE: bool = True\n",
    "    VERBOSE: bool = True\n",
    "    FEATURE_VERSION: int = 7\n",
    "\n",
    "    # OOD thresholds\n",
    "    MAHAL_THRESHOLD: float = 10.0\n",
    "    GP_ARD_NORM_THRESHOLD: float = 6.0\n",
    "\n",
    "    # Projection curve shape exponent\n",
    "    PLOT_EXPONENT: float = 1.25\n",
    "\n",
    "cfg = Config()\n",
    "if cfg.EIS_TEST_FILES is None:\n",
    "    cfg.EIS_TEST_FILES = [\n",
    "        Path(\"Mazda-Battery-Cell1.xlsx\"),\n",
    "        Path(\"Mazda-Battery-Cell2.xlsx\")\n",
    "    ]\n",
    "cfg.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 2. UTILITIES\n",
    "# =========================\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(cfg.RANDOM_STATE)\n",
    "\n",
    "def to_jsonable(x):\n",
    "    if isinstance(x, Path): return str(x)\n",
    "    if isinstance(x, dict): return {k: to_jsonable(v) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return [to_jsonable(i) for i in x]\n",
    "    return x\n",
    "\n",
    "CANON_FREQ = np.geomspace(cfg.F_MAX, cfg.F_MIN, cfg.N_FREQ)\n",
    "\n",
    "# =========================\n",
    "# 3. REGEX\n",
    "# =========================\n",
    "EIS_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_(?P<Temp>\\d+)degC_(?P<SOC>\\d+)SOC_(?P<RealSOH>\\d+)\"\n",
    ")\n",
    "CAP_META_PATTERN = re.compile(\n",
    "    r\"Cell(?P<CellID>\\d+)_(?P<SOH>80|85|90|95|100)SOH_Capacity_Check_(?P<Temp>\\d+)degC_(?P<Cycle>\\d+)cycle\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. PARSERS\n",
    "# =========================\n",
    "def parse_eis_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = EIS_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"SOC\": int(d[\"SOC\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"RealSOH_file\": int(d[\"RealSOH\"])/100.0\n",
    "    }\n",
    "\n",
    "def parse_cap_metadata(stem: str) -> Optional[Dict[str, Any]]:\n",
    "    m = CAP_META_PATTERN.search(stem)\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"CellID\": f\"Cell{d['CellID']}\",\n",
    "        \"SOH_stage\": int(d[\"SOH\"]),\n",
    "        \"Temp\": int(d[\"Temp\"]),\n",
    "        \"CycleIndex\": int(d[\"Cycle\"])\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 5. LOW-LEVEL LOADERS / INTERPOLATION\n",
    "# =========================\n",
    "def _find_matrix(mat_dict: dict):\n",
    "    for v in mat_dict.values():\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[1] >= 3 and v.shape[0] >= 10:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _interp_channel(freq_raw, y_raw, freq_target):\n",
    "    freq_raw = np.asarray(freq_raw).astype(float)\n",
    "    y_raw = np.asarray(y_raw).astype(float)\n",
    "    if freq_raw[0] < freq_raw[-1]:\n",
    "        freq_raw = freq_raw[::-1]; y_raw = y_raw[::-1]\n",
    "    uniq, idx = np.unique(freq_raw, return_index=True)\n",
    "    if len(uniq) != len(freq_raw):\n",
    "        order = np.argsort(idx)\n",
    "        freq_raw = uniq[order]; y_raw = y_raw[idx][order]\n",
    "    f = interp1d(freq_raw, y_raw, bounds_error=False,\n",
    "                 fill_value=(y_raw[0], y_raw[-1]), kind=\"linear\")\n",
    "    return f(freq_target)\n",
    "\n",
    "FREQ_CANDS = [\"frequency\",\"freq\",\"f\",\"hz\",\"frequency(hz)\",\"Frequency(Hz)\"]\n",
    "RE_CANDS   = [\"zreal\",\"re(z)\",\"re\",\"real\",\"z_re\",\"zreal(ohm)\",\"re (ohm)\",\"re(z) (ohm)\",\"Zreal\",\"Zreal (ohm)\",\"Zreal(ohm)\"]\n",
    "IM_CANDS   = [\"-zimag\",\"zimag\",\"im(z)\",\"im\",\"imag\",\"imaginary\",\"z_im\",\"zimg\",\"z_imag\",\" -Zimag (ohm)\",\" -Zimag(ohm)\",\"-Zimag\",\"Zimag\",\"Zimag (ohm)\"]\n",
    "\n",
    "def _select_column(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c.lower() in low: return low[c.lower()]\n",
    "    for c in cands:\n",
    "        for col in df.columns:\n",
    "            if c.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def load_mat_eis(path: Path):\n",
    "    mat = loadmat(path)\n",
    "    arr = _find_matrix(mat)\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"No valid EIS matrix in {path.name}\")\n",
    "    return arr[:,0].astype(float), arr[:,1].astype(float), arr[:,2].astype(float)\n",
    "\n",
    "def load_table_eis(path: Path):\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty table.\")\n",
    "    fcol = _select_column(df, FREQ_CANDS)\n",
    "    recol = _select_column(df, RE_CANDS)\n",
    "    imcol = _select_column(df, IM_CANDS)\n",
    "    if recol is None or imcol is None:\n",
    "        raise ValueError(f\"Missing Re/Im columns in {path.name}\")\n",
    "    re_vals = pd.to_numeric(df[recol], errors=\"coerce\").to_numpy()\n",
    "    im_vals = pd.to_numeric(df[imcol], errors=\"coerce\").to_numpy()\n",
    "    if fcol is not None:\n",
    "        freq_vals = pd.to_numeric(df[fcol], errors=\"coerce\").to_numpy()\n",
    "    else:\n",
    "        n = min(len(re_vals), len(im_vals))\n",
    "        freq_vals = np.geomspace(cfg.F_MAX, cfg.F_MIN, n)\n",
    "    n = min(len(freq_vals), len(re_vals), len(im_vals))\n",
    "    freq_vals = freq_vals[:n]; re_vals = re_vals[:n]; im_vals = im_vals[:n]\n",
    "    # Imag sign normalization (prefer negative semicircle)\n",
    "    if np.nanmean(im_vals) > 0:\n",
    "        im_vals = -im_vals\n",
    "    return freq_vals, re_vals.astype(float), im_vals.astype(float)\n",
    "\n",
    "def load_any_inference(path: Path):\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".mat\": return load_mat_eis(path)\n",
    "    if suf in (\".csv\",\".xls\",\".xlsx\"): return load_table_eis(path)\n",
    "    raise ValueError(f\"Unsupported test file extension: {suf}\")\n",
    "\n",
    "# =========================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# =========================\n",
    "def compute_F_features(freq, re_i, im_i):\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    F1 = re_i[0]; F2 = re_i[idx_peak]; F3 = re_i[-1]\n",
    "    sc = np.where(np.sign(im_i[:-1]) != np.sign(im_i[1:]))[0]\n",
    "    if len(sc):\n",
    "        k = sc[0]; y0,y1 = im_i[k], im_i[k+1]\n",
    "        w = -y0/(y1 - y0 + 1e-12)\n",
    "        F4 = re_i[k] + w*(re_i[k+1]-re_i[k])\n",
    "    else:\n",
    "        F4 = np.nan\n",
    "    F5 = (re_i[idx_peak]-F1) if idx_peak>0 else np.nan\n",
    "    F6 = np.min(im_i)\n",
    "    mid_target = 10.0\n",
    "    idx_mid = int(np.argmin(np.abs(freq-mid_target)))\n",
    "    F7 = re_i[idx_mid]\n",
    "    return [F1,F2,F3,F4,F5,F6,F7]\n",
    "\n",
    "PHYSICAL_FEATURE_NAMES = [\n",
    "    \"Rs\",\"Rct\",\"tau_peak\",\"warburg_sigma\",\"arc_quality\",\n",
    "    \"phase_mean_mid\",\"phase_std_mid\",\"phase_min\",\"lf_slope_negIm\",\"norm_arc\"\n",
    "]\n",
    "\n",
    "def physical_features(freq, re_i, im_i):\n",
    "    freq = np.asarray(freq); re_i = np.asarray(re_i); im_i = np.asarray(im_i)\n",
    "    neg_im = -im_i\n",
    "    idx_peak = int(np.argmax(neg_im))\n",
    "    Rs = float(re_i[0]); Rpeak = float(re_i[idx_peak]); Rlow = float(re_i[-1])\n",
    "    Rct = max(Rpeak - Rs, 0.0)\n",
    "    arc_diam = Rlow - Rs\n",
    "    norm_arc = arc_diam / (Rs + 1e-9)\n",
    "    f_peak = float(freq[idx_peak])\n",
    "    tau_peak = 1.0/(2*math.pi*f_peak) if f_peak>0 else np.nan\n",
    "    K = min(10, len(freq)//3)\n",
    "    if K >= 4:\n",
    "        w_section = (2*np.pi*freq[-K:])**(-0.5)\n",
    "        re_section = re_i[-K:]\n",
    "        if len(np.unique(w_section)) > 2:\n",
    "            warburg_sigma = float(np.polyfit(w_section, re_section, 1)[0])\n",
    "        else:\n",
    "            warburg_sigma = np.nan\n",
    "    else:\n",
    "        warburg_sigma = np.nan\n",
    "    phase = np.arctan2(-im_i, re_i)\n",
    "    mid_mask = (freq>=1) & (freq<=100)\n",
    "    if mid_mask.sum()>2:\n",
    "        phase_mean_mid = float(phase[mid_mask].mean())\n",
    "        phase_std_mid  = float(phase[mid_mask].std())\n",
    "    else:\n",
    "        phase_mean_mid = np.nan; phase_std_mid = np.nan\n",
    "    phase_min = float(phase.min())\n",
    "    lf_mask = (freq<=1.0)\n",
    "    if lf_mask.sum() >= 4:\n",
    "        x = np.log10(freq[lf_mask]+1e-12); y = neg_im[lf_mask]\n",
    "        lf_slope = np.polyfit(x, y, 1)[0]\n",
    "    else:\n",
    "        lf_slope = np.nan\n",
    "    arc_quality = (neg_im.max() - neg_im.min())/(abs(neg_im.mean())+1e-9)\n",
    "    return [Rs,Rct,tau_peak,warburg_sigma,arc_quality,\n",
    "            phase_mean_mid,phase_std_mid,phase_min,lf_slope,norm_arc]\n",
    "\n",
    "BANDS = [(1e4,1e3),(1e3,1e2),(1e2,10),(10,1),(1,1e-1),(1e-1,1e-2)]\n",
    "def band_stats(freq, re_i, im_i):\n",
    "    feats=[]; freq=np.asarray(freq)\n",
    "    for hi,lo in BANDS:\n",
    "        m=(freq<=hi)&(freq>=lo)\n",
    "        if m.sum()>1:\n",
    "            z=np.hypot(re_i[m], im_i[m])\n",
    "            feats += [z.mean(), z.std()]\n",
    "        else:\n",
    "            feats += [np.nan, np.nan]\n",
    "    return feats\n",
    "\n",
    "def diff_slopes(freq, re_i, im_i, segments=5):\n",
    "    logf = np.log10(freq)\n",
    "    edges = np.linspace(logf.min(), logf.max(), segments+1)\n",
    "    out=[]\n",
    "    for i in range(segments):\n",
    "        m=(logf>=edges[i])&(logf<=edges[i+1])\n",
    "        if m.sum()>=3:\n",
    "            x=logf[m]\n",
    "            out += [np.polyfit(x,re_i[m],1)[0], np.polyfit(x,(-im_i)[m],1)[0]]\n",
    "        else:\n",
    "            out += [np.nan, np.nan]\n",
    "    return out\n",
    "\n",
    "DRT_FEATURE_NAMES = [\n",
    "    \"drt_sum\",\"drt_mean_logtau\",\"drt_var_logtau\",\"drt_peak_tau\",\n",
    "    \"drt_peak_gamma\",\"drt_frac_low_tau\",\"drt_frac_high_tau\"\n",
    "]\n",
    "\n",
    "def compute_drt(freq,re_i,im_i,tau_min,tau_max,n_tau,lam):\n",
    "    w = 2*np.pi*freq\n",
    "    tau = np.geomspace(tau_max, tau_min, n_tau)\n",
    "    WT = w[:,None]*tau[None,:]\n",
    "    denom = 1+WT**2\n",
    "    K_re = 1.0/denom\n",
    "    K_im = -WT/denom\n",
    "    R_inf = re_i[0]\n",
    "    y_re = re_i - R_inf\n",
    "    y_im = im_i\n",
    "    Y = np.concatenate([y_re, y_im])\n",
    "    K = np.vstack([K_re, K_im])\n",
    "    A = K.T @ K + lam*np.eye(n_tau)\n",
    "    b = K.T @ Y\n",
    "    gamma = linalg.solve(A,b,assume_a='pos')\n",
    "    gamma = np.clip(gamma,0,None)\n",
    "    return tau, gamma\n",
    "\n",
    "def drt_features(freq,re_i,im_i):\n",
    "    try:\n",
    "        tau,gamma = compute_drt(freq,re_i,im_i,\n",
    "                                 cfg.DRT_TAU_MIN,cfg.DRT_TAU_MAX,\n",
    "                                 cfg.DRT_POINTS,cfg.DRT_LAMBDA)\n",
    "        log_tau = np.log10(tau)\n",
    "        g_sum = gamma.sum()+1e-12\n",
    "        w_norm = gamma/g_sum\n",
    "        mean_logtau = float((w_norm*log_tau).sum())\n",
    "        var_logtau  = float((w_norm*(log_tau-mean_logtau)**2).sum())\n",
    "        p = int(np.argmax(gamma))\n",
    "        peak_tau = float(tau[p]); peak_gamma=float(gamma[p])\n",
    "        mid = np.median(log_tau)\n",
    "        frac_low = float(w_norm[log_tau<=mid].sum())\n",
    "        frac_high = 1-frac_low\n",
    "        return [g_sum,mean_logtau,var_logtau,peak_tau,peak_gamma,frac_low,frac_high]\n",
    "    except Exception:\n",
    "        return [np.nan]*7\n",
    "\n",
    "def build_feature_vector(re_i, im_i, temp, freq, include_names=False):\n",
    "    parts=[]; names=[]\n",
    "    if cfg.INCLUDE_RAW_RE_IM:\n",
    "        parts += [re_i, im_i]\n",
    "        names += [f\"Re_{i}\" for i in range(len(re_i))] + [f\"Im_{i}\" for i in range(len(im_i))]\n",
    "    if cfg.INCLUDE_BASICS:\n",
    "        z = np.hypot(re_i, im_i)\n",
    "        basics=[re_i[0], re_i[-1], re_i[-1]-re_i[0], z.max(), z.mean(), z.std()]\n",
    "        parts.append(np.array(basics)); names += [\"hf_re\",\"lf_re\",\"arc_diam\",\"zmag_max\",\"zmag_mean\",\"zmag_std\"]\n",
    "    if cfg.INCLUDE_F_FEATS:\n",
    "        Ff=compute_F_features(freq,re_i,im_i); parts.append(np.array(Ff)); names += [f\"F{i}\" for i in range(1,8)]\n",
    "    if cfg.INCLUDE_PHYSICAL:\n",
    "        Pf=physical_features(freq,re_i,im_i); parts.append(np.array(Pf)); names += PHYSICAL_FEATURE_NAMES\n",
    "    if cfg.INCLUDE_BAND_STATS:\n",
    "        Bf=band_stats(freq,re_i,im_i); parts.append(np.array(Bf))\n",
    "        for bi in range(len(BANDS)): names += [f\"band{bi}_mean\", f\"band{bi}_std\"]\n",
    "    if cfg.INCLUDE_DIFF_SLOPES:\n",
    "        Ds=diff_slopes(freq,re_i,im_i); parts.append(np.array(Ds))\n",
    "        for i in range(len(Ds)//2): names += [f\"slope_re_seg{i}\", f\"slope_negIm_seg{i}\"]\n",
    "    if cfg.INCLUDE_DRT:\n",
    "        Df=drt_features(freq,re_i,im_i); parts.append(np.array(Df)); names += DRT_FEATURE_NAMES\n",
    "    parts.append(np.array([temp])); names += [\"Temp\"]\n",
    "    vec = np.concatenate(parts).astype(float)\n",
    "    vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if include_names: return vec, names\n",
    "    return vec\n",
    "\n",
    "def build_shape_normalized(re_i, im_i):\n",
    "    hf = re_i[0] if re_i[0] != 0 else 1.0\n",
    "    return re_i / hf, im_i / hf\n",
    "\n",
    "# =========================\n",
    "# 7. CAPACITY & CPP\n",
    "# =========================\n",
    "def load_capacity_info(cap_dir: Path) -> pd.DataFrame:\n",
    "    if not (cap_dir.exists() and cfg.REFINE_SOH_WITH_CAPACITY):\n",
    "        return pd.DataFrame()\n",
    "    recs=[]\n",
    "    for fp in cap_dir.rglob(\"*.mat\"):\n",
    "        meta = parse_cap_metadata(fp.stem)\n",
    "        if not meta: continue\n",
    "        try:\n",
    "            mat=loadmat(fp); arr=_find_matrix(mat)\n",
    "            if arr is None: continue\n",
    "            col=np.argmax(np.abs(arr[-50:, :]).mean(axis=0))\n",
    "            cap=float(np.nanmax(arr[:,col]))\n",
    "            meta[\"MeasuredCapacity_Ah\"]=cap\n",
    "            recs.append(meta)\n",
    "        except Exception:\n",
    "            pass\n",
    "    df=pd.DataFrame(recs)\n",
    "    if df.empty: return df\n",
    "    ref=df.groupby(\"CellID\")[\"MeasuredCapacity_Ah\"].transform(\"max\")\n",
    "    df[\"NormCapacity\"]=df[\"MeasuredCapacity_Ah\"]/ref\n",
    "    df[\"SoH_percent\"]=df[\"NormCapacity\"]*100.0\n",
    "    return df\n",
    "\n",
    "def estimate_cpp_per_cell(capacity_df: pd.DataFrame,\n",
    "                          window:int, min_points:int)->Dict[str,float]:\n",
    "    cpp={}\n",
    "    for cid,grp in capacity_df.groupby(\"CellID\"):\n",
    "        g=grp.sort_values(\"CycleIndex\")\n",
    "        if g.shape[0]<min_points: continue\n",
    "        tail=g.tail(window)\n",
    "        x=tail[\"CycleIndex\"].values.astype(float)\n",
    "        y=tail[\"SoH_percent\"].values.astype(float)\n",
    "        if len(np.unique(x))<2: continue\n",
    "        slope=np.polyfit(x,y,1)[0]  # SoH% / cycle\n",
    "        if slope >= -1e-6:  # non-degrading\n",
    "            continue\n",
    "        cpp[cid]=1.0/abs(slope)\n",
    "    return cpp\n",
    "\n",
    "def build_cpp_map(cap_df: pd.DataFrame):\n",
    "    if cap_df.empty: return {}, cfg.CPP_FALLBACK\n",
    "    cpp_map=estimate_cpp_per_cell(\n",
    "        cap_df[[\"CellID\",\"CycleIndex\",\"SoH_percent\"]],\n",
    "        cfg.CPP_ROLLING_WINDOW, cfg.CPP_MIN_POINTS\n",
    "    )\n",
    "    if not cpp_map:\n",
    "        return {}, cfg.CPP_FALLBACK\n",
    "    return cpp_map, float(np.median(list(cpp_map.values())))\n",
    "\n",
    "def get_cpp(meta: dict, cpp_map: Dict[str,float], global_cpp: float):\n",
    "    if not meta: return global_cpp\n",
    "    return cpp_map.get(meta.get(\"CellID\"), global_cpp)\n",
    "\n",
    "# =========================\n",
    "# 8. DATASET BUILD (training on .mat only)\n",
    "# =========================\n",
    "def load_single_eis_mat(fp: Path):\n",
    "    meta = parse_eis_metadata(fp.stem)\n",
    "    if meta is None:\n",
    "        raise ValueError(f\"Bad filename: {fp.name}\")\n",
    "    freq,re_z,im_z = load_mat_eis(fp)\n",
    "    re_i=_interp_channel(freq, re_z, CANON_FREQ)\n",
    "    im_i=_interp_channel(freq, im_z, CANON_FREQ)\n",
    "    vec=build_feature_vector(re_i, im_i, meta[\"Temp\"], CANON_FREQ)\n",
    "    return vec, meta, re_i, im_i\n",
    "\n",
    "def build_dataset(eis_dir: Path, cap_df: Optional[pd.DataFrame]):\n",
    "    files = sorted(eis_dir.rglob(\"*.mat\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .mat spectra in {eis_dir}\")\n",
    "\n",
    "    # Feature names from first file\n",
    "    f0,r0,i0 = load_mat_eis(files[0])\n",
    "    re0=_interp_channel(f0,r0,CANON_FREQ); im0=_interp_channel(f0,i0,CANON_FREQ)\n",
    "    _, feature_names = build_feature_vector(re0, im0, 25.0, CANON_FREQ, include_names=True)\n",
    "\n",
    "    feats=[]; rows=[]; shape_feats=[]\n",
    "    for fp in tqdm(files, desc=\"Loading training spectra\"):\n",
    "        try:\n",
    "            v, m, rei, imi = load_single_eis_mat(fp)\n",
    "            feats.append(v); rows.append(m)\n",
    "            if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "                rsh, ish = build_shape_normalized(rei, imi)\n",
    "                shape_vec = build_feature_vector(rsh, ish, m[\"Temp\"], CANON_FREQ)\n",
    "                shape_feats.append(shape_vec)\n",
    "        except Exception as e:\n",
    "            if cfg.VERBOSE: print(f\"[Skip] {fp.name}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No valid training spectra after filtering.\")\n",
    "\n",
    "    X = np.vstack(feats)\n",
    "    X_shape = np.vstack(shape_feats) if (cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and shape_feats) else None\n",
    "    meta_df = pd.DataFrame(rows)\n",
    "\n",
    "    # SoH refinement with capacity\n",
    "    if cap_df is not None and not cap_df.empty and cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        lookup = cap_df.set_index([\"CellID\",\"SOH_stage\"])[\"NormCapacity\"].to_dict()\n",
    "        refined=[]\n",
    "        for cid, stage, fallback in zip(meta_df.CellID, meta_df.SOH_stage, meta_df.RealSOH_file):\n",
    "            nc = lookup.get((cid, stage))\n",
    "            refined.append(100.0*nc if nc is not None else fallback)\n",
    "        meta_df[\"SoH_cont\"]=refined\n",
    "    else:\n",
    "        meta_df[\"SoH_cont\"]=meta_df[\"RealSOH_file\"]\n",
    "\n",
    "    y_soc = meta_df[\"SOC\"].values\n",
    "    y_soh = meta_df[\"SoH_cont\"].values\n",
    "\n",
    "    soh_var = float(np.var(y_soh))\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[DATA] SoH range: {y_soh.min():.2f} – {y_soh.max():.2f} (var={soh_var:.3f})\")\n",
    "        if soh_var < 1.0:\n",
    "            print(\"[WARN] Low SoH variance → model may output near-constant SoH.\")\n",
    "\n",
    "    if cfg.SAVE_FEATURE_TABLE:\n",
    "        pd.concat(\n",
    "            [meta_df.reset_index(drop=True),\n",
    "             pd.DataFrame(X, columns=feature_names)], axis=1\n",
    "        ).to_parquet(cfg.MODEL_DIR/\"training_features.parquet\", index=False)\n",
    "\n",
    "    return meta_df, X, (X_shape, feature_names), y_soc, y_soh\n",
    "\n",
    "# =========================\n",
    "# 9. SPLITTING\n",
    "# =========================\n",
    "def cell_split_mask(meta_df: pd.DataFrame):\n",
    "    cells = meta_df.CellID.unique()\n",
    "    rng = np.random.default_rng(cfg.RANDOM_STATE)\n",
    "    n_test = max(1, int(len(cells)*cfg.TEST_FRAC))\n",
    "    test_cells = rng.choice(cells, size=n_test, replace=False)\n",
    "    return meta_df.CellID.isin(test_cells)\n",
    "\n",
    "# =========================\n",
    "# 10. TRAINING\n",
    "# =========================\n",
    "def train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh):\n",
    "    X_shape, feature_names = shape_bundle\n",
    "    mask_test = cell_split_mask(meta_df)\n",
    "\n",
    "    # ----- SoC pipeline -----\n",
    "    soc_scaler = StandardScaler()\n",
    "    X_soc_s = soc_scaler.fit_transform(X_raw)\n",
    "    soc_pca = None\n",
    "    X_soc_model = X_soc_s\n",
    "    if cfg.USE_PCA_SOC:\n",
    "        soc_pca = PCA(n_components=min(cfg.PCA_SOC_COMPONENTS, X_soc_s.shape[1]-1),\n",
    "                      random_state=cfg.RANDOM_STATE)\n",
    "        X_soc_model = soc_pca.fit_transform(X_soc_s)\n",
    "\n",
    "    soc_model = RandomForestClassifier(\n",
    "        n_estimators=600, min_samples_leaf=2, class_weight='balanced',\n",
    "        n_jobs=-1, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    soc_model.fit(X_soc_model[~mask_test], y_soc[~mask_test])\n",
    "    soc_pred = soc_model.predict(X_soc_model[mask_test])\n",
    "    soc_acc = accuracy_score(y_soc[mask_test], soc_pred)\n",
    "    soc_f1  = f1_score(y_soc[mask_test], soc_pred, average='macro')\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoC] Acc={soc_acc:.3f} MacroF1={soc_f1:.3f}\")\n",
    "        print(classification_report(y_soc[mask_test], soc_pred, digits=4))\n",
    "\n",
    "    # ----- SoH raw pipeline -----\n",
    "    soh_scaler = StandardScaler()\n",
    "    X_soh_s = soh_scaler.fit_transform(X_raw)\n",
    "    soh_pca=None\n",
    "    X_soh_model = X_soh_s\n",
    "    if cfg.USE_PCA_SOH:\n",
    "        soh_pca = PCA(n_components=min(cfg.PCA_SOH_COMPONENTS, X_soh_s.shape[1]-1),\n",
    "                      random_state=cfg.RANDOM_STATE)\n",
    "        X_soh_model = soh_pca.fit_transform(X_soh_s)\n",
    "\n",
    "    candidates = {}\n",
    "\n",
    "    # (1) Raw Gaussian Process\n",
    "    dim = X_soh_model.shape[1]\n",
    "    kernel = RBF(length_scale=np.ones(dim)*3.0,\n",
    "                 length_scale_bounds=(1e-1,1e4)) + \\\n",
    "             WhiteKernel(noise_level=1e-2,\n",
    "                         noise_level_bounds=(1e-6,1e-1))\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel, alpha=0.0, normalize_y=True,\n",
    "        random_state=cfg.RANDOM_STATE, n_restarts_optimizer=3\n",
    "    )\n",
    "    if X_soh_model.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "        idx = np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "            X_soh_model.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "        gpr.fit(X_soh_model[idx], y_soh[idx])\n",
    "    else:\n",
    "        gpr.fit(X_soh_model, y_soh)\n",
    "    pred_gpr = gpr.predict(X_soh_model[mask_test])\n",
    "    r2_gpr = r2_score(y_soh[mask_test], pred_gpr)\n",
    "    rmse_gpr = math.sqrt(mean_squared_error(y_soh[mask_test], pred_gpr))\n",
    "    candidates[\"gpr_raw\"] = (gpr, r2_gpr, rmse_gpr)\n",
    "\n",
    "    # (2) HistGradientBoosting\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05, max_iter=500,\n",
    "        l2_regularization=1e-3, random_state=cfg.RANDOM_STATE\n",
    "    )\n",
    "    hgb.fit(X_soh_model[~mask_test], y_soh[~mask_test])\n",
    "    pred_hgb = hgb.predict(X_soh_model[mask_test])\n",
    "    r2_hgb = r2_score(y_soh[mask_test], pred_hgb)\n",
    "    rmse_hgb = math.sqrt(mean_squared_error(y_soh[mask_test], pred_hgb))\n",
    "    candidates[\"hgb_raw\"] = (hgb, r2_hgb, rmse_hgb)\n",
    "\n",
    "    # (3) Shape-normalized GP\n",
    "    shape_model = None; shape_scaler=None; shape_pca=None; shape_metrics=None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and (X_shape is not None):\n",
    "        shape_scaler = StandardScaler()\n",
    "        X_shape_s = shape_scaler.fit_transform(X_shape)\n",
    "        X_shape_model = X_shape_s\n",
    "        if cfg.USE_PCA_SOH:\n",
    "            shape_pca = PCA(n_components=min(cfg.PCA_SOH_COMPONENTS, X_shape_s.shape[1]-1),\n",
    "                            random_state=cfg.RANDOM_STATE)\n",
    "            X_shape_model = shape_pca.fit_transform(X_shape_s)\n",
    "        dim_s = X_shape_model.shape[1]\n",
    "        kernel_s = RBF(length_scale=np.ones(dim_s)*3.0,\n",
    "                       length_scale_bounds=(1e-1,1e4)) + \\\n",
    "                   WhiteKernel(noise_level=1e-2,\n",
    "                               noise_level_bounds=(1e-6,1e-1))\n",
    "        shape_model = GaussianProcessRegressor(\n",
    "            kernel=kernel_s, alpha=0.0, normalize_y=True,\n",
    "            random_state=cfg.RANDOM_STATE, n_restarts_optimizer=3\n",
    "        )\n",
    "        if X_shape_model.shape[0] > cfg.MAX_GPR_TRAIN_SAMPLES:\n",
    "            idx_s = np.random.default_rng(cfg.RANDOM_STATE).choice(\n",
    "                X_shape_model.shape[0], size=cfg.MAX_GPR_TRAIN_SAMPLES, replace=False)\n",
    "            shape_model.fit(X_shape_model[idx_s], y_soh[idx_s])\n",
    "        else:\n",
    "            shape_model.fit(X_shape_model, y_soh)\n",
    "        spred = shape_model.predict(X_shape_model[mask_test])\n",
    "        r2_shape = r2_score(y_soh[mask_test], spred)\n",
    "        rmse_shape = math.sqrt(mean_squared_error(y_soh[mask_test], spred))\n",
    "        candidates[\"gpr_shape\"] = (shape_model, r2_shape, rmse_shape)\n",
    "        shape_metrics = {\"r2\": r2_shape, \"rmse\": rmse_shape}\n",
    "\n",
    "    # Select best raw candidate\n",
    "    best_name = max([\"gpr_raw\",\"hgb_raw\"], key=lambda k: candidates[k][1])\n",
    "    best_model, best_r2, best_rmse = candidates[best_name]\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[SoH] GPR_raw:  R2={r2_gpr:.3f} RMSE={rmse_gpr:.2f}\")\n",
    "        print(f\"[SoH] HGB_raw:  R2={r2_hgb:.3f} RMSE={rmse_hgb:.2f}\")\n",
    "        if shape_metrics:\n",
    "            print(f\"[SoH] ShapeGP: R2={shape_metrics['r2']:.3f} RMSE={shape_metrics['rmse']:.2f}\")\n",
    "        print(f\"[SoH] Selected raw model = {best_name}\")\n",
    "\n",
    "    # Mahalanobis precomputed on scaled raw SoH space\n",
    "    cov = np.cov(X_soh_s.T)\n",
    "    try:\n",
    "        cov_inv = np.linalg.pinv(cov)\n",
    "    except Exception:\n",
    "        cov_inv = np.eye(cov.shape[0])\n",
    "    center = X_soh_s.mean(axis=0)\n",
    "\n",
    "    bundle = {\n",
    "        # SOC\n",
    "        \"soc_scaler\": soc_scaler,\n",
    "        \"soc_pca\": soc_pca,\n",
    "        \"soc_model\": soc_model,\n",
    "        # SOH primary\n",
    "        \"soh_scaler\": soh_scaler,\n",
    "        \"soh_pca\": soh_pca,\n",
    "        \"soh_model\": best_model,\n",
    "        \"soh_model_name\": best_name,\n",
    "        # Shape model (optional)\n",
    "        \"shape_scaler\": shape_scaler,\n",
    "        \"shape_pca\": shape_pca,\n",
    "        \"shape_model\": shape_model,\n",
    "        # Meta\n",
    "        \"freq_grid\": CANON_FREQ,\n",
    "        \"feature_version\": cfg.FEATURE_VERSION,\n",
    "        \"feature_manifest\": feature_names,\n",
    "        \"config\": to_jsonable(asdict(cfg)),\n",
    "        \"metrics\": {\n",
    "            \"soc_accuracy\": soc_acc,\n",
    "            \"soc_macro_f1\": soc_f1,\n",
    "            \"soh_r2_selected\": best_r2,\n",
    "            \"soh_rmse_selected\": best_rmse\n",
    "        },\n",
    "        \"soh_candidates_metrics\": {\n",
    "            \"gpr_raw\": {\"r2\": r2_gpr, \"rmse\": rmse_gpr},\n",
    "            \"hgb_raw\": {\"r2\": r2_hgb, \"rmse\": rmse_hgb},\n",
    "            \"gpr_shape\": shape_metrics\n",
    "        },\n",
    "        \"train_mahal\": {\"center\": center.tolist(), \"cov_inv\": cov_inv.tolist()}\n",
    "    }\n",
    "    out_path = cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    joblib.dump(bundle, out_path)\n",
    "    if cfg.VERBOSE:\n",
    "        print(f\"[MODEL] Saved bundle → {out_path}\")\n",
    "        print(json.dumps(bundle[\"metrics\"], indent=2))\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 11. LOAD (WITH LEGACY COMPATIBILITY SHIM)\n",
    "# =========================\n",
    "def load_bundle():\n",
    "    \"\"\"\n",
    "    Load model bundle with backward compatibility for earlier versions.\n",
    "\n",
    "    Legacy schema keys:\n",
    "        \"scaler\", \"pca\", \"soc_model\", \"soh_model\"\n",
    "    New schema keys (v7):\n",
    "        \"soc_scaler\",\"soc_pca\",\"soc_model\",\"soh_scaler\",\"soh_pca\",\"soh_model\",\"soh_model_name\",...\n",
    "\n",
    "    We synthesize missing fields for legacy bundles so inference does not break.\n",
    "    \"\"\"\n",
    "    path = cfg.MODEL_DIR / \"eis_soc_soh_phys_models.joblib\"\n",
    "    bundle = joblib.load(path)\n",
    "\n",
    "    # Detect legacy\n",
    "    legacy = (\"scaler\" in bundle) and (\"soc_scaler\" not in bundle)\n",
    "    if legacy:\n",
    "        scaler = bundle[\"scaler\"]\n",
    "        pca = bundle.get(\"pca\")\n",
    "        soc_model = bundle.get(\"soc_model\")\n",
    "        soh_model = bundle.get(\"soh_model\")\n",
    "\n",
    "        bundle[\"soc_scaler\"] = scaler\n",
    "        bundle[\"soh_scaler\"] = scaler\n",
    "        bundle[\"soc_pca\"]    = pca\n",
    "        bundle[\"soh_pca\"]    = pca\n",
    "        bundle[\"soh_model\"]  = soh_model\n",
    "        bundle[\"soh_model_name\"] = bundle.get(\"soh_model_name\",\"legacy_model\")\n",
    "        if \"metrics\" not in bundle:\n",
    "            bundle[\"metrics\"] = {}\n",
    "        bundle[\"metrics\"].setdefault(\"soh_rmse_selected\", 5.0)\n",
    "        if \"train_mahal\" not in bundle:\n",
    "            try:\n",
    "                center = scaler.mean_\n",
    "                cov_inv = np.eye(len(center))\n",
    "                bundle[\"train_mahal\"] = {\"center\": center.tolist(), \"cov_inv\": cov_inv.tolist()}\n",
    "            except Exception:\n",
    "                bundle[\"train_mahal\"] = None\n",
    "        bundle.setdefault(\"feature_version\", -1)\n",
    "\n",
    "    # Sanity check\n",
    "    for key in [\"soc_scaler\",\"soc_model\",\"soh_scaler\",\"soh_model\",\"freq_grid\"]:\n",
    "        if key not in bundle:\n",
    "            raise KeyError(f\"Bundle missing required key: {key}\")\n",
    "\n",
    "    return bundle\n",
    "\n",
    "# =========================\n",
    "# 12. INFERENCE FEATURIZATION\n",
    "# =========================\n",
    "def featurize_any(file_path: Path, bundle):\n",
    "    freq_grid = bundle[\"freq_grid\"]\n",
    "    meta = parse_eis_metadata(file_path.stem)\n",
    "    freq,re_raw,im_raw = load_any_inference(file_path)\n",
    "    re_i=_interp_channel(freq, re_raw, freq_grid)\n",
    "    im_i=_interp_channel(freq, im_raw, freq_grid)\n",
    "    if meta is None and cfg.TEST_TEMPERATURE_OVERRIDE is not None:\n",
    "        temp = cfg.TEST_TEMPERATURE_OVERRIDE\n",
    "    else:\n",
    "        temp = meta[\"Temp\"] if meta else -1\n",
    "    vec = build_feature_vector(re_i, im_i, temp, freq_grid)\n",
    "    norm_vec=None\n",
    "    if cfg.INCLUDE_NORMALIZED_SHAPE_MODEL and bundle.get(\"shape_model\") is not None:\n",
    "        if cfg.NORMALIZE_SHAPE_BY_HF_RE:\n",
    "            rsh, ish = build_shape_normalized(re_i, im_i)\n",
    "            norm_vec = build_feature_vector(rsh, ish, temp, freq_grid)\n",
    "    return vec, norm_vec, meta\n",
    "\n",
    "# =========================\n",
    "# 13. OOD UTILITIES\n",
    "# =========================\n",
    "def mahalanobis_distance(x, center, cov_inv):\n",
    "    diff = x - center\n",
    "    return float(np.sqrt(diff @ cov_inv @ diff.T))\n",
    "\n",
    "def gp_ard_norm(Xp, model):\n",
    "    try:\n",
    "        K = model.kernel_\n",
    "        from sklearn.gaussian_process.kernels import RBF\n",
    "        rbf = None\n",
    "        if hasattr(K,\"k1\") and isinstance(K.k1,RBF): rbf=K.k1\n",
    "        elif hasattr(K,\"k2\") and isinstance(K.k2,RBF): rbf=K.k2\n",
    "        if rbf is None: return None\n",
    "        ls = np.atleast_1d(rbf.length_scale)\n",
    "        z = (Xp / ls).ravel()\n",
    "        return float(np.linalg.norm(z))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# 14. PROJECTION PLOT\n",
    "# =========================\n",
    "def build_projection(soh_current, cpp, lower, exponent=None, n=160):\n",
    "    if soh_current <= lower or cpp <= 0:\n",
    "        return np.array([0.0]), np.array([soh_current])\n",
    "    total = (soh_current - lower) * cpp\n",
    "    cycles = np.linspace(0, total, n)\n",
    "    S0 = soh_current; Smin=lower\n",
    "    if exponent is None: exponent = cfg.PLOT_EXPONENT\n",
    "    soh_curve = Smin + (S0 - Smin)*(1 - cycles/total)**exponent\n",
    "    return cycles, soh_curve\n",
    "\n",
    "def plot_projection(file_base, soh_current, soh_std, cycles_to_target,\n",
    "                    cycles_to_lower, cpp, ood_flag, out_path):\n",
    "    if cycles_to_lower <= 0:\n",
    "        return\n",
    "    cycles, curve = build_projection(soh_current, cpp, cfg.ILLUSTRATIVE_MIN_SOH)\n",
    "    plt.figure(figsize=(6.4,4))\n",
    "    plt.plot(cycles, curve, lw=2, label=\"Projected SoH\")\n",
    "    plt.axhline(cfg.DECISION_SOH_PERCENT, color=\"orange\", ls=\"--\", label=f\"{cfg.DECISION_SOH_PERCENT:.0f}% target\")\n",
    "    plt.axhline(cfg.ILLUSTRATIVE_MIN_SOH, color=\"red\", ls=\":\", label=f\"{cfg.ILLUSTRATIVE_MIN_SOH:.0f}% lower\")\n",
    "    plt.scatter([0],[soh_current], c=\"green\", s=55, label=f\"Current {soh_current:.2f}%\")\n",
    "    plt.text(0, soh_current+0.7, f\"±{soh_std:.2f}\", color=\"green\", fontsize=8)\n",
    "    if cycles_to_target > 0:\n",
    "        plt.axvline(cycles_to_target, color=\"orange\", ls=\"-.\")\n",
    "        plt.scatter([cycles_to_target],[cfg.DECISION_SOH_PERCENT], c=\"orange\", s=45)\n",
    "        plt.text(cycles_to_target, cfg.DECISION_SOH_PERCENT+1.0,\n",
    "                 f\"{cycles_to_target:.0f} cyc\", ha=\"center\", color=\"orange\", fontsize=8)\n",
    "    plt.scatter([cycles[-1]],[cfg.ILLUSTRATIVE_MIN_SOH], c=\"red\", s=50)\n",
    "    plt.text(cycles[-1], cfg.ILLUSTRATIVE_MIN_SOH-2,\n",
    "             f\"{cycles[-1]:.0f} cyc\", ha=\"center\", color=\"red\", fontsize=8)\n",
    "    if ood_flag:\n",
    "        plt.text(0.98,0.05,\"OOD\", transform=plt.gca().transAxes,\n",
    "                 ha=\"right\", va=\"bottom\", color=\"crimson\", fontsize=11,\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"crimson\"))\n",
    "    plt.xlabel(\"Remaining Cycles\")\n",
    "    plt.ylabel(\"SoH (%)\")\n",
    "    plt.title(f\"RUL Projection – {file_base}\")\n",
    "    plt.grid(alpha=0.35)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 15. INFERENCE (SINGLE FILE)\n",
    "# =========================\n",
    "def predict_file(file_path: Path, bundle, cpp_map, global_cpp):\n",
    "    vec, norm_vec, meta = featurize_any(file_path, bundle)\n",
    "\n",
    "    # SoC\n",
    "    soc_scaler=bundle[\"soc_scaler\"]; soc_pca=bundle.get(\"soc_pca\")\n",
    "    soc_model=bundle[\"soc_model\"]\n",
    "    X_soc = soc_scaler.transform(vec.reshape(1,-1))\n",
    "    X_soc_in = soc_pca.transform(X_soc) if soc_pca else X_soc\n",
    "    soc_probs = soc_model.predict_proba(X_soc_in)[0]\n",
    "    soc_classes = soc_model.classes_\n",
    "    soc_pred = int(soc_classes[np.argmax(soc_probs)])\n",
    "\n",
    "    # SoH (raw primary)\n",
    "    soh_scaler=bundle[\"soh_scaler\"]; soh_pca=bundle.get(\"soh_pca\")\n",
    "    soh_model=bundle[\"soh_model\"]; model_name=bundle.get(\"soh_model_name\",\"unknown\")\n",
    "    X_soh_s = soh_scaler.transform(vec.reshape(1,-1))\n",
    "    X_soh_in = soh_pca.transform(X_soh_s) if soh_pca else X_soh_s\n",
    "\n",
    "    if \"gpr\" in model_name:\n",
    "        sm, ss = soh_model.predict(X_soh_in, return_std=True)\n",
    "        soh_mean_raw = float(sm[0]); soh_std_raw=float(ss[0])\n",
    "    else:\n",
    "        soh_mean_raw = float(soh_model.predict(X_soh_in)[0])\n",
    "        soh_std_raw  = float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    # Shape model\n",
    "    shape_model = bundle.get(\"shape_model\")\n",
    "    shape_soh_mean=None; shape_soh_std=None\n",
    "    if shape_model is not None and norm_vec is not None:\n",
    "        sscaler = bundle.get(\"shape_scaler\")\n",
    "        spca = bundle.get(\"shape_pca\")\n",
    "        X_shape_s = sscaler.transform(norm_vec.reshape(1,-1))\n",
    "        X_shape_in = spca.transform(X_shape_s) if spca else X_shape_s\n",
    "        if hasattr(shape_model,\"predict\"):\n",
    "            if isinstance(shape_model, GaussianProcessRegressor):\n",
    "                sm2, ss2 = shape_model.predict(X_shape_in, return_std=True)\n",
    "                shape_soh_mean=float(sm2[0]); shape_soh_std=float(ss2[0])\n",
    "            else:\n",
    "                shape_soh_mean=float(shape_model.predict(X_shape_in)[0])\n",
    "                shape_soh_std=float(bundle[\"metrics\"].get(\"soh_rmse_selected\", 5.0))\n",
    "\n",
    "    # Ensemble\n",
    "    if cfg.ENSEMBLE_SOH and shape_soh_mean is not None:\n",
    "        soh_mean = 0.5*(soh_mean_raw + shape_soh_mean)\n",
    "        stds = [soh_std_raw]\n",
    "        if shape_soh_std is not None: stds.append(shape_soh_std)\n",
    "        soh_std = float(np.sqrt(np.mean(np.array(stds)**2)))\n",
    "    else:\n",
    "        soh_mean, soh_std = soh_mean_raw, soh_std_raw\n",
    "\n",
    "    # RUL\n",
    "    cpp = get_cpp(meta, cpp_map, global_cpp)\n",
    "    if soh_mean > cfg.DECISION_SOH_PERCENT:\n",
    "        cycles_to_target = (soh_mean - cfg.DECISION_SOH_PERCENT)*cpp\n",
    "    else:\n",
    "        cycles_to_target = 0.0\n",
    "    if soh_mean > cfg.ILLUSTRATIVE_MIN_SOH:\n",
    "        cycles_to_lower = (soh_mean - cfg.ILLUSTRATIVE_MIN_SOH)*cpp\n",
    "    else:\n",
    "        cycles_to_lower = 0.0\n",
    "\n",
    "    # OOD diagnostics\n",
    "    train_mahal = bundle.get(\"train_mahal\")\n",
    "    mahal_dist=None\n",
    "    if train_mahal:\n",
    "        cov_inv = np.array(train_mahal[\"cov_inv\"])\n",
    "        center = np.array(train_mahal[\"center\"])\n",
    "        mahal_dist = mahalanobis_distance(X_soh_s[0], center, cov_inv)\n",
    "    ard_norm=None\n",
    "    if \"gpr\" in model_name:\n",
    "        ard_norm = gp_ard_norm(X_soh_in, soh_model)\n",
    "    ood_flag=False\n",
    "    if (mahal_dist is not None and mahal_dist > cfg.MAHAL_THRESHOLD) or \\\n",
    "       (ard_norm is not None and ard_norm > cfg.GP_ARD_NORM_THRESHOLD):\n",
    "        ood_flag=True\n",
    "\n",
    "    result={\n",
    "        \"file\": str(file_path),\n",
    "        \"parsed_metadata\": meta,\n",
    "        \"predicted_SoC\": soc_pred,\n",
    "        \"SoC_probabilities\": {int(c): float(p) for c,p in zip(soc_classes, soc_probs)},\n",
    "        \"predicted_SoH_percent\": soh_mean,\n",
    "        \"SoH_std_estimate\": soh_std,\n",
    "        \"raw_model_mean\": soh_mean_raw,\n",
    "        \"raw_model_std\": soh_std_raw,\n",
    "        \"shape_model_mean\": shape_soh_mean,\n",
    "        \"shape_model_std\": shape_soh_std,\n",
    "        \"cycles_per_percent_used\": cpp,\n",
    "        \"cycles_to_target\": cycles_to_target,\n",
    "        \"cycles_to_lower\": cycles_to_lower,\n",
    "        \"decision_threshold_percent\": cfg.DECISION_SOH_PERCENT,\n",
    "        \"lower_threshold_percent\": cfg.ILLUSTRATIVE_MIN_SOH,\n",
    "        \"feature_version\": bundle.get(\"feature_version\"),\n",
    "        \"soh_model_chosen\": model_name,\n",
    "        \"OOD_mahal\": mahal_dist,\n",
    "        \"OOD_gp_ard_norm\": ard_norm,\n",
    "        \"OOD_flag\": ood_flag\n",
    "    }\n",
    "    return result, ood_flag, cycles_to_target, cycles_to_lower\n",
    "\n",
    "# =========================\n",
    "# 16. MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    if cfg.VERBOSE:\n",
    "        print(\"Configuration:\\n\", json.dumps(to_jsonable(asdict(cfg)), indent=2))\n",
    "\n",
    "    # Directory assertions\n",
    "    assert cfg.EIS_DIR.exists(), f\"EIS_DIR missing: {cfg.EIS_DIR}\"\n",
    "    if cfg.REFINE_SOH_WITH_CAPACITY:\n",
    "        assert cfg.CAP_DIR.exists(), f\"CAP_DIR missing: {cfg.CAP_DIR}\"\n",
    "\n",
    "    # Capacity + dynamic CPP\n",
    "    cap_df = load_capacity_info(cfg.CAP_DIR)\n",
    "    if cap_df.empty:\n",
    "        if cfg.VERBOSE:\n",
    "            print(\"[INFO] No / empty capacity data -> fallback Cpp.\")\n",
    "        cpp_map, global_cpp = {}, cfg.CPP_FALLBACK\n",
    "    else:\n",
    "        cpp_map, global_cpp = build_cpp_map(cap_df)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[CPP] dynamic cells={len(cpp_map)} global_cpp_median={global_cpp:.2f}\")\n",
    "\n",
    "    bundle_path = cfg.MODEL_DIR/\"eis_soc_soh_phys_models.joblib\"\n",
    "    if bundle_path.exists() and not cfg.FORCE_RETRAIN:\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[LOAD] Using existing model bundle: {bundle_path}\")\n",
    "        bundle = load_bundle()\n",
    "    else:\n",
    "        if cfg.VERBOSE:\n",
    "            print(\"[TRAIN] Building dataset & training models...\")\n",
    "        meta_df, X_raw, shape_bundle, y_soc, y_soh = build_dataset(cfg.EIS_DIR, cap_df)\n",
    "        if cfg.VERBOSE:\n",
    "            print(f\"[TRAIN] Samples={X_raw.shape[0]} Features={X_raw.shape[1]} Cells={meta_df.CellID.nunique()}\")\n",
    "        bundle = train_models(meta_df, X_raw, shape_bundle, y_soc, y_soh)\n",
    "\n",
    "    # Inference\n",
    "    for test_fp in cfg.EIS_TEST_FILES:\n",
    "        print(f\"\\n===== TEST: {test_fp.name} =====\")\n",
    "        if not test_fp.exists():\n",
    "            print(f\"[WARN] Test file not found: {test_fp}\")\n",
    "            continue\n",
    "        try:\n",
    "            result, ood_flag, cyc_target, cyc_lower = predict_file(test_fp, bundle, cpp_map, global_cpp)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Prediction failed for {test_fp.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        out_plot = cfg.MODEL_DIR / f\"{test_fp.stem}_projection.png\"\n",
    "        plot_projection(\n",
    "            test_fp.stem,\n",
    "            result[\"predicted_SoH_percent\"],\n",
    "            result[\"SoH_std_estimate\"],\n",
    "            result[\"cycles_to_target\"],\n",
    "            result[\"cycles_to_lower\"],\n",
    "            result[\"cycles_per_percent_used\"],\n",
    "            result[\"OOD_flag\"],\n",
    "            out_plot\n",
    "        )\n",
    "\n",
    "        out_json = cfg.MODEL_DIR / f\"{test_fp.stem}_prediction.json\"\n",
    "        with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(f\"[PLOT] Saved: {out_plot}\")\n",
    "        print(f\"[JSON] Saved: {out_json}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286764f6-bf98-4425-8fc6-102120ca4b70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m s\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22414a-5cfb-4082-9a34-a222d782f5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
